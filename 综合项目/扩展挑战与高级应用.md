# 扩展挑战与高级应用

## 1. 高级挑战项目

### 1.1 多机器人协作导航

#### 理论基础

**多智能体强化学习框架**：
在多机器人环境中，每个机器人$i$都有自己的状态$s_i$、动作$a_i$和奖励$r_i$。系统的联合状态为$\mathbf{s} = \{s_1, s_2, ..., s_n\}$，联合动作为$\mathbf{a} = \{a_1, a_2, ..., a_n\}$。

**去中心化执行框架**：
- **Independent Learning**：每个智能体独立学习
- **Centralized Training, Decentralized Execution (CTDE)**：集中训练，分布执行
- **Multi-Agent Deep Deterministic Policy Gradient (MADDPG)**

**分布式传感器融合**：
使用Consensus-based融合算法：
$$\hat{x}_i^{(k+1)} = \hat{x}_i^{(k)} + \epsilon \sum_{j \in \mathcal{N}_i} (\hat{x}_j^{(k)} - \hat{x}_i^{(k)})$$

其中$\mathcal{N}_i$是机器人$i$的邻居集合，$\epsilon$是共识增益。

#### 实现代码

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from collections import deque
import random
import threading
import time

class MultiAgentEnvironment:
    """多机器人导航环境"""

    def __init__(self, num_robots=3, map_size=(20, 20), num_obstacles=15):
        self.num_robots = num_robots
        self.map_size = map_size
        self.num_obstacles = num_obstacles
        self.reset()

        # 通信范围
        self.communication_range = 5.0

    def reset(self):
        """重置环境"""
        # 生成地图
        self.grid = np.zeros(self.map_size)

        # 随机放置障碍物
        obstacle_positions = set()
        while len(obstacle_positions) < self.num_obstacles:
            pos = (np.random.randint(2, self.map_size[0]-2),
                   np.random.randint(2, self.map_size[1]-2))
            obstacle_positions.add(pos)

        for pos in obstacle_positions:
            self.grid[pos] = 1

        # 初始化机器人位置
        self.robot_positions = []
        self.robot_goals = []

        for i in range(self.num_robots):
            # 随机起始位置
            while True:
                pos = (np.random.randint(0, self.map_size[0]),
                       np.random.randint(0, self.map_size[1]))
                if self.grid[pos] == 0 and pos not in self.robot_positions:
                    self.robot_positions.append(pos)
                    break

            # 随机目标位置
            while True:
                goal = (np.random.randint(0, self.map_size[0]),
                        np.random.randint(0, self.map_size[1]))
                if (self.grid[goal] == 0 and
                    goal not in self.robot_goals and
                    goal not in self.robot_positions):
                    self.robot_goals.append(goal)
                    break

        self.steps = 0
        self.max_steps = 500

        return self.get_observations()

    def get_observations(self):
        """获取所有机器人的观测"""
        observations = []

        for i in range(self.num_robots):
            obs = self.get_robot_observation(i)
            observations.append(obs)

        return observations

    def get_robot_observation(self, robot_id):
        """获取单个机器人的观测"""
        pos = self.robot_positions[robot_id]
        goal = self.robot_goals[robot_id]

        # 基本状态：位置、目标
        obs = []
        obs.extend([pos[0] / self.map_size[0], pos[1] / self.map_size[1]])
        obs.extend([goal[0] / self.map_size[0], goal[1] / self.map_size[1]])

        # 到目标的距离和角度
        dx = goal[0] - pos[0]
        dy = goal[1] - pos[1]
        distance = np.sqrt(dx**2 + dy**2) / np.sqrt(self.map_size[0]**2 + self.map_size[1]**2)
        angle = np.arctan2(dy, dx) / np.pi
        obs.extend([distance, np.sin(angle), np.cos(angle)])

        # 局部环境信息（3x3网格）
        for dx in [-1, 0, 1]:
            for dy in [-1, 0, 1]:
                new_x, new_y = pos[0] + dx, pos[1] + dy
                if (0 <= new_x < self.map_size[0] and
                    0 <= new_y < self.map_size[1]):
                    # 检查障碍物
                    obstacle = self.grid[new_x, new_y]
                    # 检查其他机器人
                    other_robot = 0
                    for j, other_pos in enumerate(self.robot_positions):
                        if j != robot_id and other_pos == (new_x, new_y):
                            other_robot = 1
                            break
                    obs.extend([obstacle, other_robot])
                else:
                    obs.extend([1, 0])  # 边界

        # 邻居机器人信息
        neighbor_info = []
        for j in range(self.num_robots):
            if j != robot_id:
                other_pos = self.robot_positions[j]
                dist = np.sqrt((pos[0] - other_pos[0])**2 + (pos[1] - other_pos[1])**2)

                if dist <= self.communication_range:
                    # 在通信范围内
                    relative_x = (other_pos[0] - pos[0]) / self.communication_range
                    relative_y = (other_pos[1] - pos[1]) / self.communication_range
                    neighbor_info.extend([relative_x, relative_y, 1])  # 1表示在通信范围内
                else:
                    neighbor_info.extend([0, 0, 0])

        obs.extend(neighbor_info)

        return np.array(obs, dtype=np.float32)

    def step(self, actions):
        """执行所有机器人的动作"""
        # 动作：0-上，1-下，2-左，3-右，4-停止
        moves = [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0)]

        new_positions = []
        rewards = []
        dones = []

        # 计算新位置
        for i, action in enumerate(actions):
            pos = self.robot_positions[i]
            dx, dy = moves[action]
            new_x, new_y = pos[0] + dx, pos[1] + dy

            # 检查边界和障碍物
            if (0 <= new_x < self.map_size[0] and
                0 <= new_y < self.map_size[1] and
                self.grid[new_x, new_y] == 0):
                new_positions.append((new_x, new_y))
            else:
                new_positions.append(pos)  # 保持原位置

        # 解决冲突（多个机器人移动到同一位置）
        final_positions = self.resolve_conflicts(new_positions)

        # 计算奖励
        for i in range(self.num_robots):
            reward = self.calculate_reward(i, final_positions[i], actions[i])
            rewards.append(reward)

            # 检查是否到达目标
            goal_distance = np.sqrt((final_positions[i][0] - self.robot_goals[i][0])**2 +
                                   (final_positions[i][1] - self.robot_goals[i][1])**2)
            dones.append(goal_distance < 1.0)

        self.robot_positions = final_positions
        self.steps += 1

        # 全局终止条件
        global_done = (self.steps >= self.max_steps or all(dones))

        return self.get_observations(), rewards, dones, global_done

    def resolve_conflicts(self, new_positions):
        """解决位置冲突"""
        final_positions = list(new_positions)

        # 检查冲突
        for i in range(len(new_positions)):
            for j in range(i+1, len(new_positions)):
                if new_positions[i] == new_positions[j]:
                    # 冲突：优先级基于机器人ID，较小ID保持新位置
                    final_positions[j] = self.robot_positions[j]

        return final_positions

    def calculate_reward(self, robot_id, new_pos, action):
        """计算机器人奖励"""
        old_pos = self.robot_positions[robot_id]
        goal = self.robot_goals[robot_id]

        # 距离奖励
        old_distance = np.sqrt((old_pos[0] - goal[0])**2 + (old_pos[1] - goal[1])**2)
        new_distance = np.sqrt((new_pos[0] - goal[0])**2 + (new_pos[1] - goal[1])**2)

        distance_reward = (old_distance - new_distance) * 10

        # 到达目标奖励
        if new_distance < 1.0:
            arrival_reward = 100
        else:
            arrival_reward = 0

        # 碰撞惩罚
        collision_penalty = 0
        if new_pos == old_pos and action != 4:  # 想移动但没有移动
            collision_penalty = -5

        # 协作奖励：与其他机器人的距离
        cooperation_reward = 0
        for j in range(self.num_robots):
            if j != robot_id:
                other_pos = self.robot_positions[j]
                dist = np.sqrt((new_pos[0] - other_pos[0])**2 + (new_pos[1] - other_pos[1])**2)

                # 鼓励保持适当距离（避免过近或过远）
                optimal_distance = 3.0
                cooperation_reward += -0.1 * abs(dist - optimal_distance)

        # 时间惩罚
        time_penalty = -0.1

        total_reward = (distance_reward + arrival_reward + collision_penalty +
                       cooperation_reward + time_penalty)

        return total_reward

    def get_communication_graph(self):
        """获取通信图"""
        comm_matrix = np.zeros((self.num_robots, self.num_robots))

        for i in range(self.num_robots):
            for j in range(self.num_robots):
                if i != j:
                    pos_i = self.robot_positions[i]
                    pos_j = self.robot_positions[j]
                    distance = np.sqrt((pos_i[0] - pos_j[0])**2 + (pos_i[1] - pos_j[1])**2)

                    if distance <= self.communication_range:
                        comm_matrix[i, j] = 1

        return comm_matrix

    def render(self):
        """可视化环境"""
        fig, ax = plt.subplots(figsize=(10, 10))

        # 绘制地图
        ax.imshow(self.grid, cmap='Greys', origin='lower', alpha=0.3)

        # 绘制机器人
        colors = ['red', 'blue', 'green', 'orange', 'purple']
        for i, (pos, goal) in enumerate(zip(self.robot_positions, self.robot_goals)):
            color = colors[i % len(colors)]

            # 机器人位置
            ax.scatter(pos[1], pos[0], c=color, s=200, marker='o', edgecolors='black', linewidth=2)
            ax.text(pos[1], pos[0], str(i), ha='center', va='center', fontweight='bold', color='white')

            # 目标位置
            ax.scatter(goal[1], goal[0], c=color, s=200, marker='*', edgecolors='black', linewidth=2)

            # 连接线
            ax.plot([pos[1], goal[1]], [pos[0], goal[0]], color=color, linestyle='--', alpha=0.5)

        # 绘制通信连接
        comm_graph = self.get_communication_graph()
        for i in range(self.num_robots):
            for j in range(i+1, self.num_robots):
                if comm_graph[i, j]:
                    pos_i = self.robot_positions[i]
                    pos_j = self.robot_positions[j]
                    ax.plot([pos_i[1], pos_j[1]], [pos_i[0], pos_j[0]], 'k-', alpha=0.3, linewidth=1)

        ax.set_xlim(-0.5, self.map_size[1] - 0.5)
        ax.set_ylim(-0.5, self.map_size[0] - 0.5)
        ax.set_title(f'多机器人协作导航 - 步数: {self.steps}')
        ax.grid(True, alpha=0.3)

        plt.show()

class MADDPGAgent:
    """MADDPG智能体"""

    def __init__(self, num_agents, obs_dim, action_dim, hidden_dim=64):
        self.num_agents = num_agents
        self.obs_dim = obs_dim
        self.action_dim = action_dim

        # 为每个智能体创建Actor和Critic网络
        self.actors = []
        self.critics = []
        self.target_actors = []
        self.target_critics = []

        for i in range(num_agents):
            actor = self.create_actor_network(obs_dim, action_dim, hidden_dim)
            critic = self.create_critic_network(obs_dim * num_agents, action_dim * num_agents, hidden_dim)

            self.actors.append(actor)
            self.critics.append(critic)

            # 目标网络
            target_actor = self.create_actor_network(obs_dim, action_dim, hidden_dim)
            target_critic = self.create_critic_network(obs_dim * num_agents, action_dim * num_agents, hidden_dim)

            target_actor.load_state_dict(actor.state_dict())
            target_critic.load_state_dict(critic.state_dict())

            self.target_actors.append(target_actor)
            self.target_critics.append(target_critic)

        # 优化器
        self.actor_optimizers = [optim.Adam(actor.parameters(), lr=1e-3) for actor in self.actors]
        self.critic_optimizers = [optim.Adam(critic.parameters(), lr=1e-3) for critic in self.critics]

        # 经验回放
        self.memory = deque(maxlen=10000)
        self.batch_size = 64

        # 噪声参数
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    def create_actor_network(self, obs_dim, action_dim, hidden_dim):
        """创建Actor网络"""
        return nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )

    def create_critic_network(self, obs_dim, action_dim, hidden_dim):
        """创建Critic网络"""
        return nn.Sequential(
            nn.Linear(obs_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def select_actions(self, observations):
        """选择动作"""
        actions = []

        for i, obs in enumerate(observations):
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            action_probs = self.actors[i](obs_tensor)

            if random.random() < self.epsilon:
                # 随机探索
                action = random.randint(0, self.action_dim - 1)
            else:
                # 贪心选择
                action = action_probs.argmax().item()

            actions.append(action)

        return actions

    def store_transition(self, obs, actions, rewards, next_obs, dones):
        """存储经验"""
        self.memory.append((obs, actions, rewards, next_obs, dones))

    def learn(self):
        """学习"""
        if len(self.memory) < self.batch_size:
            return

        # 采样经验
        batch = random.sample(self.memory, self.batch_size)
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*batch)

        # 转换为tensor
        obs_batch = torch.FloatTensor(obs_batch)
        action_batch = torch.LongTensor(action_batch)
        reward_batch = torch.FloatTensor(reward_batch)
        next_obs_batch = torch.FloatTensor(next_obs_batch)
        done_batch = torch.BoolTensor(done_batch)

        # 为每个智能体更新网络
        for agent_id in range(self.num_agents):
            self.update_critic(agent_id, obs_batch, action_batch, reward_batch, next_obs_batch, done_batch)
            self.update_actor(agent_id, obs_batch)

        # 软更新目标网络
        self.soft_update_target_networks()

        # 衰减探索率
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_critic(self, agent_id, obs_batch, action_batch, reward_batch, next_obs_batch, done_batch):
        """更新Critic网络"""
        batch_size = obs_batch.size(0)

        # 当前智能体的观测和奖励
        current_obs = obs_batch[:, agent_id, :]
        current_rewards = reward_batch[:, agent_id]
        current_dones = done_batch[:, agent_id]

        # 全局观测和动作
        global_obs = obs_batch.view(batch_size, -1)
        global_actions = action_batch.view(batch_size, -1).float()
        global_next_obs = next_obs_batch.view(batch_size, -1)

        # 目标Q值计算
        with torch.no_grad():
            next_actions = []
            for i in range(self.num_agents):
                next_action_probs = self.target_actors[i](next_obs_batch[:, i, :])
                next_actions.append(next_action_probs.argmax(dim=1, keepdim=True).float())

            global_next_actions = torch.cat(next_actions, dim=1)
            global_next_state_actions = torch.cat([global_next_obs, global_next_actions], dim=1)

            target_q_values = self.target_critics[agent_id](global_next_state_actions).squeeze()
            target_q_values = current_rewards + 0.99 * target_q_values * ~current_dones

        # 当前Q值
        global_state_actions = torch.cat([global_obs, global_actions], dim=1)
        current_q_values = self.critics[agent_id](global_state_actions).squeeze()

        # Critic损失
        critic_loss = nn.MSELoss()(current_q_values, target_q_values)

        # 更新Critic
        self.critic_optimizers[agent_id].zero_grad()
        critic_loss.backward()
        self.critic_optimizers[agent_id].step()

    def update_actor(self, agent_id, obs_batch):
        """更新Actor网络"""
        batch_size = obs_batch.size(0)

        # 当前智能体的观测
        current_obs = obs_batch[:, agent_id, :]

        # 计算动作
        action_probs = self.actors[agent_id](current_obs)
        actions = action_probs.argmax(dim=1, keepdim=True).float()

        # 构建全局状态-动作对
        global_obs = obs_batch.view(batch_size, -1)

        # 获取其他智能体的动作（使用当前策略）
        other_actions = []
        for i in range(self.num_agents):
            if i != agent_id:
                other_action_probs = self.actors[i](obs_batch[:, i, :])
                other_actions.append(other_action_probs.argmax(dim=1, keepdim=True).float())

        # 插入当前智能体的动作
        all_actions = []
        action_idx = 0
        for i in range(self.num_agents):
            if i == agent_id:
                all_actions.append(actions)
            else:
                all_actions.append(other_actions[action_idx])
                action_idx += 1

        global_actions = torch.cat(all_actions, dim=1)
        global_state_actions = torch.cat([global_obs, global_actions], dim=1)

        # Actor损失（负Q值）
        actor_loss = -self.critics[agent_id](global_state_actions).mean()

        # 更新Actor
        self.actor_optimizers[agent_id].zero_grad()
        actor_loss.backward()
        self.actor_optimizers[agent_id].step()

    def soft_update_target_networks(self, tau=0.01):
        """软更新目标网络"""
        for i in range(self.num_agents):
            for target_param, param in zip(self.target_actors[i].parameters(), self.actors[i].parameters()):
                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

            for target_param, param in zip(self.target_critics[i].parameters(), self.critics[i].parameters()):
                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

def train_multi_robot_navigation():
    """训练多机器人导航"""

    print("=== 多机器人协作导航训练 ===")

    # 创建环境和智能体
    env = MultiAgentEnvironment(num_robots=3, map_size=(15, 15), num_obstacles=10)
    obs = env.reset()

    obs_dim = len(obs[0])
    action_dim = 5  # 5个动作
    agent = MADDPGAgent(env.num_robots, obs_dim, action_dim)

    print(f"环境: {env.num_robots}个机器人, 观测维度: {obs_dim}, 动作维度: {action_dim}")

    # 训练参数
    num_episodes = 1000
    max_steps_per_episode = 500

    # 记录训练过程
    episode_rewards = []
    success_rates = []
    cooperation_scores = []

    for episode in range(num_episodes):
        obs = env.reset()
        episode_reward = np.zeros(env.num_robots)
        steps = 0

        while steps < max_steps_per_episode:
            # 选择动作
            actions = agent.select_actions(obs)

            # 执行动作
            next_obs, rewards, dones, global_done = env.step(actions)

            # 存储经验
            agent.store_transition(obs, actions, rewards, next_obs, dones)

            # 学习
            if episode > 10:  # 收集一些经验后开始学习
                agent.learn()

            obs = next_obs
            episode_reward += np.array(rewards)
            steps += 1

            if global_done:
                break

        # 记录指标
        episode_rewards.append(np.mean(episode_reward))

        # 计算成功率
        success_count = sum(dones)
        success_rate = success_count / env.num_robots
        success_rates.append(success_rate)

        # 计算协作分数（基于机器人间的距离）
        cooperation_score = 0
        for i in range(env.num_robots):
            for j in range(i+1, env.num_robots):
                pos_i = env.robot_positions[i]
                pos_j = env.robot_positions[j]
                distance = np.sqrt((pos_i[0] - pos_j[0])**2 + (pos_i[1] - pos_j[1])**2)
                cooperation_score += min(distance / 5.0, 1.0)  # 归一化距离

        cooperation_score /= (env.num_robots * (env.num_robots - 1) / 2)
        cooperation_scores.append(cooperation_score)

        if episode % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
            avg_success = np.mean(success_rates[-100:]) if len(success_rates) >= 100 else np.mean(success_rates)
            avg_cooperation = np.mean(cooperation_scores[-100:]) if len(cooperation_scores) >= 100 else np.mean(cooperation_scores)

            print(f"Episode {episode}:")
            print(f"  平均奖励: {avg_reward:.2f}")
            print(f"  成功率: {avg_success:.2%}")
            print(f"  协作分数: {avg_cooperation:.3f}")
            print(f"  探索率: {agent.epsilon:.3f}")

    # 可视化训练结果
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 奖励曲线
    axes[0, 0].plot(episode_rewards, alpha=0.3)
    axes[0, 0].plot(np.convolve(episode_rewards, np.ones(50)/50, mode='valid'), linewidth=2)
    axes[0, 0].set_title('训练奖励曲线')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('平均奖励')
    axes[0, 0].grid(True)

    # 成功率
    axes[0, 1].plot(success_rates, alpha=0.3)
    axes[0, 1].plot(np.convolve(success_rates, np.ones(50)/50, mode='valid'), linewidth=2)
    axes[0, 1].set_title('成功率趋势')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('成功率')
    axes[0, 1].grid(True)

    # 协作分数
    axes[1, 0].plot(cooperation_scores, alpha=0.3)
    axes[1, 0].plot(np.convolve(cooperation_scores, np.ones(50)/50, mode='valid'), linewidth=2)
    axes[1, 0].set_title('协作分数')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('协作分数')
    axes[1, 0].grid(True)

    # 最终演示
    obs = env.reset()
    agent.epsilon = 0  # 关闭探索
    trajectory = [env.robot_positions.copy()]

    for step in range(100):
        actions = agent.select_actions(obs)
        obs, rewards, dones, global_done = env.step(actions)
        trajectory.append(env.robot_positions.copy())

        if global_done or all(dones):
            break

    # 绘制轨迹
    axes[1, 1].imshow(env.grid, cmap='Greys', origin='lower', alpha=0.3)

    colors = ['red', 'blue', 'green']
    for robot_id in range(env.num_robots):
        robot_traj = [pos[robot_id] for pos in trajectory]
        x_coords = [pos[1] for pos in robot_traj]
        y_coords = [pos[0] for pos in robot_traj]

        axes[1, 1].plot(x_coords, y_coords, color=colors[robot_id], linewidth=2, alpha=0.7, label=f'Robot {robot_id}')
        axes[1, 1].scatter(x_coords[0], y_coords[0], color=colors[robot_id], s=100, marker='o')
        axes[1, 1].scatter(env.robot_goals[robot_id][1], env.robot_goals[robot_id][0],
                          color=colors[robot_id], s=100, marker='*')

    axes[1, 1].set_title('学习后的协作轨迹')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('multi_robot_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 交互式演示
    print("\n开始交互式演示...")
    for demo_round in range(3):
        print(f"\n演示 {demo_round + 1}:")
        obs = env.reset()
        agent.epsilon = 0

        for step in range(100):
            env.render()
            actions = agent.select_actions(obs)
            obs, rewards, dones, global_done = env.step(actions)

            print(f"步骤 {step}: 动作={actions}, 奖励={[f'{r:.1f}' for r in rewards]}")

            if global_done or all(dones):
                print("所有机器人到达目标！")
                break

            time.sleep(0.5)  # 暂停以便观察

    return agent, env

if __name__ == "__main__":
    trained_agent, test_env = train_multi_robot_navigation()
    print("多机器人协作导航训练完成！")
```

### 1.2 动态环境适应性导航

#### 理论基础

**环境变化建模**：
- **障碍物动态变化**：$O_t = O_{t-1} + \Delta O_t$
- **目标位置变化**：$g_t = g_{t-1} + v_g \cdot \Delta t$
- **传感器可靠性变化**：$R_t = R_0 \cdot \exp(-\lambda t)$

**自适应滤波算法**：
使用变遗忘因子的递推最小二乘法：
$$\mathbf{P}_k = \frac{1}{\lambda_k}[\mathbf{P}_{k-1} - \frac{\mathbf{P}_{k-1}\mathbf{h}_k\mathbf{h}_k^T\mathbf{P}_{k-1}}{\lambda_k + \mathbf{h}_k^T\mathbf{P}_{k-1}\mathbf{h}_k}]$$

其中$\lambda_k$是根据创新序列自适应调整的遗忘因子。

#### 实现代码

```python
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import time
import random

class DynamicEnvironment:
    """动态环境"""

    def __init__(self, map_size=(20, 20)):
        self.map_size = map_size
        self.time_step = 0

        # 动态元素
        self.moving_obstacles = []
        self.dynamic_goals = []
        self.environmental_changes = []

        self.reset()

    def reset(self):
        """重置环境"""
        self.time_step = 0
        self.static_grid = np.zeros(self.map_size)

        # 静态障碍物
        num_static_obstacles = 10
        for _ in range(num_static_obstacles):
            pos = (np.random.randint(2, self.map_size[0]-2),
                   np.random.randint(2, self.map_size[1]-2))
            self.static_grid[pos] = 1

        # 移动障碍物
        self.moving_obstacles = []
        num_moving_obstacles = 3
        for i in range(num_moving_obstacles):
            obstacle = {
                'id': i,
                'position': (np.random.randint(0, self.map_size[0]),
                           np.random.randint(0, self.map_size[1])),
                'velocity': (np.random.uniform(-0.5, 0.5),
                           np.random.uniform(-0.5, 0.5)),
                'size': np.random.uniform(1.0, 2.0),
                'type': 'circular'
            }
            self.moving_obstacles.append(obstacle)

        # 动态目标
        self.dynamic_goals = []
        num_dynamic_goals = 2
        for i in range(num_dynamic_goals):
            goal = {
                'id': i,
                'position': (np.random.randint(0, self.map_size[0]),
                           np.random.randint(0, self.map_size[1])),
                'velocity': (np.random.uniform(-0.2, 0.2),
                           np.random.uniform(-0.2, 0.2)),
                'priority': np.random.uniform(0.5, 1.0),
                'lifetime': np.random.randint(100, 300)
            }
            self.dynamic_goals.append(goal)

        # 环境变化事件
        self.environmental_changes = []

        return self.get_current_state()

    def step(self, dt=1.0):
        """环境演化一步"""
        self.time_step += dt

        # 更新移动障碍物
        self.update_moving_obstacles(dt)

        # 更新动态目标
        self.update_dynamic_goals(dt)

        # 处理环境变化事件
        self.process_environmental_changes()

        # 随机事件
        self.generate_random_events()

        return self.get_current_state()

    def update_moving_obstacles(self, dt):
        """更新移动障碍物"""
        for obstacle in self.moving_obstacles:
            # 更新位置
            pos = obstacle['position']
            vel = obstacle['velocity']

            new_x = pos[0] + vel[0] * dt
            new_y = pos[1] + vel[1] * dt

            # 边界反弹
            if new_x <= 0 or new_x >= self.map_size[0] - 1:
                obstacle['velocity'] = (-vel[0], vel[1])
                new_x = max(0, min(self.map_size[0] - 1, new_x))

            if new_y <= 0 or new_y >= self.map_size[1] - 1:
                obstacle['velocity'] = (vel[0], -vel[1])
                new_y = max(0, min(self.map_size[1] - 1, new_y))

            obstacle['position'] = (new_x, new_y)

            # 随机改变方向
            if np.random.random() < 0.02:  # 2%概率
                obstacle['velocity'] = (np.random.uniform(-0.5, 0.5),
                                      np.random.uniform(-0.5, 0.5))

    def update_dynamic_goals(self, dt):
        """更新动态目标"""
        goals_to_remove = []

        for i, goal in enumerate(self.dynamic_goals):
            # 减少生命周期
            goal['lifetime'] -= dt

            if goal['lifetime'] <= 0:
                goals_to_remove.append(i)
                continue

            # 更新位置
            pos = goal['position']
            vel = goal['velocity']

            new_x = pos[0] + vel[0] * dt
            new_y = pos[1] + vel[1] * dt

            # 边界处理
            new_x = max(0, min(self.map_size[0] - 1, new_x))
            new_y = max(0, min(self.map_size[1] - 1, new_y))

            goal['position'] = (new_x, new_y)

        # 移除过期目标
        for i in reversed(goals_to_remove):
            del self.dynamic_goals[i]

        # 生成新目标
        if len(self.dynamic_goals) < 2 and np.random.random() < 0.1:
            new_goal = {
                'id': self.time_step,
                'position': (np.random.randint(0, self.map_size[0]),
                           np.random.randint(0, self.map_size[1])),
                'velocity': (np.random.uniform(-0.2, 0.2),
                           np.random.uniform(-0.2, 0.2)),
                'priority': np.random.uniform(0.5, 1.0),
                'lifetime': np.random.randint(100, 300)
            }
            self.dynamic_goals.append(new_goal)

    def process_environmental_changes(self):
        """处理环境变化事件"""
        changes_to_remove = []

        for i, change in enumerate(self.environmental_changes):
            if change['start_time'] <= self.time_step <= change['end_time']:
                self.apply_environmental_change(change)

            if self.time_step > change['end_time']:
                changes_to_remove.append(i)

        # 移除过期事件
        for i in reversed(changes_to_remove):
            del self.environmental_changes[i]

    def apply_environmental_change(self, change):
        """应用环境变化"""
        if change['type'] == 'obstacle_appear':
            # 临时障碍物出现
            for pos in change['positions']:
                if (0 <= pos[0] < self.map_size[0] and
                    0 <= pos[1] < self.map_size[1]):
                    self.static_grid[pos] = 1

        elif change['type'] == 'obstacle_disappear':
            # 障碍物消失
            for pos in change['positions']:
                if (0 <= pos[0] < self.map_size[0] and
                    0 <= pos[1] < self.map_size[1]):
                    self.static_grid[pos] = 0

    def generate_random_events(self):
        """生成随机事件"""
        # 障碍物出现事件
        if np.random.random() < 0.05:  # 5%概率
            positions = []
            num_obstacles = np.random.randint(1, 4)

            for _ in range(num_obstacles):
                pos = (np.random.randint(0, self.map_size[0]),
                       np.random.randint(0, self.map_size[1]))
                positions.append(pos)

            event = {
                'type': 'obstacle_appear',
                'start_time': self.time_step,
                'end_time': self.time_step + np.random.randint(50, 150),
                'positions': positions
            }
            self.environmental_changes.append(event)

        # 传感器干扰事件
        if np.random.random() < 0.02:  # 2%概率
            event = {
                'type': 'sensor_interference',
                'start_time': self.time_step,
                'end_time': self.time_step + np.random.randint(10, 30),
                'affected_sensors': ['gps', 'imu'],
                'interference_level': np.random.uniform(2.0, 5.0)
            }
            self.environmental_changes.append(event)

    def get_current_state(self):
        """获取当前环境状态"""
        # 合并静态和动态障碍物
        current_grid = self.static_grid.copy()

        # 添加移动障碍物
        for obstacle in self.moving_obstacles:
            pos = obstacle['position']
            size = obstacle['size']

            # 简化为圆形障碍物
            center_x, center_y = int(pos[0]), int(pos[1])
            radius = int(size)

            for dx in range(-radius, radius + 1):
                for dy in range(-radius, radius + 1):
                    if dx*dx + dy*dy <= radius*radius:
                        new_x, new_y = center_x + dx, center_y + dy
                        if (0 <= new_x < self.map_size[0] and
                            0 <= new_y < self.map_size[1]):
                            current_grid[new_x, new_y] = 1

        return {
            'static_grid': self.static_grid.copy(),
            'current_grid': current_grid,
            'moving_obstacles': self.moving_obstacles.copy(),
            'dynamic_goals': self.dynamic_goals.copy(),
            'environmental_changes': self.environmental_changes.copy(),
            'time_step': self.time_step
        }

    def get_sensor_noise_level(self, sensor_type):
        """获取传感器噪声水平"""
        base_noise = {
            'imu': 0.01,
            'gps': 1.0,
            'lidar': 0.1,
            'camera': 0.05
        }

        noise_level = base_noise.get(sensor_type, 0.1)

        # 检查是否有传感器干扰
        for change in self.environmental_changes:
            if (change['type'] == 'sensor_interference' and
                change['start_time'] <= self.time_step <= change['end_time'] and
                sensor_type in change['affected_sensors']):
                noise_level *= change['interference_level']

        return noise_level

    def render(self):
        """可视化环境"""
        state = self.get_current_state()
        current_grid = state['current_grid']

        fig, ax = plt.subplots(figsize=(10, 10))

        # 绘制静态环境
        ax.imshow(current_grid, cmap='Greys', origin='lower', alpha=0.7)

        # 绘制移动障碍物
        for obstacle in self.moving_obstacles:
            pos = obstacle['position']
            size = obstacle['size']
            circle = plt.Circle((pos[1], pos[0]), size, color='red', alpha=0.6)
            ax.add_patch(circle)

            # 绘制速度向量
            vel = obstacle['velocity']
            ax.arrow(pos[1], pos[0], vel[1]*5, vel[0]*5,
                    head_width=0.3, head_length=0.2, fc='red', ec='red')

        # 绘制动态目标
        for goal in self.dynamic_goals:
            pos = goal['position']
            priority = goal['priority']
            lifetime = goal['lifetime']

            # 颜色基于优先级
            color_intensity = priority
            ax.scatter(pos[1], pos[0], c='gold', s=200*priority, marker='*',
                      edgecolors='black', linewidth=2, alpha=color_intensity)

            # 显示剩余生命周期
            ax.text(pos[1], pos[0]-1, f'{int(lifetime)}', ha='center', va='top',
                   fontsize=8, fontweight='bold')

        # 显示环境变化事件
        active_events = [change for change in self.environmental_changes
                        if change['start_time'] <= self.time_step <= change['end_time']]

        event_text = f"时间步: {int(self.time_step)}\n活跃事件: {len(active_events)}"
        for event in active_events[:3]:  # 只显示前3个
            event_text += f"\n- {event['type']}"

        ax.text(0.02, 0.98, event_text, transform=ax.transAxes, fontsize=10,
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        ax.set_xlim(-0.5, self.map_size[1] - 0.5)
        ax.set_ylim(-0.5, self.map_size[0] - 0.5)
        ax.set_title(f'动态环境 - 时间步: {int(self.time_step)}')
        ax.grid(True, alpha=0.3)

        plt.show()

class AdaptiveNavigationAgent:
    """自适应导航智能体"""

    def __init__(self, obs_dim, action_dim):
        self.obs_dim = obs_dim
        self.action_dim = action_dim

        # 基础DQN
        self.q_network = self.create_network()
        self.target_network = self.create_network()
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=1e-3)

        # 自适应模块
        self.adaptation_memory = deque(maxlen=1000)
        self.environment_classifier = self.create_classifier()
        self.adaptation_optimizer = torch.optim.Adam(self.environment_classifier.parameters(), lr=1e-3)

        # 多个专门化网络
        self.specialized_networks = {
            'static': self.create_network(),
            'dynamic': self.create_network(),
            'noisy': self.create_network()
        }

        # 自适应参数
        self.adaptation_rate = 0.1
        self.environment_type_history = deque(maxlen=50)
        self.performance_history = deque(maxlen=100)

    def create_network(self):
        """创建神经网络"""
        return nn.Sequential(
            nn.Linear(self.obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, self.action_dim)
        )

    def create_classifier(self):
        """创建环境分类器"""
        return nn.Sequential(
            nn.Linear(self.obs_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 3)  # 3种环境类型
        )

    def classify_environment(self, observation):
        """分类当前环境类型"""
        obs_tensor = torch.FloatTensor(observation).unsqueeze(0)
        env_probs = torch.softmax(self.environment_classifier(obs_tensor), dim=1)
        env_type = env_probs.argmax().item()

        env_types = ['static', 'dynamic', 'noisy']
        return env_types[env_type], env_probs[0].detach().numpy()

    def select_action(self, observation, epsilon=0.1):
        """自适应动作选择"""
        # 分类环境
        env_type, env_probs = self.classify_environment(observation)

        # 选择合适的网络
        if env_type in self.specialized_networks:
            active_network = self.specialized_networks[env_type]
        else:
            active_network = self.q_network

        # ε-贪心策略
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)

        obs_tensor = torch.FloatTensor(observation).unsqueeze(0)
        q_values = active_network(obs_tensor)
        return q_values.argmax().item()

    def adapt_to_environment(self, observation, reward, performance_metrics):
        """环境适应"""
        # 记录性能
        self.performance_history.append(performance_metrics)

        # 环境类型识别
        env_type, _ = self.classify_environment(observation)
        self.environment_type_history.append(env_type)

        # 检测环境变化
        if len(self.environment_type_history) >= 10:
            recent_types = list(self.environment_type_history)[-10:]
            if len(set(recent_types)) > 1:  # 环境类型变化
                self.trigger_adaptation()

        # 性能监控
        if len(self.performance_history) >= 20:
            recent_performance = list(self.performance_history)[-20:]
            avg_performance = np.mean(recent_performance)

            if avg_performance < 0.5:  # 性能下降
                self.trigger_learning_rate_adaptation()

    def trigger_adaptation(self):
        """触发网络适应"""
        print("检测到环境变化，触发网络适应...")

        # 增加学习率
        for param_group in self.optimizer.param_groups:
            param_group['lr'] *= 1.5

        # 增加探索率
        self.adaptation_rate = min(0.3, self.adaptation_rate * 1.2)

    def trigger_learning_rate_adaptation(self):
        """触发学习率适应"""
        print("检测到性能下降，调整学习策略...")

        # 调整学习率
        for param_group in self.optimizer.param_groups:
            param_group['lr'] *= 0.8

    def train_specialized_networks(self, experiences_by_type):
        """训练专门化网络"""
        for env_type, experiences in experiences_by_type.items():
            if env_type in self.specialized_networks and len(experiences) > 32:
                network = self.specialized_networks[env_type]
                self.train_network(network, experiences)

    def train_network(self, network, experiences):
        """训练网络"""
        # 简化的训练过程
        batch = random.sample(experiences, min(32, len(experiences)))
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.BoolTensor(dones)

        current_q_values = network(states).gather(1, actions.unsqueeze(1))
        next_q_values = network(next_states).max(1)[0].detach()
        target_q_values = rewards + (0.99 * next_q_values * ~dones)

        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)

        optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

def test_dynamic_environment_adaptation():
    """测试动态环境适应"""

    print("=== 动态环境适应性测试 ===")

    # 创建动态环境
    env = DynamicEnvironment(map_size=(15, 15))

    # 创建自适应智能体
    obs_dim = 20  # 简化观测维度
    action_dim = 5
    agent = AdaptiveNavigationAgent(obs_dim, action_dim)

    # 测试参数
    num_episodes = 100
    max_steps_per_episode = 200

    # 记录数据
    adaptation_events = []
    performance_metrics = []
    environment_changes = []

    for episode in range(num_episodes):
        state = env.reset()
        episode_performance = []

        for step in range(max_steps_per_episode):
            # 模拟观测（简化）
            observation = np.random.randn(obs_dim)

            # 选择动作
            action = agent.select_action(observation, epsilon=0.1)

            # 环境步进
            next_state = env.step(dt=1.0)

            # 计算性能指标
            performance = np.random.random()  # 简化的性能指标
            episode_performance.append(performance)

            # 智能体适应
            agent.adapt_to_environment(observation, 0, performance)

            # 记录环境变化
            if len(next_state['environmental_changes']) > 0:
                environment_changes.append(episode * max_steps_per_episode + step)

            # 可视化（每50步）
            if step % 50 == 0:
                env.render()

        # 记录回合性能
        avg_episode_performance = np.mean(episode_performance)
        performance_metrics.append(avg_episode_performance)

        if episode % 10 == 0:
            print(f"Episode {episode}: 平均性能 = {avg_episode_performance:.3f}")

    # 分析结果
    print("\n=== 适应性分析 ===")
    print(f"总环境变化次数: {len(environment_changes)}")
    print(f"平均性能: {np.mean(performance_metrics):.3f}")
    print(f"性能标准差: {np.std(performance_metrics):.3f}")

    # 可视化结果
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 性能趋势
    axes[0, 0].plot(performance_metrics, linewidth=2)
    for change_point in environment_changes:
        episode_point = change_point // max_steps_per_episode
        if episode_point < len(performance_metrics):
            axes[0, 0].axvline(episode_point, color='red', alpha=0.5, linestyle='--')

    axes[0, 0].set_title('性能变化与环境变化')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('性能指标')
    axes[0, 0].grid(True)

    # 适应事件时间线
    adaptation_timeline = [i for i, perf in enumerate(performance_metrics) if perf < 0.3]
    axes[0, 1].scatter(adaptation_timeline, [0.5] * len(adaptation_timeline), c='orange', s=50)
    axes[0, 1].set_title('适应事件时间线')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('适应触发')

    # 环境类型分布
    if agent.environment_type_history:
        env_types = list(agent.environment_type_history)
        type_counts = {t: env_types.count(t) for t in set(env_types)}
        axes[1, 0].bar(type_counts.keys(), type_counts.values())
        axes[1, 0].set_title('环境类型分布')
        axes[1, 0].set_ylabel('出现次数')

    # 性能分布
    axes[1, 1].hist(performance_metrics, bins=20, alpha=0.7, edgecolor='black')
    axes[1, 1].axvline(np.mean(performance_metrics), color='red', linestyle='--', label='平均值')
    axes[1, 1].set_title('性能分布')
    axes[1, 1].set_xlabel('性能指标')
    axes[1, 1].set_ylabel('频次')
    axes[1, 1].legend()

    plt.tight_layout()
    plt.savefig('dynamic_environment_adaptation_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return agent, env

if __name__ == "__main__":
    adaptive_agent, dynamic_env = test_dynamic_environment_adaptation()
    print("动态环境适应性测试完成！")
```

## 2. 深度学习集成

### 2.1 端到端深度强化学习

#### 理论基础

**Actor-Critic with Experience Replay (ACER)**：
结合on-policy和off-policy学习的优势：

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{\theta_{old}}}[\rho_t \nabla_\theta \log \pi_\theta(a_t|s_t) Q^{ret}(s_t, a_t)]$$

其中重要性权重：$\rho_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$

**Proximal Policy Optimization (PPO)**：
使用代理目标函数避免大的策略更新：

$$L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$$

#### 实现代码

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical
import numpy as np
import matplotlib.pyplot as plt
from collections import deque

class AttentionModule(nn.Module):
    """注意力机制模块"""

    def __init__(self, input_dim, attention_dim=64):
        super(AttentionModule, self).__init__()
        self.input_dim = input_dim
        self.attention_dim = attention_dim

        self.query = nn.Linear(input_dim, attention_dim)
        self.key = nn.Linear(input_dim, attention_dim)
        self.value = nn.Linear(input_dim, attention_dim)

        self.scale = np.sqrt(attention_dim)

    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        batch_size, seq_len, _ = x.size()

        Q = self.query(x)  # (batch_size, seq_len, attention_dim)
        K = self.key(x)    # (batch_size, seq_len, attention_dim)
        V = self.value(x)  # (batch_size, seq_len, attention_dim)

        # 注意力权重
        attention_weights = torch.softmax(torch.bmm(Q, K.transpose(1, 2)) / self.scale, dim=-1)

        # 加权求和
        attended_output = torch.bmm(attention_weights, V)

        return attended_output, attention_weights

class ConvolutionalEncoder(nn.Module):
    """卷积编码器（用于处理2D环境信息）"""

    def __init__(self, input_channels=1, output_dim=256):
        super(ConvolutionalEncoder, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((4, 4))
        )

        self.fc = nn.Sequential(
            nn.Linear(128 * 4 * 4, 512),
            nn.ReLU(),
            nn.Linear(512, output_dim),
            nn.ReLU()
        )

    def forward(self, x):
        # x shape: (batch_size, channels, height, width)
        conv_out = self.conv_layers(x)
        flattened = conv_out.view(conv_out.size(0), -1)
        encoded = self.fc(flattened)
        return encoded

class MultiModalFusionNetwork(nn.Module):
    """多模态融合网络"""

    def __init__(self, visual_dim=256, sensor_dim=20, fusion_dim=128):
        super(MultiModalFusionNetwork, self).__init__()

        # 视觉特征处理
        self.visual_processor = nn.Sequential(
            nn.Linear(visual_dim, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # 传感器特征处理
        self.sensor_processor = nn.Sequential(
            nn.Linear(sensor_dim, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # 融合层
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.ReLU(),
            nn.Linear(fusion_dim, fusion_dim)
        )

        # 注意力机制
        self.attention = nn.MultiheadAttention(fusion_dim, num_heads=4, batch_first=True)

    def forward(self, visual_features, sensor_features):
        # 处理各模态特征
        visual_processed = self.visual_processor(visual_features)
        sensor_processed = self.sensor_processor(sensor_features)

        # 特征融合
        fused_features = torch.cat([visual_processed, sensor_processed], dim=-1)
        fused_output = self.fusion_layer(fused_features)

        # 自注意力
        attended_output, attention_weights = self.attention(
            fused_output.unsqueeze(1), fused_output.unsqueeze(1), fused_output.unsqueeze(1)
        )

        return attended_output.squeeze(1), attention_weights

class EndToEndNavigationNetwork(nn.Module):
    """端到端导航网络"""

    def __init__(self, map_size=(20, 20), sensor_dim=20, action_dim=5):
        super(EndToEndNavigationNetwork, self).__init__()

        self.map_size = map_size
        self.sensor_dim = sensor_dim
        self.action_dim = action_dim

        # 视觉编码器
        self.visual_encoder = ConvolutionalEncoder(input_channels=1, output_dim=256)

        # 多模态融合
        self.fusion_network = MultiModalFusionNetwork(visual_dim=256, sensor_dim=sensor_dim)

        # 时间建模
        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=2, batch_first=True)

        # 策略网络（Actor）
        self.actor = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

        # 值网络（Critic）
        self.critic = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

        # LSTM隐藏状态
        self.hidden_state = None

    def forward(self, visual_input, sensor_input, hidden_state=None):
        batch_size = visual_input.size(0)

        # 视觉特征提取
        visual_features = self.visual_encoder(visual_input)

        # 多模态融合
        fused_features, attention_weights = self.fusion_network(visual_features, sensor_input)

        # 时间建模
        if hidden_state is None:
            hidden_state = self.init_hidden_state(batch_size)

        lstm_input = fused_features.unsqueeze(1)  # (batch_size, seq_len=1, feature_dim)
        lstm_output, new_hidden_state = self.lstm(lstm_input, hidden_state)
        lstm_output = lstm_output.squeeze(1)  # (batch_size, feature_dim)

        # 策略和值估计
        action_probs = self.actor(lstm_output)
        state_value = self.critic(lstm_output)

        return action_probs, state_value, new_hidden_state, attention_weights

    def init_hidden_state(self, batch_size):
        """初始化LSTM隐藏状态"""
        device = next(self.parameters()).device
        h0 = torch.zeros(2, batch_size, 128).to(device)
        c0 = torch.zeros(2, batch_size, 128).to(device)
        return (h0, c0)

class PPOAgent:
    """PPO智能体"""

    def __init__(self, map_size=(20, 20), sensor_dim=20, action_dim=5):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 网络
        self.network = EndToEndNavigationNetwork(map_size, sensor_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=3e-4)

        # PPO参数
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_epsilon = 0.2
        self.value_loss_coef = 0.5
        self.entropy_coef = 0.01

        # 经验存储
        self.memory = {
            'visual_states': [],
            'sensor_states': [],
            'actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': [],
            'hidden_states': []
        }

    def select_action(self, visual_state, sensor_state, hidden_state=None):
        """选择动作"""
        with torch.no_grad():
            visual_tensor = torch.FloatTensor(visual_state).unsqueeze(0).to(self.device)
            sensor_tensor = torch.FloatTensor(sensor_state).unsqueeze(0).to(self.device)

            action_probs, state_value, new_hidden_state, attention_weights = self.network(
                visual_tensor, sensor_tensor, hidden_state
            )

            # 采样动作
            dist = Categorical(action_probs)
            action = dist.sample()
            log_prob = dist.log_prob(action)

            return (action.item(), log_prob.item(), state_value.item(),
                   new_hidden_state, attention_weights.squeeze().cpu().numpy())

    def store_transition(self, visual_state, sensor_state, action, reward, value, log_prob, done, hidden_state):
        """存储经验"""
        self.memory['visual_states'].append(visual_state)
        self.memory['sensor_states'].append(sensor_state)
        self.memory['actions'].append(action)
        self.memory['rewards'].append(reward)
        self.memory['values'].append(value)
        self.memory['log_probs'].append(log_prob)
        self.memory['dones'].append(done)
        self.memory['hidden_states'].append(hidden_state)

    def compute_gae(self, rewards, values, dones):
        """计算广义优势估计"""
        advantages = []
        returns = []

        advantage = 0
        next_value = 0

        for i in reversed(range(len(rewards))):
            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
            advantage = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * advantage

            advantages.insert(0, advantage)
            returns.insert(0, advantage + values[i])

            next_value = values[i]

        return advantages, returns

    def update(self):
        """PPO更新"""
        if len(self.memory['rewards']) == 0:
            return

        # 计算优势和回报
        advantages, returns = self.compute_gae(
            self.memory['rewards'],
            self.memory['values'],
            self.memory['dones']
        )

        # 转换为张量
        visual_states = torch.FloatTensor(self.memory['visual_states']).to(self.device)
        sensor_states = torch.FloatTensor(self.memory['sensor_states']).to(self.device)
        actions = torch.LongTensor(self.memory['actions']).to(self.device)
        old_log_probs = torch.FloatTensor(self.memory['log_probs']).to(self.device)
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)

        # 标准化优势
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # PPO更新
        for _ in range(4):  # PPO epochs
            # 前向传播
            action_probs, values, _, _ = self.network(visual_states, sensor_states)

            # 计算策略损失
            dist = Categorical(action_probs)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()

            ratio = torch.exp(new_log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # 计算值损失
            value_loss = F.mse_loss(values.squeeze(), returns)

            # 总损失
            total_loss = policy_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy

            # 反向传播
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)
            self.optimizer.step()

        # 清空记忆
        for key in self.memory:
            self.memory[key].clear()

        return policy_loss.item(), value_loss.item(), entropy.item()

class EndToEndEnvironment:
    """端到端学习环境"""

    def __init__(self, map_size=(20, 20)):
        self.map_size = map_size
        self.reset()

    def reset(self):
        """重置环境"""
        # 生成随机地图
        self.grid = np.zeros(self.map_size)

        # 添加障碍物
        num_obstacles = np.random.randint(5, 15)
        for _ in range(num_obstacles):
            x = np.random.randint(0, self.map_size[0])
            y = np.random.randint(0, self.map_size[1])
            self.grid[x, y] = 1

        # 机器人初始位置
        while True:
            self.robot_pos = [np.random.randint(0, self.map_size[0]),
                             np.random.randint(0, self.map_size[1])]
            if self.grid[self.robot_pos[0], self.robot_pos[1]] == 0:
                break

        # 目标位置
        while True:
            self.goal_pos = [np.random.randint(0, self.map_size[0]),
                           np.random.randint(0, self.map_size[1])]
            if (self.grid[self.goal_pos[0], self.goal_pos[1]] == 0 and
                abs(self.goal_pos[0] - self.robot_pos[0]) + abs(self.goal_pos[1] - self.robot_pos[1]) > 5):
                break

        self.steps = 0
        self.max_steps = 200

        return self.get_observation()

    def get_observation(self):
        """获取观测"""
        # 视觉观测（局部地图）
        visual_obs = self.get_local_map()

        # 传感器观测
        sensor_obs = self.get_sensor_data()

        return visual_obs, sensor_obs

    def get_local_map(self, radius=5):
        """获取局部地图"""
        local_map = np.zeros((2 * radius + 1, 2 * radius + 1))

        robot_x, robot_y = self.robot_pos
        goal_x, goal_y = self.goal_pos

        for i in range(-radius, radius + 1):
            for j in range(-radius, radius + 1):
                map_x = robot_x + i
                map_y = robot_y + j

                local_i = i + radius
                local_j = j + radius

                if 0 <= map_x < self.map_size[0] and 0 <= map_y < self.map_size[1]:
                    if map_x == goal_x and map_y == goal_y:
                        local_map[local_i, local_j] = 0.7  # 目标
                    elif self.grid[map_x, map_y] == 1:
                        local_map[local_i, local_j] = 1.0  # 障碍物
                    else:
                        local_map[local_i, local_j] = 0.0  # 自由空间
                else:
                    local_map[local_i, local_j] = 1.0  # 边界视为障碍物

        # 标记机器人位置
        local_map[radius, radius] = 0.5

        return local_map

    def get_sensor_data(self):
        """获取传感器数据"""
        robot_x, robot_y = self.robot_pos
        goal_x, goal_y = self.goal_pos

        # 基本状态信息
        sensor_data = []

        # 位置信息（归一化）
        sensor_data.extend([robot_x / self.map_size[0], robot_y / self.map_size[1]])

        # 目标信息
        dx = goal_x - robot_x
        dy = goal_y - robot_y
        distance = np.sqrt(dx**2 + dy**2)
        angle = np.arctan2(dy, dx)

        sensor_data.extend([
            distance / np.sqrt(self.map_size[0]**2 + self.map_size[1]**2),
            np.sin(angle),
            np.cos(angle)
        ])

        # 激光雷达模拟
        laser_ranges = []
        angles = np.linspace(0, 2*np.pi, 8, endpoint=False)

        for angle in angles:
            max_range = 5
            for r in range(1, max_range + 1):
                check_x = robot_x + int(r * np.cos(angle))
                check_y = robot_y + int(r * np.sin(angle))

                if (check_x < 0 or check_x >= self.map_size[0] or
                    check_y < 0 or check_y >= self.map_size[1] or
                    self.grid[check_x, check_y] == 1):
                    laser_ranges.append(r / max_range)
                    break
            else:
                laser_ranges.append(1.0)

        sensor_data.extend(laser_ranges)

        # 运动状态
        sensor_data.extend([self.steps / self.max_steps])

        # 填充到固定长度
        while len(sensor_data) < 20:
            sensor_data.append(0.0)

        return np.array(sensor_data[:20], dtype=np.float32)

    def step(self, action):
        """执行动作"""
        # 动作映射
        actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (0, 0)]  # 上下左右停止
        dx, dy = actions[action]

        new_x = self.robot_pos[0] + dx
        new_y = self.robot_pos[1] + dy

        # 碰撞检测
        if (0 <= new_x < self.map_size[0] and 0 <= new_y < self.map_size[1] and
            self.grid[new_x, new_y] == 0):
            self.robot_pos = [new_x, new_y]
            collision = False
        else:
            collision = True

        self.steps += 1

        # 计算奖励
        reward = self.calculate_reward(collision)

        # 检查终止条件
        done = self.is_done()

        return self.get_observation(), reward, done

    def calculate_reward(self, collision):
        """计算奖励"""
        robot_x, robot_y = self.robot_pos
        goal_x, goal_y = self.goal_pos

        # 距离奖励
        distance = np.sqrt((goal_x - robot_x)**2 + (goal_y - robot_y)**2)
        distance_reward = -distance * 0.1

        # 到达目标
        if distance < 1.0:
            arrival_reward = 100
        else:
            arrival_reward = 0

        # 碰撞惩罚
        collision_penalty = -10 if collision else 0

        # 时间惩罚
        time_penalty = -0.1

        return distance_reward + arrival_reward + collision_penalty + time_penalty

    def is_done(self):
        """检查是否结束"""
        # 到达目标
        distance = np.sqrt((self.goal_pos[0] - self.robot_pos[0])**2 +
                          (self.goal_pos[1] - self.robot_pos[1])**2)
        if distance < 1.0:
            return True

        # 超时
        if self.steps >= self.max_steps:
            return True

        return False

    def render(self):
        """可视化"""
        display_grid = self.grid.copy()
        display_grid[self.robot_pos[0], self.robot_pos[1]] = 0.5
        display_grid[self.goal_pos[0], self.goal_pos[1]] = 0.7

        plt.imshow(display_grid, cmap='RdYlBu', origin='lower')
        plt.title(f'步数: {self.steps}')
        plt.colorbar()
        plt.show()

def train_end_to_end_navigation():
    """训练端到端导航"""

    print("=== 端到端深度强化学习导航训练 ===")

    # 创建环境和智能体
    env = EndToEndEnvironment(map_size=(15, 15))
    agent = PPOAgent(map_size=(15, 15), sensor_dim=20, action_dim=5)

    print(f"使用设备: {agent.device}")

    # 训练参数
    num_episodes = 2000
    update_interval = 20  # 每20个episode更新一次

    # 记录数据
    episode_rewards = []
    episode_lengths = []
    success_rates = []
    attention_weights_history = []

    for episode in range(num_episodes):
        visual_obs, sensor_obs = env.reset()
        hidden_state = None
        episode_reward = 0
        episode_attention = []

        for step in range(env.max_steps):
            # 选择动作
            action, log_prob, value, hidden_state, attention_weights = agent.select_action(
                visual_obs, sensor_obs, hidden_state
            )

            # 执行动作
            next_obs, reward, done = env.step(action)
            next_visual_obs, next_sensor_obs = next_obs

            # 存储经验
            agent.store_transition(visual_obs, sensor_obs, action, reward, value, log_prob, done, hidden_state)

            visual_obs, sensor_obs = next_visual_obs, next_sensor_obs
            episode_reward += reward
            episode_attention.append(attention_weights)

            if done:
                break

        # 记录数据
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
        attention_weights_history.append(np.mean(episode_attention, axis=0) if episode_attention else np.zeros(4))

        # 计算成功率
        if episode >= 99:
            recent_rewards = episode_rewards[-100:]
            success_count = sum(1 for r in recent_rewards if r > 50)
            success_rates.append(success_count / 100)

        # 更新网络
        if episode % update_interval == 0 and episode > 0:
            policy_loss, value_loss, entropy = agent.update()

            if episode % 200 == 0:
                print(f"Episode {episode}:")
                print(f"  平均奖励: {np.mean(episode_rewards[-100:]):.2f}")
                print(f"  平均长度: {np.mean(episode_lengths[-100:]):.1f}")
                if success_rates:
                    print(f"  成功率: {success_rates[-1]:.2%}")
                print(f"  策略损失: {policy_loss:.4f}")
                print(f"  值损失: {value_loss:.4f}")
                print(f"  熵: {entropy:.4f}")

    # 可视化训练结果
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # 奖励曲线
    axes[0, 0].plot(episode_rewards, alpha=0.3)
    axes[0, 0].plot(np.convolve(episode_rewards, np.ones(100)/100, mode='valid'), linewidth=2)
    axes[0, 0].set_title('训练奖励曲线')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('奖励')
    axes[0, 0].grid(True)

    # 回合长度
    axes[0, 1].plot(episode_lengths, alpha=0.3)
    axes[0, 1].plot(np.convolve(episode_lengths, np.ones(100)/100, mode='valid'), linewidth=2)
    axes[0, 1].set_title('回合长度')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('步数')
    axes[0, 1].grid(True)

    # 成功率
    if success_rates:
        axes[0, 2].plot(success_rates, linewidth=2)
        axes[0, 2].set_title('成功率')
        axes[0, 2].set_xlabel('Episode (滑动窗口)')
        axes[0, 2].set_ylabel('成功率')
        axes[0, 2].grid(True)

    # 注意力权重可视化
    if attention_weights_history:
        attention_weights = np.array(attention_weights_history[-500:])  # 最近500个episode
        avg_attention = np.mean(attention_weights, axis=0)

        axes[1, 0].bar(range(len(avg_attention)), avg_attention)
        axes[1, 0].set_title('平均注意力权重')
        axes[1, 0].set_xlabel('注意力头')
        axes[1, 0].set_ylabel('权重')

        # 注意力权重随时间变化
        axes[1, 1].plot(attention_weights[:, 0], label='头1', alpha=0.7)
        axes[1, 1].plot(attention_weights[:, 1], label='头2', alpha=0.7)
        axes[1, 1].plot(attention_weights[:, 2], label='头3', alpha=0.7)
        axes[1, 1].plot(attention_weights[:, 3], label='头4', alpha=0.7)
        axes[1, 1].set_title('注意力权重演化')
        axes[1, 1].set_xlabel('Episode')
        axes[1, 1].set_ylabel('权重')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

    # 演示一个测试回合
    visual_obs, sensor_obs = env.reset()
    hidden_state = None
    trajectory = [env.robot_pos.copy()]
    attention_map = []

    for step in range(50):
        action, _, _, hidden_state, attention_weights = agent.select_action(
            visual_obs, sensor_obs, hidden_state
        )
        next_obs, reward, done = env.step(action)
        visual_obs, sensor_obs = next_obs

        trajectory.append(env.robot_pos.copy())
        attention_map.append(attention_weights)

        if done:
            break

    # 绘制测试轨迹
    trajectory = np.array(trajectory)
    axes[1, 2].imshow(env.grid, cmap='Greys', origin='lower', alpha=0.3)
    axes[1, 2].plot(trajectory[:, 1], trajectory[:, 0], 'b-', linewidth=3, alpha=0.7)
    axes[1, 2].scatter(trajectory[0, 1], trajectory[0, 0], c='green', s=100, marker='o', label='起点')
    axes[1, 2].scatter(env.goal_pos[1], env.goal_pos[0], c='red', s=100, marker='*', label='目标')
    axes[1, 2].set_title('学习后测试轨迹')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('end_to_end_training_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    return agent, env

if __name__ == "__main__":
    trained_agent, test_env = train_end_to_end_navigation()
    print("端到端深度强化学习导航训练完成！")
```

## 3. 前沿研究方向

### 3.1 模仿学习与专家演示

#### 理论基础

**Behavioral Cloning**：
直接从专家演示学习策略：
$$\min_\theta \mathbb{E}_{(s,a) \sim \mathcal{D}_{expert}}[L(\pi_\theta(s), a)]$$

**Inverse Reinforcement Learning (IRL)**：
从演示中学习奖励函数：
$$\max_R \min_\pi [\mathbb{E}_{\pi^*}[R(s,a)] - \mathbb{E}_{\pi}[R(s,a)]]$$

**Generative Adversarial Imitation Learning (GAIL)**：
使用对抗训练进行模仿学习：
$$\min_{\pi} \max_{D} \mathbb{E}_{\pi}[\log D(s,a)] + \mathbb{E}_{\pi_E}[\log(1-D(s,a))]$$

### 3.2 量子传感器融合

#### 理论基础

**量子增强传感**：
利用量子纠缠和压缩态提高测量精度：
$$\Delta \phi \geq \frac{1}{\sqrt{N}} \rightarrow \Delta \phi \geq \frac{1}{N}$$

从标准量子极限提升到海森堡极限。

**量子卡尔曼滤波**：
在量子测量框架下的状态估计：
$$|\psi\rangle_{k|k} = \frac{\hat{M}|\psi\rangle_{k|k-1}}{\sqrt{\langle\psi|_{k|k-1}\hat{M}^\dagger\hat{M}|\psi\rangle_{k|k-1}}}$$

### 3.3 神经符号融合

#### 理论基础

**Neural-Symbolic Integration**：
结合神经网络的学习能力和符号推理的可解释性：

$$\text{Knowledge} + \text{Data} \rightarrow \text{Learned Representation}$$

**Graph Neural Networks for Navigation**：
使用图神经网络建模环境拓扑结构：
$$h_v^{(l+1)} = \sigma\left(\sum_{u \in \mathcal{N}(v)} W^{(l)} h_u^{(l)} + b^{(l)}\right)$$

### 3.4 分布式协作SLAM

#### 理论基础

**Distributed SLAM**：
多机器人协作建图与定位：
$$\max p(\mathbf{x}_{1:T}^{1:N}, \mathbf{m} | \mathbf{z}_{1:T}^{1:N}, \mathbf{u}_{1:T}^{1:N})$$

其中$N$是机器人数量，$\mathbf{x}$是轨迹，$\mathbf{m}$是地图。

**Consensus-based Map Merging**：
使用一致性算法合并局部地图：
$$\mathbf{m}_i^{(k+1)} = \mathbf{m}_i^{(k)} + \epsilon \sum_{j \in \mathcal{N}_i} (\mathbf{m}_j^{(k)} - \mathbf{m}_i^{(k)})$$

## 4. 研究项目建议

### 4.1 短期项目（2-3个月）

1. **自适应传感器融合**：
   - 实现在线噪声参数估计
   - 开发传感器故障检测算法
   - 验证在不同环境条件下的性能

2. **多模态深度强化学习**：
   - 结合视觉、激光雷达和IMU数据
   - 设计端到端学习架构
   - 在仿真环境中验证效果

3. **人机协作导航**：
   - 实现人类演示的学习算法
   - 开发交互式策略修正机制
   - 评估人机协作效率

### 4.2 中期项目（6-12个月）

1. **大规模多机器人系统**：
   - 扩展到10+机器人协作
   - 研究通信约束下的协调策略
   - 开发分布式学习算法

2. **复杂动态环境导航**：
   - 处理移动障碍物和行人
   - 实现实时路径重规划
   - 集成预测模型

3. **跨域迁移学习**：
   - 从仿真到现实的迁移
   - 不同环境间的知识迁移
   - 少样本学习算法

### 4.3 长期项目（1-2年）

1. **通用智能导航系统**：
   - 支持多种机器人平台
   - 适应各种环境和任务
   - 具备持续学习能力

2. **量子增强机器人感知**：
   - 研究量子传感器应用
   - 开发量子滤波算法
   - 验证量子优势

3. **神经符号导航架构**：
   - 结合深度学习和符号推理
   - 实现可解释的导航决策
   - 支持复杂任务规划

## 5. 学术资源与发展建议

### 5.1 重要会议和期刊

**顶级会议**：
- ICRA (International Conference on Robotics and Automation)
- IROS (Intelligent Robots and Systems)
- NeurIPS (Neural Information Processing Systems)
- ICML (International Conference on Machine Learning)
- RSS (Robotics: Science and Systems)

**重要期刊**：
- IEEE Transactions on Robotics
- International Journal of Robotics Research
- Autonomous Robots
- IEEE Transactions on Pattern Analysis and Machine Intelligence
- Journal of Machine Learning Research

### 5.2 开源资源

**机器人框架**：
- ROS (Robot Operating System)
- Drake (Manipulation Planning)
- OMPL (Open Motion Planning Library)
- MoveIt (Motion Planning Framework)

**强化学习库**：
- Stable-Baselines3
- Ray RLlib
- OpenAI Gym
- PyBullet

**传感器融合**：
- GTSAM (Georgia Tech Smoothing and Mapping)
- Kalibr (Camera-IMU Calibration)
- Robot Localization (ROS package)

### 5.3 研究方向发展建议

1. **基础理论掌握**：
   - 深入学习概率论和统计学
   - 掌握最优化理论
   - 理解机器学习基础

2. **实践技能培养**：
   - 熟练使用ROS和相关工具
   - 掌握深度学习框架
   - 具备硬件调试能力

3. **跨学科知识**：
   - 了解控制理论
   - 学习计算机视觉
   - 掌握信号处理

4. **科研能力发展**：
   - 培养问题发现能力
   - 提高实验设计水平
   - 加强论文写作技能

通过系统学习本课程内容，学生将具备在机器人导航、传感器融合和强化学习交叉领域进行深入研究的能力，为未来的学术研究或工业应用奠定坚实基础。