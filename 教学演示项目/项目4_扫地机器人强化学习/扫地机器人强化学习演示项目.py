#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®4ï¼šæ‰«åœ°æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ¼”ç¤ºé¡¹ç›®
===============================

æœ¬é¡¹ç›®æ¼”ç¤ºäº†ä¸€ä¸ªå®Œæ•´çš„æ‰«åœ°æœºå™¨äººå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š
1. æˆ¿é—´ç¯å¢ƒå»ºæ¨¡ï¼ˆå¸¦æœ‰ç°å°˜åˆ†å¸ƒï¼‰
2. æ‰«åœ°æœºå™¨äººæ™ºèƒ½ä½“è®¾è®¡
3. æ¸…æ‰«è·¯å¾„è§„åˆ’è®­ç»ƒ
4. æ¯æ¬¡æ¸…æ‰«æ´»åŠ¨è·¯çº¿å¯è§†åŒ–
5. æ¸…æ‰«æ•ˆç‡åˆ†æå’Œç»“æœä¿å­˜

ä¸»è¦ç‰¹ç‚¹ï¼š
- åŠ¨æ€ç°å°˜ç”Ÿæˆå’Œæ¸…ç†æœºåˆ¶
- å®æ—¶è·¯å¾„è¿½è¸ªå’Œå¯è§†åŒ–
- ç”µæ± ç”µé‡ç®¡ç†
- å……ç”µç­–ç•¥å­¦ä¹ 
- æ¸…æ‰«è¦†ç›–ç‡ä¼˜åŒ–

ä½œè€…ï¼šæœºå™¨äººè¯¾ç¨‹å›¢é˜Ÿ
æ—¥æœŸï¼š2025å¹´
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import json
import os
import time
from datetime import datetime
import matplotlib
import platform
import matplotlib.font_manager as fm

# è®¾ç½®ä¸­æ–‡å­—ä½“ - å¤šé‡å¤‡ç”¨æ–¹æ¡ˆ
def setup_chinese_fonts():
    """è®¾ç½®ä¸­æ–‡å­—ä½“çš„å‡½æ•°"""
    # é¦–å…ˆå°è¯•è®¾ç½®ç³»ç»Ÿç‰¹å®šçš„å­—ä½“
    if platform.system() == 'Darwin':  # macOS
        font_candidates = ['Arial Unicode MS', 'PingFang SC', 'Helvetica', 'DejaVu Sans']
    elif platform.system() == 'Windows':
        font_candidates = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'DejaVu Sans']
    else:  # Linux
        font_candidates = ['DejaVu Sans', 'WenQuanYi Micro Hei', 'SimHei', 'Liberation Sans']

    # æ£€æŸ¥å¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]

    # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„å­—ä½“
    selected_font = 'DejaVu Sans'  # é»˜è®¤å­—ä½“
    for font in font_candidates:
        if font in available_fonts:
            selected_font = font
            break

    # è®¾ç½®å­—ä½“
    plt.rcParams['font.sans-serif'] = [selected_font] + font_candidates
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 10

    print(f"å­—ä½“è®¾ç½®å®Œæˆï¼Œä½¿ç”¨å­—ä½“: {selected_font}")
    return selected_font

# è°ƒç”¨å­—ä½“è®¾ç½®å‡½æ•°
try:
    setup_chinese_fonts()
except Exception as e:
    print(f"å­—ä½“è®¾ç½®è­¦å‘Š: {e}")
    # ä½¿ç”¨åŸºæœ¬è®¾ç½®ä½œä¸ºå¤‡ç”¨
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®å›¾è¡¨æ ·å¼
sns.set_style("whitegrid")
matplotlib.rcParams['figure.figsize'] = [12, 8]
matplotlib.rcParams['figure.dpi'] = 100

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

class VacuumCleanerEnvironment:
    """
    æ‰«åœ°æœºå™¨äººç¯å¢ƒ

    ç‰¹ç‚¹ï¼š
    - æˆ¿é—´åœ°å›¾ä¸ç°å°˜åˆ†å¸ƒ
    - å……ç”µç«™ä½ç½®
    - ç”µæ± ç”µé‡ç³»ç»Ÿ
    - åŠ¨æ€ç°å°˜ç”Ÿæˆ
    - éšœç¢ç‰©åˆ†å¸ƒ
    """

    def __init__(self, room_size=15, num_obstacles=8, dirt_density=0.3, max_battery=100):
        self.room_size = room_size
        self.num_obstacles = num_obstacles
        self.dirt_density = dirt_density
        self.max_battery = max_battery

        # æˆ¿é—´çŠ¶æ€
        self.room_map = np.zeros((room_size, room_size))  # 0=ç©ºåœ°, 1=éšœç¢ç‰©, 2=å……ç”µç«™
        self.dirt_map = np.zeros((room_size, room_size))  # ç°å°˜å¯†åº¦ 0-1
        self.visited_map = np.zeros((room_size, room_size))  # è®¿é—®è®°å½•

        # å……ç”µç«™ä½ç½®ï¼ˆå›ºå®šåœ¨è§’è½ï¼‰
        self.charging_station = (0, 0)
        self.room_map[self.charging_station] = 2

        # ç”Ÿæˆæˆ¿é—´å¸ƒå±€
        self.generate_room_layout()
        self.generate_initial_dirt()

        # æ‰«åœ°æœºå™¨äººçŠ¶æ€
        self.robot_pos = list(self.charging_station)
        self.battery_level = self.max_battery
        self.is_cleaning = True
        self.dirt_collected = 0

        # åŠ¨ä½œç©ºé—´ï¼šä¸Šä¸‹å·¦å³ç§»åŠ¨ + å¼€å§‹/åœæ­¢æ¸…æ‰« + è¿”å›å……ç”µ
        self.actions = [
            (0, 1),   # ä¸Š
            (0, -1),  # ä¸‹
            (1, 0),   # å³
            (-1, 0),  # å·¦
            (0, 0),   # åŸåœ°æ¸…æ‰«
        ]
        self.action_names = ['ä¸Šç§»', 'ä¸‹ç§»', 'å³ç§»', 'å·¦ç§»', 'æ¸…æ‰«']

        # è·¯å¾„è®°å½•
        self.path_history = [self.robot_pos.copy()]
        self.action_history = []
        self.battery_history = [self.battery_level]
        self.dirt_collected_history = [0]

        # æ€§èƒ½ç»Ÿè®¡
        self.episode_stats = {
            'total_dirt_collected': 0,
            'coverage_rate': 0,
            'energy_efficiency': 0,
            'cleaning_time': 0
        }

    def generate_room_layout(self):
        """ç”Ÿæˆæˆ¿é—´å¸ƒå±€"""
        # æ·»åŠ è¾¹ç•Œå¢™ï¼ˆé™¤äº†å……ç”µç«™ä½ç½®ï¼‰
        self.room_map[0, 1:] = 1  # ä¸Šè¾¹ç•Œ
        self.room_map[-1, :] = 1  # ä¸‹è¾¹ç•Œ
        self.room_map[1:, 0] = 1  # å·¦è¾¹ç•Œ
        self.room_map[:, -1] = 1  # å³è¾¹ç•Œ

        # ç¡®ä¿å……ç”µç«™å‘¨å›´æœ‰ç©ºé—´
        self.room_map[0, 0] = 2  # å……ç”µç«™
        self.room_map[0, 1] = 0  # å……ç”µç«™å³ä¾§
        self.room_map[1, 0] = 0  # å……ç”µç«™ä¸‹æ–¹

        # éšæœºæ·»åŠ å†…éƒ¨éšœç¢ç‰©ï¼ˆå®¶å…·ï¼‰
        for _ in range(self.num_obstacles):
            while True:
                x, y = np.random.randint(2, self.room_size-2, 2)
                # ç¡®ä¿ä¸åœ¨å……ç”µç«™é™„è¿‘å’Œä¸é˜»å¡è·¯å¾„
                if (self.room_map[x, y] == 0 and
                    np.sqrt((x-0)**2 + (y-0)**2) > 3):
                    self.room_map[x, y] = 1
                    break

    def generate_initial_dirt(self):
        """ç”Ÿæˆåˆå§‹ç°å°˜åˆ†å¸ƒ"""
        # åœ¨å¯æ¸…æ‰«åŒºåŸŸéšæœºåˆ†å¸ƒç°å°˜
        for i in range(self.room_size):
            for j in range(self.room_size):
                if self.room_map[i, j] == 0:  # åªåœ¨ç©ºåœ°ä¸Šæœ‰ç°å°˜
                    # ä½¿ç”¨betaåˆ†å¸ƒç”Ÿæˆæ›´çœŸå®çš„ç°å°˜åˆ†å¸ƒ
                    if np.random.random() < self.dirt_density:
                        self.dirt_map[i, j] = np.random.beta(2, 5)  # åå‘å°‘é‡ç°å°˜

    def add_random_dirt(self, amount=0.1):
        """åŠ¨æ€æ·»åŠ æ–°ç°å°˜"""
        dirt_spots = int(self.room_size * self.room_size * amount)
        for _ in range(dirt_spots):
            x, y = np.random.randint(0, self.room_size, 2)
            if self.room_map[x, y] == 0:  # åªåœ¨ç©ºåœ°ä¸Šæ·»åŠ 
                self.dirt_map[x, y] = min(1.0, self.dirt_map[x, y] + np.random.uniform(0.1, 0.3))

    def get_state(self):
        """è·å–å½“å‰çŠ¶æ€"""
        # å±€éƒ¨è§‚æµ‹ï¼šæœºå™¨äººå‘¨å›´5x5åŒºåŸŸ
        local_size = 5
        half_size = local_size // 2

        local_room = np.zeros((local_size, local_size))
        local_dirt = np.zeros((local_size, local_size))
        local_visited = np.zeros((local_size, local_size))

        rx, ry = self.robot_pos

        for i in range(local_size):
            for j in range(local_size):
                world_x = rx - half_size + i
                world_y = ry - half_size + j

                if 0 <= world_x < self.room_size and 0 <= world_y < self.room_size:
                    local_room[i, j] = self.room_map[world_x, world_y]
                    local_dirt[i, j] = self.dirt_map[world_x, world_y]
                    local_visited[i, j] = self.visited_map[world_x, world_y]
                else:
                    local_room[i, j] = 1  # è¾¹ç•Œå¤–è§†ä¸ºéšœç¢ç‰©

        # å…¨å±€ä¿¡æ¯
        battery_ratio = self.battery_level / self.max_battery
        dirt_ratio = np.sum(self.dirt_map) / (self.room_size * self.room_size)
        coverage_ratio = np.sum(self.visited_map > 0) / np.sum(self.room_map == 0)

        # è·ç¦»å……ç”µç«™çš„è·ç¦»
        dist_to_charging = np.sqrt((rx - self.charging_station[0])**2 +
                                 (ry - self.charging_station[1])**2) / self.room_size

        # ç»„åˆçŠ¶æ€
        state = np.concatenate([
            local_room.flatten(),
            local_dirt.flatten(),
            local_visited.flatten(),
            [battery_ratio, dirt_ratio, coverage_ratio, dist_to_charging]
        ])

        return state

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        reward = 0
        done = False

        # æ¶ˆè€—ç”µæ± ï¼ˆç§»åŠ¨æ¯”æ¸…æ‰«è€—ç”µæ›´å¤šï¼‰
        if action < 4:  # ç§»åŠ¨åŠ¨ä½œ
            self.battery_level -= 2
        else:  # æ¸…æ‰«åŠ¨ä½œ
            self.battery_level -= 1

        # æ£€æŸ¥ç”µæ± ç”µé‡
        if self.battery_level <= 0:
            reward -= 50  # ç”µé‡è€—å°½æƒ©ç½š
            done = True

        # æ‰§è¡ŒåŠ¨ä½œ
        if action < 4:  # ç§»åŠ¨åŠ¨ä½œ
            dx, dy = self.actions[action]
            new_x = self.robot_pos[0] + dx
            new_y = self.robot_pos[1] + dy

            # æ£€æŸ¥ç§»åŠ¨æ˜¯å¦æœ‰æ•ˆ
            if (0 <= new_x < self.room_size and 0 <= new_y < self.room_size and
                self.room_map[new_x, new_y] != 1):  # ä¸æ˜¯éšœç¢ç‰©

                self.robot_pos = [new_x, new_y]
                self.visited_map[new_x, new_y] = 1

                # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å……ç”µç«™
                if (new_x, new_y) == self.charging_station:
                    self.battery_level = min(self.max_battery, self.battery_level + 10)
                    reward += 5  # å……ç”µå¥–åŠ±

                # ç§»åŠ¨å¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢æœªè®¿é—®åŒºåŸŸï¼‰
                if self.visited_map[new_x, new_y] == 0:
                    reward += 1  # æ¢ç´¢å¥–åŠ±
                else:
                    reward -= 0.2  # é‡å¤è®¿é—®æƒ©ç½š

            else:
                reward -= 5  # ç¢°æ’æƒ©ç½š

        elif action == 4:  # æ¸…æ‰«åŠ¨ä½œ
            x, y = self.robot_pos
            if self.dirt_map[x, y] > 0:
                # æ¸…æ‰«ç°å°˜
                dirt_amount = self.dirt_map[x, y]
                self.dirt_map[x, y] = max(0, self.dirt_map[x, y] - 0.5)
                collected = dirt_amount - self.dirt_map[x, y]
                self.dirt_collected += collected
                reward += collected * 20  # å¢åŠ æ¸…æ‰«å¥–åŠ±
            else:
                reward -= 1  # æ— æ•ˆæ¸…æ‰«æƒ©ç½š

        # è®°å½•å†å²
        self.path_history.append(self.robot_pos.copy())
        self.action_history.append(action)
        self.battery_history.append(self.battery_level)
        self.dirt_collected_history.append(self.dirt_collected)

        # åŸºç¡€æ—¶é—´æƒ©ç½šï¼ˆé¼“åŠ±æ•ˆç‡ï¼‰
        reward -= 0.1

        # æ£€æŸ¥ä»»åŠ¡å®Œæˆæ¡ä»¶
        total_dirt_remaining = np.sum(self.dirt_map)
        if total_dirt_remaining < 0.1:  # å‡ ä¹æ‰€æœ‰ç°å°˜éƒ½è¢«æ¸…ç†
            reward += 100  # å®Œæˆä»»åŠ¡å¤§å¥–åŠ±
            done = True

        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if len(self.path_history) > self.room_size * 20:
            done = True
            reward -= 20  # è¶…æ—¶æƒ©ç½š

        # åŠ¨æ€æ·»åŠ æ–°ç°å°˜ï¼ˆæ¨¡æ‹Ÿç°å®ä¸­çš„æŒç»­æ±¡æŸ“ï¼‰
        if len(self.path_history) % 50 == 0:  # æ¯50æ­¥æ·»åŠ ä¸€æ¬¡
            self.add_random_dirt(0.02)

        return self.get_state(), reward, done

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        # é‡ç½®æœºå™¨äººçŠ¶æ€
        self.robot_pos = list(self.charging_station)
        self.battery_level = self.max_battery
        self.dirt_collected = 0

        # é‡ç½®åœ°å›¾
        self.visited_map.fill(0)
        self.generate_initial_dirt()

        # é‡ç½®å†å²è®°å½•
        self.path_history = [self.robot_pos.copy()]
        self.action_history = []
        self.battery_history = [self.battery_level]
        self.dirt_collected_history = [0]

        # è®¡ç®—ä¸Šä¸€æ¬¡çš„ç»Ÿè®¡æ•°æ®
        if len(self.path_history) > 1:
            self.episode_stats = {
                'total_dirt_collected': self.dirt_collected,
                'coverage_rate': np.sum(self.visited_map > 0) / np.sum(self.room_map == 0),
                'energy_efficiency': self.dirt_collected / max(1, self.max_battery - self.battery_level),
                'cleaning_time': len(self.path_history)
            }

        return self.get_state()

class VacuumDQN(nn.Module):
    """æ‰«åœ°æœºå™¨äººä¸“ç”¨DQNç½‘ç»œ"""

    def __init__(self, state_size, action_size, hidden_size=256):
        super(VacuumDQN, self).__init__()

        # è¾“å…¥å±‚
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)

        # éšè—å±‚ - æ›´æ·±çš„ç½‘ç»œé€‚åº”å¤æ‚çš„æ¸…æ‰«ç­–ç•¥
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.bn2 = nn.BatchNorm1d(hidden_size)

        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)
        self.bn3 = nn.BatchNorm1d(hidden_size // 2)

        # è¾“å‡ºå±‚
        self.fc4 = nn.Linear(hidden_size // 2, action_size)

        # Dropoutå±‚é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        # å¤„ç†æ‰¹æ¬¡ç»´åº¦ä¸º1çš„æƒ…å†µ
        if x.dim() == 1:
            x = x.unsqueeze(0)

        x = F.relu(self.bn1(self.fc1(x)))
        x = self.dropout(x)

        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)

        x = F.relu(self.bn3(self.fc3(x)))
        x = self.dropout(x)

        x = self.fc4(x)

        return x

class VacuumAgent:
    """æ‰«åœ°æœºå™¨äººæ™ºèƒ½ä½“"""

    def __init__(self, state_size, action_size, learning_rate=1e-3, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.state_size = state_size
        self.action_size = action_size
        self.lr = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_min = epsilon_end
        self.epsilon_decay = epsilon_decay

        # è®¾å¤‡é€‰æ‹©
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ç¥ç»ç½‘ç»œ
        self.q_network = VacuumDQN(state_size, action_size).to(self.device)
        self.target_network = VacuumDQN(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=50000)  # æ›´å¤§çš„ç¼“å†²åŒº
        self.batch_size = 64

        # è®­ç»ƒå‚æ•°
        self.update_target_freq = 200  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
        self.min_replay_size = 1000    # å¼€å§‹è®­ç»ƒçš„æœ€å°æ ·æœ¬æ•°
        self.train_freq = 4            # è®­ç»ƒé¢‘ç‡

        # è®¡æ•°å™¨
        self.step_count = 0
        self.episode_count = 0

    def choose_action(self, state):
        """é€‰æ‹©åŠ¨ä½œ"""
        if np.random.random() <= self.epsilon:
            return np.random.choice(self.action_size)

        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥é¿å…BatchNormé—®é¢˜
        self.q_network.eval()
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        self.q_network.train()
        return q_values.cpu().data.numpy().argmax()

    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))

    def replay(self):
        """ç»éªŒå›æ”¾å­¦ä¹ """
        if len(self.memory) < self.min_replay_size:
            return None

        if self.step_count % self.train_freq != 0:
            return None

        batch = random.sample(self.memory, self.batch_size)
        # ä¼˜åŒ–æ•°æ®è½¬æ¢ï¼Œé¿å…åˆ›å»ºå¼ é‡æ—¶çš„è­¦å‘Š
        states = torch.FloatTensor(np.array([e[0] for e in batch])).to(self.device)
        actions = torch.LongTensor(np.array([e[1] for e in batch])).to(self.device)
        rewards = torch.FloatTensor(np.array([e[2] for e in batch])).to(self.device)
        next_states = torch.FloatTensor(np.array([e[3] for e in batch])).to(self.device)
        dones = torch.BoolTensor(np.array([e[4] for e in batch])).to(self.device)

        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.step_count % self.update_target_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        # æ›´æ–°æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        self.step_count += 1
        return loss.item()

class VacuumTrainingManager:
    """æ‰«åœ°æœºå™¨äººè®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, env, agent, save_dir="results"):
        self.env = env
        self.agent = agent
        self.save_dir = save_dir

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(f"{save_dir}/episode_paths", exist_ok=True)

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'dirt_collected': [],
            'coverage_rates': [],
            'energy_efficiency': [],
            'exploration_rate': [],
            'loss_values': [],
            'battery_usage': []
        }

    def train(self, num_episodes=1500, save_interval=100, render_interval=200):
        """è®­ç»ƒæ‰«åœ°æœºå™¨äºº"""
        print(f"å¼€å§‹è®­ç»ƒæ‰«åœ°æœºå™¨äººæ™ºèƒ½ä½“ï¼Œæ€»è®¡ {num_episodes} è½®...")
        start_time = time.time()

        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            episode_losses = []

            while True:
                action = self.agent.choose_action(state)
                next_state, reward, done = self.env.step(action)

                self.agent.remember(state, action, reward, next_state, done)
                loss = self.agent.replay()

                if loss is not None:
                    episode_losses.append(loss)

                state = next_state
                total_reward += reward

                if done:
                    break

            # è®°å½•ç»Ÿè®¡æ•°æ®
            self.training_stats['episode_rewards'].append(total_reward)
            self.training_stats['episode_lengths'].append(len(self.env.path_history))
            self.training_stats['dirt_collected'].append(self.env.dirt_collected)
            self.training_stats['coverage_rates'].append(
                np.sum(self.env.visited_map > 0) / np.sum(self.env.room_map == 0)
            )
            self.training_stats['energy_efficiency'].append(
                self.env.dirt_collected / max(1, self.env.max_battery - self.env.battery_level)
            )
            self.training_stats['exploration_rate'].append(self.agent.epsilon)
            self.training_stats['loss_values'].append(np.mean(episode_losses) if episode_losses else 0)
            self.training_stats['battery_usage'].append(self.env.max_battery - self.env.battery_level)

            # ä¿å­˜è·¯å¾„å¯è§†åŒ–
            if episode % render_interval == 0 or episode < 10:
                self.save_episode_path(episode)

            # è¿›åº¦æŠ¥å‘Š
            if episode % 50 == 0:
                avg_reward = np.mean(self.training_stats['episode_rewards'][-50:])
                avg_coverage = np.mean(self.training_stats['coverage_rates'][-50:])
                avg_dirt = np.mean(self.training_stats['dirt_collected'][-50:])
                print(f"Episode {episode:4d}: "
                      f"å¥–åŠ±={avg_reward:7.1f}, "
                      f"è¦†ç›–ç‡={avg_coverage:5.1%}, "
                      f"æ¸…æ‰«={avg_dirt:5.1f}, "
                      f"Îµ={self.agent.epsilon:.3f}")

            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode > 0 and episode % save_interval == 0:
                self.save_model(episode)

        training_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆ! ç”¨æ—¶: {training_time:.1f}ç§’")

        return self.training_stats

    def save_episode_path(self, episode):
        """ä¿å­˜å•æ¬¡æ¸…æ‰«è·¯å¾„å¯è§†åŒ–"""
        try:
            setup_chinese_fonts()
        except Exception:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'ç¬¬{episode}è½®æ¸…æ‰«æ´»åŠ¨è·¯çº¿å›¾ / Episode {episode} Cleaning Path',
                     fontsize=16, fontweight='bold')

        # 1. æˆ¿é—´å¸ƒå±€å’Œè·¯å¾„
        ax = axes[0, 0]
        room_display = self.env.room_map.copy()

        # ç»˜åˆ¶æˆ¿é—´
        room_colors = np.where(room_display == 1, 0.3, 1.0)  # éšœç¢ç‰©æ·±è‰²
        room_colors = np.where(room_display == 2, 0.7, room_colors)  # å……ç”µç«™ä¸­ç­‰è‰²
        ax.imshow(room_colors, cmap='gray', alpha=0.8)

        # ç»˜åˆ¶è·¯å¾„
        if len(self.env.path_history) > 1:
            path_array = np.array(self.env.path_history)
            ax.plot(path_array[:, 1], path_array[:, 0], 'b-', linewidth=2, alpha=0.7, label='æ¸…æ‰«è·¯å¾„')
            ax.plot(path_array[0, 1], path_array[0, 0], 'go', markersize=8, label='èµ·ç‚¹')
            ax.plot(path_array[-1, 1], path_array[-1, 0], 'ro', markersize=8, label='ç»ˆç‚¹')

        # æ ‡è®°å……ç”µç«™
        ax.plot(self.env.charging_station[1], self.env.charging_station[0],
                's', color='orange', markersize=10, label='å……ç”µç«™')

        ax.set_title('æˆ¿é—´å¸ƒå±€ä¸æ¸…æ‰«è·¯å¾„ / Room Layout & Path')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. ç°å°˜åˆ†å¸ƒå›¾
        ax = axes[0, 1]
        dirt_display = ax.imshow(self.env.dirt_map, cmap='YlOrBr', vmin=0, vmax=1)
        plt.colorbar(dirt_display, ax=ax, label='ç°å°˜å¯†åº¦ / Dirt Density')

        # å åŠ è®¿é—®è·¯å¾„
        if len(self.env.path_history) > 1:
            path_array = np.array(self.env.path_history)
            ax.plot(path_array[:, 1], path_array[:, 0], 'b-', linewidth=1, alpha=0.5)

        ax.set_title('ç°å°˜åˆ†å¸ƒå›¾ / Dirt Distribution')
        ax.grid(True, alpha=0.3)

        # 3. ç”µæ± ç”µé‡å˜åŒ–
        ax = axes[1, 0]
        steps = range(len(self.env.battery_history))
        ax.plot(steps, self.env.battery_history, 'g-', linewidth=2)
        ax.axhline(y=20, color='r', linestyle='--', alpha=0.7, label='ä½ç”µé‡è­¦å‘Š')
        ax.set_xlabel('æ­¥æ•° / Steps')
        ax.set_ylabel('ç”µæ± ç”µé‡ / Battery Level')
        ax.set_title('ç”µæ± ç”µé‡å˜åŒ– / Battery Level')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 4. ç´¯è®¡æ¸…æ‰«ç°å°˜
        ax = axes[1, 1]
        ax.plot(steps, self.env.dirt_collected_history, 'brown', linewidth=2)
        ax.set_xlabel('æ­¥æ•° / Steps')
        ax.set_ylabel('ç´¯è®¡æ¸…æ‰«ç°å°˜ / Cumulative Dirt')
        ax.set_title('æ¸…æ‰«æ•ˆæœ / Cleaning Progress')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        # å¤„ç†ä¸åŒç±»å‹çš„episodeå‚æ•°ï¼ˆæ•´æ•°æˆ–å­—ç¬¦ä¸²ï¼‰
        if isinstance(episode, str):
            filename = f'{self.save_dir}/episode_paths/episode_{episode}_path.png'
        else:
            filename = f'{self.save_dir}/episode_paths/episode_{episode:04d}_path.png'

        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.close()  # å…³é—­å›¾å½¢é¿å…å†…å­˜æ³„æ¼

    def save_model(self, episode):
        """ä¿å­˜æ¨¡å‹"""
        model_path = f"{self.save_dir}/vacuum_model_episode_{episode}.pth"
        torch.save({
            'episode': episode,
            'model_state_dict': self.agent.q_network.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'epsilon': self.agent.epsilon,
            'training_stats': self.training_stats
        }, model_path)

    def evaluate(self, num_episodes=20):
        """è¯„ä¼°è®­ç»ƒåçš„æ™ºèƒ½ä½“"""
        print(f"è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ï¼Œæµ‹è¯• {num_episodes} è½®...")

        # ä¿å­˜å½“å‰æ¢ç´¢ç‡
        original_epsilon = self.agent.epsilon
        self.agent.epsilon = 0  # è¯„ä¼°æ—¶ä¸æ¢ç´¢

        eval_results = {
            'rewards': [],
            'lengths': [],
            'dirt_collected': [],
            'coverage_rates': [],
            'energy_efficiency': [],
            'paths': []
        }

        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0

            while True:
                action = self.agent.choose_action(state)
                state, reward, done = self.env.step(action)
                total_reward += reward

                if done:
                    break

            # è®°å½•è¯„ä¼°ç»“æœ
            eval_results['rewards'].append(total_reward)
            eval_results['lengths'].append(len(self.env.path_history))
            eval_results['dirt_collected'].append(self.env.dirt_collected)
            eval_results['coverage_rates'].append(
                np.sum(self.env.visited_map > 0) / np.sum(self.env.room_map == 0)
            )
            eval_results['energy_efficiency'].append(
                self.env.dirt_collected / max(1, self.env.max_battery - self.env.battery_level)
            )
            eval_results['paths'].append(self.env.path_history.copy())

            # ä¿å­˜æœ€åå‡ æ¬¡è¯„ä¼°çš„è·¯å¾„
            if episode >= num_episodes - 3:
                self.save_episode_path(f"eval_{episode}")

        # æ¢å¤æ¢ç´¢ç‡
        self.agent.epsilon = original_epsilon

        # è®¡ç®—å¹³å‡ç»“æœ
        eval_results['avg_reward'] = np.mean(eval_results['rewards'])
        eval_results['avg_length'] = np.mean(eval_results['lengths'])
        eval_results['avg_dirt_collected'] = np.mean(eval_results['dirt_collected'])
        eval_results['avg_coverage_rate'] = np.mean(eval_results['coverage_rates'])
        eval_results['avg_energy_efficiency'] = np.mean(eval_results['energy_efficiency'])

        print(f"è¯„ä¼°å®Œæˆ:")
        print(f"  å¹³å‡å¥–åŠ±: {eval_results['avg_reward']:.1f}")
        print(f"  å¹³å‡æ­¥æ•°: {eval_results['avg_length']:.1f}")
        print(f"  å¹³å‡æ¸…æ‰«: {eval_results['avg_dirt_collected']:.2f}")
        print(f"  å¹³å‡è¦†ç›–ç‡: {eval_results['avg_coverage_rate']:.1%}")
        print(f"  å¹³å‡èƒ½æ•ˆ: {eval_results['avg_energy_efficiency']:.2f}")

        return eval_results

class VacuumResultsAnalyzer:
    """æ‰«åœ°æœºå™¨äººç»“æœåˆ†æå™¨"""

    def __init__(self, training_stats, eval_results, save_dir="results"):
        self.training_stats = training_stats
        self.eval_results = eval_results
        self.save_dir = save_dir

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nç”Ÿæˆæ‰«åœ°æœºå™¨äººåˆ†ææŠ¥å‘Š...")

        # åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–
        self.create_training_analysis()
        self.create_performance_analysis()
        self.create_evaluation_summary()
        self.create_text_report()

        print(f"å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜åˆ° {self.save_dir}/")

    def create_training_analysis(self):
        """åˆ›å»ºè®­ç»ƒè¿‡ç¨‹åˆ†æå›¾"""
        try:
            setup_chinese_fonts()
        except Exception:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(3, 2, figsize=(16, 14))
        fig.suptitle('æ‰«åœ°æœºå™¨äººè®­ç»ƒè¿‡ç¨‹åˆ†æ / Vacuum Robot Training Analysis',
                     fontsize=16, fontweight='bold')

        episodes = range(len(self.training_stats['episode_rewards']))

        # 1. å¥–åŠ±æ›²çº¿
        ax = axes[0, 0]
        ax.plot(episodes, self.training_stats['episode_rewards'], alpha=0.3, color='blue')
        if len(episodes) > 100:
            smoothed = np.convolve(self.training_stats['episode_rewards'],
                                 np.ones(100)/100, mode='valid')
            ax.plot(range(99, len(episodes)), smoothed, linewidth=2, color='red', label='å¹³æ»‘æ›²çº¿')
        ax.set_title('è®­ç»ƒå¥–åŠ±å˜åŒ– / Training Rewards')
        ax.set_xlabel('è®­ç»ƒè½®æ•° / Episodes')
        ax.set_ylabel('ç´¯è®¡å¥–åŠ± / Total Reward')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. æ¸…æ‰«æ•ˆæœ
        ax = axes[0, 1]
        ax.plot(episodes, self.training_stats['dirt_collected'], alpha=0.7, color='brown')
        if len(episodes) > 50:
            smoothed = np.convolve(self.training_stats['dirt_collected'],
                                 np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(episodes)), smoothed, linewidth=2, color='darkred', label='è¶‹åŠ¿çº¿')
        ax.set_title('æ¸…æ‰«ç°å°˜æ•°é‡ / Dirt Collected')
        ax.set_xlabel('è®­ç»ƒè½®æ•° / Episodes')
        ax.set_ylabel('æ¸…æ‰«ç°å°˜ / Dirt Amount')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. æˆ¿é—´è¦†ç›–ç‡
        ax = axes[1, 0]
        coverage_percent = [rate * 100 for rate in self.training_stats['coverage_rates']]
        ax.plot(episodes, coverage_percent, alpha=0.7, color='green')
        if len(episodes) > 50:
            smoothed = np.convolve(coverage_percent, np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(episodes)), smoothed, linewidth=2, color='darkgreen', label='è¶‹åŠ¿çº¿')
        ax.set_title('æˆ¿é—´è¦†ç›–ç‡ / Room Coverage')
        ax.set_xlabel('è®­ç»ƒè½®æ•° / Episodes')
        ax.set_ylabel('è¦†ç›–ç‡ (%) / Coverage (%)')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 4. èƒ½è€—æ•ˆç‡
        ax = axes[1, 1]
        ax.plot(episodes, self.training_stats['energy_efficiency'], alpha=0.7, color='orange')
        if len(episodes) > 50:
            smoothed = np.convolve(self.training_stats['energy_efficiency'],
                                 np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(episodes)), smoothed, linewidth=2, color='darkorange', label='è¶‹åŠ¿çº¿')
        ax.set_title('èƒ½è€—æ•ˆç‡ / Energy Efficiency')
        ax.set_xlabel('è®­ç»ƒè½®æ•° / Episodes')
        ax.set_ylabel('æ•ˆç‡å€¼ / Efficiency')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 5. æ¢ç´¢ç‡å˜åŒ–
        ax = axes[2, 0]
        ax.plot(episodes, self.training_stats['exploration_rate'],
                linewidth=2, color='purple')
        ax.set_title('æ¢ç´¢ç‡è¡°å‡ / Exploration Rate Decay')
        ax.set_xlabel('è®­ç»ƒè½®æ•° / Episodes')
        ax.set_ylabel('æ¢ç´¢ç‡ Îµ / Epsilon')
        ax.grid(True, alpha=0.3)

        # 6. ç”µæ± ä½¿ç”¨æƒ…å†µ
        ax = axes[2, 1]
        ax.plot(episodes, self.training_stats['battery_usage'], alpha=0.7, color='red')
        if len(episodes) > 50:
            smoothed = np.convolve(self.training_stats['battery_usage'],
                                 np.ones(50)/50, mode='valid')
            ax.plot(range(49, len(episodes)), smoothed, linewidth=2, color='darkred', label='è¶‹åŠ¿çº¿')
        ax.set_title('ç”µæ± æ¶ˆè€— / Battery Usage')
        ax.set_xlabel('è®­ç»ƒè½®æ•° / Episodes')
        ax.set_ylabel('ç”µé‡æ¶ˆè€— / Battery Used')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/training_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_performance_analysis(self):
        """åˆ›å»ºæ€§èƒ½åˆ†æå›¾"""
        try:
            setup_chinese_fonts()
        except Exception:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('æ‰«åœ°æœºå™¨äººæ€§èƒ½åˆ†æ / Performance Analysis',
                     fontsize=16, fontweight='bold')

        # 1. æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        ax = axes[0, 0]
        final_metrics = {
            'å¹³å‡å¥–åŠ±': np.mean(self.training_stats['episode_rewards'][-100:]),
            'æ¸…æ‰«æ•ˆæœ': np.mean(self.training_stats['dirt_collected'][-100:]),
            'è¦†ç›–ç‡(%)': np.mean(self.training_stats['coverage_rates'][-100:]) * 100,
            'èƒ½è€—æ•ˆç‡': np.mean(self.training_stats['energy_efficiency'][-100:])
        }

        metrics, values = zip(*final_metrics.items())
        colors = ['skyblue', 'brown', 'green', 'orange']
        bars = ax.bar(metrics, values, color=colors, alpha=0.7)
        ax.set_title('æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡ / Final Performance')
        ax.set_ylabel('æŒ‡æ ‡å€¼ / Values')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                   f'{value:.1f}', ha='center', va='bottom')

        # 2. è®­ç»ƒvsè¯„ä¼°å¯¹æ¯”
        ax = axes[0, 1]
        train_final = np.mean(self.training_stats['episode_rewards'][-50:])
        eval_avg = self.eval_results['avg_reward']

        comparison_data = ['è®­ç»ƒåæœŸ', 'è¯„ä¼°æµ‹è¯•']
        comparison_values = [train_final, eval_avg]
        bars = ax.bar(comparison_data, comparison_values,
                     color=['lightcoral', 'lightblue'], alpha=0.7)
        ax.set_title('è®­ç»ƒvsè¯„ä¼°å¯¹æ¯” / Train vs Eval')
        ax.set_ylabel('å¹³å‡å¥–åŠ± / Avg Reward')

        for bar, value in zip(bars, comparison_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                   f'{value:.1f}', ha='center', va='bottom')

        # 3. æ¸…æ‰«æ•ˆç‡åˆ†å¸ƒ
        ax = axes[1, 0]
        efficiency_scores = self.training_stats['energy_efficiency'][-200:]  # æœ€å200è½®
        ax.hist(efficiency_scores, bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax.axvline(np.mean(efficiency_scores), color='red', linestyle='--',
                  label=f'å¹³å‡å€¼: {np.mean(efficiency_scores):.2f}')
        ax.set_title('èƒ½è€—æ•ˆç‡åˆ†å¸ƒ / Efficiency Distribution')
        ax.set_xlabel('æ•ˆç‡å€¼ / Efficiency')
        ax.set_ylabel('é¢‘æ¬¡ / Frequency')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 4. è¦†ç›–ç‡æ”¹å–„è¶‹åŠ¿
        ax = axes[1, 1]
        coverage_percent = [rate * 100 for rate in self.training_stats['coverage_rates']]

        # åˆ†æ®µæ˜¾ç¤ºæ”¹å–„è¶‹åŠ¿
        stages = ['åˆæœŸ(0-500)', 'ä¸­æœŸ(500-1000)', 'åæœŸ(1000+)']
        stage_coverage = [
            np.mean(coverage_percent[:500]) if len(coverage_percent) > 500 else np.mean(coverage_percent[:len(coverage_percent)//3]),
            np.mean(coverage_percent[500:1000]) if len(coverage_percent) > 1000 else np.mean(coverage_percent[len(coverage_percent)//3:2*len(coverage_percent)//3]),
            np.mean(coverage_percent[1000:]) if len(coverage_percent) > 1000 else np.mean(coverage_percent[2*len(coverage_percent)//3:])
        ]

        bars = ax.bar(stages, stage_coverage, color=['lightcoral', 'lightyellow', 'lightgreen'], alpha=0.7)
        ax.set_title('è¦†ç›–ç‡æ”¹å–„è¶‹åŠ¿ / Coverage Improvement')
        ax.set_ylabel('å¹³å‡è¦†ç›–ç‡ (%) / Avg Coverage (%)')

        for bar, value in zip(bars, stage_coverage):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                   f'{value:.1f}%', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/performance_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_evaluation_summary(self):
        """åˆ›å»ºè¯„ä¼°æ€»ç»“å›¾"""
        try:
            setup_chinese_fonts()
        except Exception:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('æ‰«åœ°æœºå™¨äººè¯„ä¼°ç»“æœæ€»ç»“ / Evaluation Summary',
                     fontsize=16, fontweight='bold')

        # 1. è¯„ä¼°å¥–åŠ±åˆ†å¸ƒ
        ax = axes[0, 0]
        ax.hist(self.eval_results['rewards'], bins=15, alpha=0.7,
               color='lightblue', edgecolor='black')
        ax.axvline(self.eval_results['avg_reward'], color='red', linestyle='--',
                  label=f"å¹³å‡å€¼: {self.eval_results['avg_reward']:.1f}")
        ax.set_title('è¯„ä¼°å¥–åŠ±åˆ†å¸ƒ / Evaluation Rewards')
        ax.set_xlabel('å¥–åŠ± / Reward')
        ax.set_ylabel('é¢‘æ¬¡ / Frequency')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. æ¸…æ‰«é‡åˆ†å¸ƒ
        ax = axes[0, 1]
        ax.hist(self.eval_results['dirt_collected'], bins=15, alpha=0.7,
               color='brown', edgecolor='black')
        ax.axvline(self.eval_results['avg_dirt_collected'], color='red', linestyle='--',
                  label=f"å¹³å‡å€¼: {self.eval_results['avg_dirt_collected']:.2f}")
        ax.set_title('æ¸…æ‰«é‡åˆ†å¸ƒ / Dirt Collected Distribution')
        ax.set_xlabel('æ¸…æ‰«ç°å°˜ / Dirt Amount')
        ax.set_ylabel('é¢‘æ¬¡ / Frequency')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. è¦†ç›–ç‡åˆ†å¸ƒ
        ax = axes[1, 0]
        coverage_percent = [rate * 100 for rate in self.eval_results['coverage_rates']]
        ax.hist(coverage_percent, bins=15, alpha=0.7,
               color='green', edgecolor='black')
        ax.axvline(self.eval_results['avg_coverage_rate'] * 100, color='red', linestyle='--',
                  label=f"å¹³å‡å€¼: {self.eval_results['avg_coverage_rate']*100:.1f}%")
        ax.set_title('è¦†ç›–ç‡åˆ†å¸ƒ / Coverage Distribution')
        ax.set_xlabel('è¦†ç›–ç‡ (%) / Coverage (%)')
        ax.set_ylabel('é¢‘æ¬¡ / Frequency')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 4. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        ax = axes[1, 1]
        metrics = ['å¥–åŠ±', 'æ¸…æ‰«é‡', 'è¦†ç›–ç‡', 'èƒ½æ•ˆ', 'æ­¥æ•°æ•ˆç‡']

        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´è¿›è¡Œæ¯”è¾ƒ
        values = [
            self.eval_results['avg_reward'] / 100,  # å‡è®¾æœ€å¤§å¥–åŠ±100
            self.eval_results['avg_dirt_collected'] / 10,  # å‡è®¾æœ€å¤§æ¸…æ‰«10
            self.eval_results['avg_coverage_rate'],  # å·²ç»æ˜¯0-1
            self.eval_results['avg_energy_efficiency'] / 5,  # å‡è®¾æœ€å¤§æ•ˆç‡5
            1 - (self.eval_results['avg_length'] / 300)  # æ­¥æ•°è¶Šå°‘è¶Šå¥½ï¼Œå½’ä¸€åŒ–
        ]

        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        values = [max(0, min(1, v)) for v in values]

        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)
        values += values[:1]  # é—­åˆå›¾å½¢
        angles = np.concatenate((angles, [angles[0]]))

        ax.plot(angles, values, 'o-', linewidth=2, color='blue', alpha=0.8)
        ax.fill(angles, values, alpha=0.25, color='blue')
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.set_title('ç»¼åˆæ€§èƒ½è¯„ä¼° / Overall Performance')
        ax.grid(True)

        plt.tight_layout()
        plt.savefig(f"{self.save_dir}/evaluation_summary.png", dpi=300, bbox_inches='tight')
        plt.close()

    def create_text_report(self):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report = f"""
æ‰«åœ°æœºå™¨äººå¼ºåŒ–å­¦ä¹ è®­ç»ƒæŠ¥å‘Š
==========================
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

è®­ç»ƒé…ç½®
--------
- è®­ç»ƒè½®æ•°: {len(self.training_stats['episode_rewards'])}
- æˆ¿é—´å¤§å°: 15x15 ç½‘æ ¼
- éšœç¢ç‰©æ•°é‡: 8ä¸ª
- åˆå§‹ç°å°˜å¯†åº¦: 30%
- æœ€å¤§ç”µæ± å®¹é‡: 100

è®­ç»ƒç»“æœ
--------
æœ€ç»ˆè®­ç»ƒè¡¨ç° (æœ€å100è½®å¹³å‡):
- å¹³å‡å¥–åŠ±: {np.mean(self.training_stats['episode_rewards'][-100:]):.1f}
- å¹³å‡æ¸…æ‰«ç°å°˜: {np.mean(self.training_stats['dirt_collected'][-100:]):.2f}
- å¹³å‡æˆ¿é—´è¦†ç›–ç‡: {np.mean(self.training_stats['coverage_rates'][-100:])*100:.1f}%
- å¹³å‡èƒ½è€—æ•ˆç‡: {np.mean(self.training_stats['energy_efficiency'][-100:]):.2f}
- å¹³å‡ç”µæ± ä½¿ç”¨: {np.mean(self.training_stats['battery_usage'][-100:]):.1f}

å­¦ä¹ è¿›å±•:
- åˆæœŸè¡¨ç° (å‰100è½®): å¥–åŠ±={np.mean(self.training_stats['episode_rewards'][:100]):.1f}
- æœ€ç»ˆè¡¨ç° (å100è½®): å¥–åŠ±={np.mean(self.training_stats['episode_rewards'][-100:]):.1f}
- æ”¹å–„ç¨‹åº¦: {((np.mean(self.training_stats['episode_rewards'][-100:]) - np.mean(self.training_stats['episode_rewards'][:100])) / abs(np.mean(self.training_stats['episode_rewards'][:100])) * 100):.1f}%

è¯„ä¼°ç»“æœ
--------
æ— æ¢ç´¢æµ‹è¯•è¡¨ç° (20è½®æµ‹è¯•):
- å¹³å‡å¥–åŠ±: {self.eval_results['avg_reward']:.1f}
- å¹³å‡æ¸…æ‰«ç°å°˜: {self.eval_results['avg_dirt_collected']:.2f}
- å¹³å‡æˆ¿é—´è¦†ç›–ç‡: {self.eval_results['avg_coverage_rate']*100:.1f}%
- å¹³å‡èƒ½è€—æ•ˆç‡: {self.eval_results['avg_energy_efficiency']:.2f}
- å¹³å‡å®Œæˆæ­¥æ•°: {self.eval_results['avg_length']:.1f}

æ€§èƒ½ç¨³å®šæ€§:
- å¥–åŠ±æ ‡å‡†å·®: {np.std(self.eval_results['rewards']):.1f}
- æ¸…æ‰«é‡æ ‡å‡†å·®: {np.std(self.eval_results['dirt_collected']):.2f}
- è¦†ç›–ç‡æ ‡å‡†å·®: {np.std(self.eval_results['coverage_rates'])*100:.1f}%

ç®—æ³•è¡¨ç°è¯„ä¼°
----------
1. å­¦ä¹ èƒ½åŠ›: {'ä¼˜ç§€' if np.mean(self.training_stats['episode_rewards'][-100:]) > np.mean(self.training_stats['episode_rewards'][:100]) * 1.5 else 'è‰¯å¥½' if np.mean(self.training_stats['episode_rewards'][-100:]) > np.mean(self.training_stats['episode_rewards'][:100]) * 1.2 else 'ä¸€èˆ¬'}
   - æ™ºèƒ½ä½“æˆåŠŸå­¦ä¼šäº†æœ‰æ•ˆçš„æ¸…æ‰«ç­–ç•¥

2. æ¸…æ‰«æ•ˆç‡: {'ä¼˜ç§€' if self.eval_results['avg_coverage_rate'] > 0.8 else 'è‰¯å¥½' if self.eval_results['avg_coverage_rate'] > 0.6 else 'ä¸€èˆ¬'}
   - æˆ¿é—´è¦†ç›–ç‡è¾¾åˆ° {self.eval_results['avg_coverage_rate']*100:.1f}%

3. èƒ½æºç®¡ç†: {'ä¼˜ç§€' if self.eval_results['avg_energy_efficiency'] > 1.0 else 'è‰¯å¥½' if self.eval_results['avg_energy_efficiency'] > 0.5 else 'éœ€æ”¹è¿›'}
   - èƒ½è€—æ•ˆç‡ä¸º {self.eval_results['avg_energy_efficiency']:.2f}

4. ç­–ç•¥ç¨³å®šæ€§: {'ä¼˜ç§€' if np.std(self.eval_results['rewards']) < 20 else 'è‰¯å¥½' if np.std(self.eval_results['rewards']) < 50 else 'ä¸€èˆ¬'}
   - æµ‹è¯•ç»“æœæ ‡å‡†å·®ä¸º {np.std(self.eval_results['rewards']):.1f}

å…³é”®å‘ç°
--------
1. è®­ç»ƒæ”¶æ•›æ€§: æ™ºèƒ½ä½“åœ¨çº¦ {len(self.training_stats['episode_rewards'])//3} è½®åå¼€å§‹æ˜¾è‘—æ”¹å–„
2. æ¢ç´¢ç­–ç•¥: æ¢ç´¢ç‡ä» 1.0 è¡°å‡åˆ° {self.training_stats['exploration_rate'][-1]:.3f}
3. æ¸…æ‰«æ¨¡å¼: æ™ºèƒ½ä½“å­¦ä¼šäº†ç³»ç»Ÿæ€§çš„æˆ¿é—´æ¸…æ‰«æ¨¡å¼
4. ç”µæ± ç®¡ç†: èƒ½å¤Ÿåœ¨ç”µé‡ä¸è¶³æ—¶ä¸»åŠ¨è¿”å›å……ç”µç«™

æŠ€æœ¯å»ºè®®
--------
1. ç®—æ³•æ”¹è¿›:
   - å¯å°è¯•Double DQNæˆ–Dueling DQNæå‡æ€§èƒ½
   - è€ƒè™‘æ·»åŠ ä¼˜å…ˆç»éªŒå›æ”¾(PER)
   - å®ç°å¤šæ­¥å­¦ä¹ æé«˜æ ·æœ¬æ•ˆç‡

2. ç¯å¢ƒå¢å¼º:
   - å¢åŠ ä¸åŒæˆ¿é—´å¸ƒå±€çš„è®­ç»ƒ
   - æ·»åŠ åŠ¨æ€éšœç¢ç‰©ï¼ˆå¦‚ç§»åŠ¨çš„å®¶å…·ï¼‰
   - å®ç°æ›´å¤æ‚çš„ç°å°˜åˆ†å¸ƒæ¨¡å¼

3. å®é™…åº”ç”¨:
   - å¢åŠ ä¼ æ„Ÿå™¨å™ªå£°æ¨¡æ‹ŸçœŸå®æ¡ä»¶
   - è€ƒè™‘åœ°æ¯¯ã€ç¡¬åœ°æ¿ç­‰ä¸åŒæ¸…æ‰«éš¾åº¦
   - æ·»åŠ å¤šæˆ¿é—´å¯¼èˆªèƒ½åŠ›

æ–‡ä»¶è¯´æ˜
--------
- training_analysis.png: è®­ç»ƒè¿‡ç¨‹è¯¦ç»†åˆ†æ
- performance_analysis.png: æ€§èƒ½æŒ‡æ ‡åˆ†æ
- evaluation_summary.png: è¯„ä¼°ç»“æœæ€»ç»“
- episode_paths/: å„è½®æ¬¡æ¸…æ‰«è·¯å¾„å¯è§†åŒ–
- vacuum_report.txt: æœ¬æ–‡æœ¬æŠ¥å‘Š

å®éªŒç»“è®º
--------
æœ¬æ¬¡å®éªŒæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»æ¸…æ‰«æˆ¿é—´çš„æ‰«åœ°æœºå™¨äººæ™ºèƒ½ä½“ã€‚
é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œæœºå™¨äººå­¦ä¼šäº†:
1. é«˜æ•ˆçš„æˆ¿é—´éå†ç­–ç•¥
2. æ™ºèƒ½çš„ç”µæ± ç®¡ç†
3. ä¼˜åŒ–çš„æ¸…æ‰«è·¯å¾„è§„åˆ’

è®­ç»ƒåçš„æ™ºèƒ½ä½“åœ¨æµ‹è¯•ä¸­è¡¨ç°ç¨³å®šï¼Œå…·å¤‡äº†å®é™…åº”ç”¨çš„æ½œåŠ›ã€‚

æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚
========================
"""

        with open(f'{self.save_dir}/vacuum_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)

        # ä¿å­˜è¯¦ç»†æ•°æ®
        results_data = {
            'training_stats': self.training_stats,
            'eval_results': self.eval_results,
            'summary_metrics': {
                'final_avg_reward': np.mean(self.training_stats['episode_rewards'][-100:]),
                'final_avg_coverage': np.mean(self.training_stats['coverage_rates'][-100:]),
                'final_avg_efficiency': np.mean(self.training_stats['energy_efficiency'][-100:]),
                'eval_avg_reward': self.eval_results['avg_reward'],
                'eval_avg_coverage': self.eval_results['avg_coverage_rate'],
                'eval_avg_efficiency': self.eval_results['avg_energy_efficiency']
            }
        }

        with open(f'{self.save_dir}/vacuum_results_data.json', 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„æ‰«åœ°æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ¼”ç¤º"""
    print("=" * 60)
    print("æ‰«åœ°æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ¼”ç¤ºé¡¹ç›®")
    print("=" * 60)
    print("æœ¬é¡¹ç›®æ¼”ç¤ºæ™ºèƒ½æ‰«åœ°æœºå™¨äººçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹")
    print("åŒ…å«è·¯å¾„è§„åˆ’ã€æ¸…æ‰«ç­–ç•¥å’Œç”µæ± ç®¡ç†çš„å­¦ä¹ ")
    print("=" * 60)

    # æ­¥éª¤1ï¼šåˆ›å»ºç¯å¢ƒ
    print("\næ­¥éª¤1: åˆ›å»ºæ‰«åœ°æœºå™¨äººç¯å¢ƒ...")
    env = VacuumCleanerEnvironment(room_size=15, num_obstacles=8,
                                   dirt_density=0.3, max_battery=100)
    state_size = len(env.get_state())
    action_size = len(env.actions)
    print(f"   æˆ¿é—´å¤§å°: {env.room_size}x{env.room_size}")
    print(f"   éšœç¢ç‰©æ•°é‡: {env.num_obstacles}")
    print(f"   çŠ¶æ€ç»´åº¦: {state_size}")
    print(f"   åŠ¨ä½œæ•°é‡: {action_size}")

    # æ­¥éª¤2ï¼šåˆ›å»ºæ™ºèƒ½ä½“
    print("\næ­¥éª¤2: åˆ›å»ºæ‰«åœ°æœºå™¨äººDQNæ™ºèƒ½ä½“...")
    agent = VacuumAgent(state_size, action_size,
                       learning_rate=1e-3, gamma=0.99,
                       epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995)
    print(f"   ç½‘ç»œç»“æ„: {state_size} -> 256 -> 256 -> 128 -> {action_size}")
    print(f"   ä½¿ç”¨è®¾å¤‡: {agent.device}")

    # æ­¥éª¤3ï¼šè®­ç»ƒæ™ºèƒ½ä½“
    print("\næ­¥éª¤3: å¼€å§‹è®­ç»ƒæ‰«åœ°æœºå™¨äºº...")
    trainer = VacuumTrainingManager(env, agent, save_dir="results")
    training_stats = trainer.train(num_episodes=1500, save_interval=200, render_interval=100)

    final_reward = np.mean(training_stats['episode_rewards'][-50:])
    final_coverage = np.mean(training_stats['coverage_rates'][-50:])
    final_efficiency = np.mean(training_stats['energy_efficiency'][-50:])

    print(f"\nâœ“ è®­ç»ƒå®Œæˆ!")
    print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_reward:.1f}")
    print(f"  æœ€ç»ˆè¦†ç›–ç‡: {final_coverage*100:.1f}%")
    print(f"  æœ€ç»ˆèƒ½è€—æ•ˆç‡: {final_efficiency:.2f}")
    print(f"  æœ€ç»ˆæ¢ç´¢ç‡: {agent.epsilon:.3f}")

    # æ­¥éª¤4ï¼šè¯„ä¼°æ€§èƒ½
    print("\næ­¥éª¤4: è¯„ä¼°æ‰«åœ°æœºå™¨äººæ€§èƒ½...")
    eval_results = trainer.evaluate(num_episodes=20)

    print(f"âœ“ è¯„ä¼°å®Œæˆ!")
    print(f"  æµ‹è¯•å¹³å‡å¥–åŠ±: {eval_results['avg_reward']:.1f}")
    print(f"  æµ‹è¯•è¦†ç›–ç‡: {eval_results['avg_coverage_rate']*100:.1f}%")
    print(f"  æµ‹è¯•èƒ½è€—æ•ˆç‡: {eval_results['avg_energy_efficiency']:.2f}")

    # æ­¥éª¤5ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š
    print("\næ­¥éª¤5: ç”Ÿæˆåˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–...")
    analyzer = VacuumResultsAnalyzer(training_stats, eval_results, save_dir="results")
    analyzer.generate_comprehensive_report()

    print("\n" + "=" * 60)
    print("æ‰«åœ°æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)

    print(f"\nğŸ“Š æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
    print(f"   æ¸…æ‰«æ•ˆç‡: {eval_results['avg_coverage_rate']*100:.1f}%")
    print(f"   èƒ½è€—ä¼˜åŒ–: {eval_results['avg_energy_efficiency']:.2f}")
    print(f"   å­¦ä¹ æˆæ•ˆ: {((final_reward - np.mean(training_stats['episode_rewards'][:100])) / abs(np.mean(training_stats['episode_rewards'][:100])) * 100):.1f}% æ”¹å–„")

    print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
    print(f"   results/training_analysis.png - è®­ç»ƒè¿‡ç¨‹åˆ†æ")
    print(f"   results/performance_analysis.png - æ€§èƒ½æŒ‡æ ‡åˆ†æ")
    print(f"   results/evaluation_summary.png - è¯„ä¼°ç»“æœæ€»ç»“")
    print(f"   results/episode_paths/ - æ¸…æ‰«è·¯å¾„å¯è§†åŒ–å›¾é›†")
    print(f"   results/vacuum_report.txt - å®Œæ•´åˆ†ææŠ¥å‘Š")
    print(f"   results/vacuum_results_data.json - è¯¦ç»†å®éªŒæ•°æ®")

    print(f"\nğŸ¯ ç³»ç»Ÿç‰¹ç‚¹:")
    print(f"   âœ“ æ™ºèƒ½è·¯å¾„è§„åˆ’å’Œæˆ¿é—´éå†")
    print(f"   âœ“ è‡ªé€‚åº”æ¸…æ‰«ç­–ç•¥å­¦ä¹ ")
    print(f"   âœ“ ç”µæ± ç”µé‡ç®¡ç†å’Œå……ç”µç­–ç•¥")
    print(f"   âœ“ åŠ¨æ€ç°å°˜åˆ†å¸ƒå¤„ç†")
    print(f"   âœ“ å®Œæ•´çš„è·¯å¾„å¯è§†åŒ–è¿½è¸ª")

    print(f"\nğŸ  å®é™…åº”ç”¨åœºæ™¯:")
    print(f"   â€¢ å®¶åº­è‡ªåŠ¨æ‰«åœ°æœºå™¨äºº")
    print(f"   â€¢ åŠå…¬å®¤æ¸…æ´æœºå™¨äºº")
    print(f"   â€¢ å•†åœºæ¸…æ‰«è®¾å¤‡")
    print(f"   â€¢ ä»“åº“åœ°é¢æ¸…ç†ç³»ç»Ÿ")

    # æ€§èƒ½è¯„ä»·
    if eval_results['avg_coverage_rate'] > 0.8 and eval_results['avg_energy_efficiency'] > 1.0:
        print(f"\nğŸ‰ ç³»ç»Ÿæ€§èƒ½è¯„ä»·: ä¼˜ç§€")
        print(f"   æ‰«åœ°æœºå™¨äººå·²å…·å¤‡å®é™…éƒ¨ç½²èƒ½åŠ›ï¼")
    elif eval_results['avg_coverage_rate'] > 0.6 and eval_results['avg_energy_efficiency'] > 0.5:
        print(f"\nğŸ‘ ç³»ç»Ÿæ€§èƒ½è¯„ä»·: è‰¯å¥½")
        print(f"   æ‰«åœ°æœºå™¨äººè¡¨ç°è‰¯å¥½ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print(f"\nâš ï¸  ç³»ç»Ÿæ€§èƒ½è¯„ä»·: éœ€è¦æ”¹è¿›")
        print(f"   å»ºè®®è°ƒæ•´è®­ç»ƒå‚æ•°æˆ–ç¯å¢ƒè®¾ç½®")

    print(f"\nğŸ“ˆ è·¯å¾„å¯è§†åŒ–:")
    print(f"   æ¯æ¬¡æ¸…æ‰«çš„å®Œæ•´è·¯å¾„éƒ½å·²ä¿å­˜ä¸ºå›¾ç‰‡")
    print(f"   å¯æŸ¥çœ‹ results/episode_paths/ æ–‡ä»¶å¤¹äº†è§£å­¦ä¹ è¿›å±•")

if __name__ == "__main__":
    main()