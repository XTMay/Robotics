# 阶段三：机器人导航综合项目

## 第7讲：机器人导航问题建模

### 7.1 导航任务概述

#### 7.1.1 任务定义

机器人自主导航是强化学习在机器人领域的经典应用，包含以下核心要素：

**主要目标**：
- 从起始位置导航到目标位置
- 避免与障碍物碰撞
- 优化路径长度和时间

**挑战**：
- 连续状态和动作空间
- 部分可观测环境
- 动态障碍物
- 传感器噪声
- 实时性要求

#### 7.1.2 导航问题的分类

**按环境复杂度**：
- 静态环境：障碍物位置固定
- 动态环境：存在移动障碍物
- 未知环境：需要同时进行SLAM

**按任务类型**：
- 点到点导航
- 路径跟踪
- 区域探索
- 多目标导航

### 7.2 状态空间设计

#### 7.2.1 位置和方向信息

**基本状态变量**：
```python
# 机器人状态
state = {
    'position': [x, y],           # 2D位置
    'orientation': theta,         # 朝向角度
    'velocity': [vx, vy],         # 线速度
    'angular_velocity': omega     # 角速度
}
```

**目标相关状态**：
```python
# 目标信息
goal_info = {
    'distance_to_goal': d_goal,   # 到目标的距离
    'angle_to_goal': theta_goal,  # 到目标的角度
    'goal_position': [gx, gy]     # 目标位置（可选）
}
```

#### 7.2.2 传感器信息

**激光雷达数据**：
```python
# 简化的激光雷达扫描
lidar_ranges = [r1, r2, ..., rN]  # N个方向的距离测量
# 通常N=8,16,32等，覆盖360度

# 处理方式1：直接使用原始数据
state_lidar = lidar_ranges

# 处理方式2：特征提取
def process_lidar(ranges, num_sectors=8):
    """将激光雷达数据分扇区处理"""
    sector_size = len(ranges) // num_sectors
    processed = []
    for i in range(num_sectors):
        start_idx = i * sector_size
        end_idx = (i + 1) * sector_size
        min_distance = min(ranges[start_idx:end_idx])
        processed.append(min_distance)
    return processed
```

**相机数据**：
```python
# 对于视觉导航
image_features = {
    'rgb_image': image,           # 原始图像
    'depth_map': depth,           # 深度图
    'feature_map': features       # 提取的特征
}
```

#### 7.2.3 状态归一化

```python
class StateNormalizer:
    def __init__(self):
        # 定义各状态变量的范围
        self.position_range = [-10, 10]      # 位置范围
        self.velocity_range = [-2, 2]        # 速度范围
        self.angle_range = [-np.pi, np.pi]   # 角度范围
        self.distance_range = [0, 10]        # 距离范围

    def normalize_state(self, state):
        """状态归一化到[-1, 1]"""
        normalized = {}

        # 位置归一化
        normalized['x'] = self.normalize_value(
            state['x'], self.position_range)
        normalized['y'] = self.normalize_value(
            state['y'], self.position_range)

        # 角度归一化
        normalized['theta'] = self.normalize_value(
            state['theta'], self.angle_range)

        # 距离归一化
        for i, dist in enumerate(state['lidar']):
            normalized[f'lidar_{i}'] = self.normalize_value(
                dist, self.distance_range)

        return normalized

    def normalize_value(self, value, value_range):
        """将值归一化到[-1, 1]"""
        min_val, max_val = value_range
        return 2 * (value - min_val) / (max_val - min_val) - 1
```

### 7.3 动作空间设计

#### 7.3.1 离散动作空间

**简化的导航动作**：
```python
class DiscreteActions:
    def __init__(self):
        self.actions = {
            0: 'forward',      # 前进
            1: 'backward',     # 后退
            2: 'turn_left',    # 左转
            3: 'turn_right',   # 右转
            4: 'stop'          # 停止
        }

        # 动作对应的控制指令
        self.action_map = {
            0: [1.0, 0.0],     # [线速度, 角速度]
            1: [-0.5, 0.0],
            2: [0.0, 1.0],
            3: [0.0, -1.0],
            4: [0.0, 0.0]
        }

    def action_to_velocity(self, action):
        return self.action_map[action]
```

**更精细的动作空间**：
```python
class GridActions:
    def __init__(self):
        # 3种线速度 × 3种角速度 = 9个动作
        linear_speeds = [-0.5, 0.0, 1.0]
        angular_speeds = [-1.0, 0.0, 1.0]

        self.actions = []
        for v in linear_speeds:
            for w in angular_speeds:
                self.actions.append([v, w])

    def get_action_space_size(self):
        return len(self.actions)
```

#### 7.3.2 连续动作空间

```python
class ContinuousActions:
    def __init__(self):
        # 动作范围
        self.v_min, self.v_max = -1.0, 2.0    # 线速度范围
        self.w_min, self.w_max = -2.0, 2.0    # 角速度范围

    def clip_action(self, action):
        """限制动作在有效范围内"""
        v, w = action
        v = np.clip(v, self.v_min, self.v_max)
        w = np.clip(w, self.w_min, self.w_max)
        return [v, w]

    def add_noise(self, action, noise_std=0.1):
        """为动作添加噪声（用于探索）"""
        noise = np.random.normal(0, noise_std, len(action))
        return action + noise
```

### 7.4 奖励函数设计

#### 7.4.1 基础奖励组件

**距离奖励**：
```python
def distance_reward(current_pos, goal_pos, prev_distance):
    """基于到目标距离的奖励"""
    current_distance = np.linalg.norm(
        np.array(current_pos) - np.array(goal_pos))

    # 距离减少给正奖励，增加给负奖励
    distance_reward = prev_distance - current_distance

    return distance_reward, current_distance

def shaped_distance_reward(distance, max_distance=10):
    """形状奖励：距离越近奖励越大"""
    return (max_distance - distance) / max_distance
```

**碰撞惩罚**：
```python
def collision_penalty(lidar_readings, safety_distance=0.5):
    """碰撞检测和惩罚"""
    min_distance = min(lidar_readings)

    if min_distance < safety_distance:
        # 距离越近惩罚越大
        penalty = -10 * (safety_distance - min_distance) / safety_distance
        collision = min_distance < 0.2  # 实际碰撞阈值
        return penalty, collision
    else:
        return 0, False
```

**目标到达奖励**：
```python
def goal_reached_reward(distance_to_goal, threshold=0.5):
    """到达目标的奖励"""
    if distance_to_goal < threshold:
        return 100  # 大的正奖励
    else:
        return 0
```

#### 7.4.2 综合奖励函数

```python
class NavigationReward:
    def __init__(self):
        self.prev_distance_to_goal = None
        self.weights = {
            'distance': 1.0,
            'collision': 5.0,
            'goal': 100.0,
            'time': -0.01,
            'smoothness': 1.0
        }

    def calculate_reward(self, state, action, done_info):
        """计算综合奖励"""
        total_reward = 0

        # 1. 距离奖励
        current_distance = state['distance_to_goal']
        if self.prev_distance_to_goal is not None:
            distance_reward = self.prev_distance_to_goal - current_distance
            total_reward += self.weights['distance'] * distance_reward

        self.prev_distance_to_goal = current_distance

        # 2. 碰撞惩罚
        min_lidar_distance = min(state['lidar'])
        if min_lidar_distance < 0.5:
            collision_penalty = -10 * (0.5 - min_lidar_distance)
            total_reward += self.weights['collision'] * collision_penalty

        # 3. 到达目标奖励
        if done_info['goal_reached']:
            total_reward += self.weights['goal']

        # 4. 时间惩罚（鼓励快速到达）
        total_reward += self.weights['time']

        # 5. 平滑性奖励（鼓励平滑的运动）
        if hasattr(self, 'prev_action'):
            action_diff = np.linalg.norm(
                np.array(action) - np.array(self.prev_action))
            smoothness_reward = -action_diff
            total_reward += self.weights['smoothness'] * smoothness_reward

        self.prev_action = action

        return total_reward

    def reset(self):
        """重置奖励计算状态"""
        self.prev_distance_to_goal = None
        self.prev_action = None
```

#### 7.4.3 稀疏奖励vs稠密奖励

**稀疏奖励**：
```python
def sparse_reward(distance_to_goal, collision):
    """稀疏奖励：只在关键时刻给奖励"""
    if collision:
        return -100
    elif distance_to_goal < 0.5:
        return 100
    else:
        return 0  # 大部分时间奖励为0
```

**稠密奖励**：
```python
def dense_reward(state, action, prev_state):
    """稠密奖励：每步都有指导信号"""
    reward = 0

    # 每步都有距离指导
    reward += distance_reward(state, prev_state)

    # 每步都有行为指导
    reward += smoothness_reward(action)

    # 每步都有安全指导
    reward += safety_reward(state['lidar'])

    return reward
```

### 7.5 终止条件设计

```python
class NavigationTermination:
    def __init__(self, goal_threshold=0.5, collision_threshold=0.2,
                 max_steps=1000, out_of_bounds=[-10, 10]):
        self.goal_threshold = goal_threshold
        self.collision_threshold = collision_threshold
        self.max_steps = max_steps
        self.out_of_bounds = out_of_bounds

    def check_termination(self, state, step_count):
        """检查是否满足终止条件"""
        done = False
        done_info = {
            'goal_reached': False,
            'collision': False,
            'timeout': False,
            'out_of_bounds': False
        }

        # 1. 到达目标
        if state['distance_to_goal'] < self.goal_threshold:
            done = True
            done_info['goal_reached'] = True

        # 2. 碰撞检测
        min_distance = min(state['lidar'])
        if min_distance < self.collision_threshold:
            done = True
            done_info['collision'] = True

        # 3. 超时
        if step_count >= self.max_steps:
            done = True
            done_info['timeout'] = True

        # 4. 超出边界
        x, y = state['position']
        if (x < self.out_of_bounds[0] or x > self.out_of_bounds[1] or
            y < self.out_of_bounds[0] or y > self.out_of_bounds[1]):
            done = True
            done_info['out_of_bounds'] = True

        return done, done_info
```

## 第8讲：仿真环境搭建

### 8.1 环境选择和比较

#### 8.1.1 主要仿真平台

| 平台 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| PyBullet | 轻量、易用、免费 | 图形质量一般 | 研究和原型开发 |
| Gazebo | 高保真、ROS集成 | 重量级、配置复杂 | 专业机器人开发 |
| AirSim | 高质量图像、无人机支持 | 主要面向无人机 | 视觉导航研究 |
| 自定义2D环境 | 简单、可控、快速 | 真实感不足 | 算法验证 |

#### 8.1.2 简化2D环境的优势

对于教学和算法验证，我们选择自定义2D环境：

**优势**：
- 快速运行，便于实验
- 易于理解和调试
- 参数完全可控
- 可视化直观

### 8.2 2D导航环境实现

#### 8.2.1 基础环境框架

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
import gym
from gym import spaces

class Simple2DNavigation(gym.Env):
    def __init__(self, world_size=10, goal_threshold=0.5):
        super(Simple2DNavigation, self).__init__()

        # 环境参数
        self.world_size = world_size
        self.goal_threshold = goal_threshold
        self.dt = 0.1  # 时间步长

        # 机器人参数
        self.robot_radius = 0.2
        self.max_linear_vel = 2.0
        self.max_angular_vel = 2.0

        # 状态空间：[x, y, theta, vx, vy, omega, goal_x, goal_y, lidar...]
        self.num_lidar_rays = 8
        state_dim = 8 + self.num_lidar_rays  # 基本状态 + 激光雷达
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32)

        # 动作空间：[linear_vel, angular_vel]
        self.action_space = spaces.Box(
            low=np.array([-1.0, -2.0]),
            high=np.array([2.0, 2.0]),
            dtype=np.float32)

        # 环境状态
        self.reset()

    def reset(self):
        """重置环境"""
        # 随机生成起始位置和目标位置
        self.robot_pos = self.random_free_position()
        self.robot_theta = np.random.uniform(-np.pi, np.pi)
        self.robot_vel = np.array([0.0, 0.0])
        self.robot_omega = 0.0

        self.goal_pos = self.random_free_position()

        # 确保起始位置和目标位置有一定距离
        while np.linalg.norm(self.robot_pos - self.goal_pos) < 2.0:
            self.goal_pos = self.random_free_position()

        # 生成障碍物
        self.generate_obstacles()

        # 重置步数
        self.step_count = 0
        self.max_steps = 1000

        return self.get_observation()

    def generate_obstacles(self):
        """生成随机障碍物"""
        self.obstacles = []
        num_obstacles = np.random.randint(3, 8)

        for _ in range(num_obstacles):
            # 随机位置和大小
            center = self.random_position()
            size = np.random.uniform(0.5, 1.5)

            # 确保障碍物不与起始位置和目标位置重叠
            robot_dist = np.linalg.norm(center - self.robot_pos)
            goal_dist = np.linalg.norm(center - self.goal_pos)

            if robot_dist > 1.0 and goal_dist > 1.0:
                self.obstacles.append({
                    'center': center,
                    'size': size,
                    'type': 'circle'
                })

    def random_position(self):
        """生成随机位置"""
        return np.random.uniform(-self.world_size/2, self.world_size/2, 2)

    def random_free_position(self):
        """生成不在障碍物内的随机位置"""
        for _ in range(100):  # 最多尝试100次
            pos = self.random_position()
            if self.is_position_free(pos):
                return pos
        # 如果找不到空闲位置，返回原点附近
        return np.array([0.0, 0.0])

    def is_position_free(self, pos):
        """检查位置是否无障碍物"""
        for obstacle in self.obstacles:
            distance = np.linalg.norm(pos - obstacle['center'])
            if distance < obstacle['size'] + self.robot_radius:
                return False
        return True

    def get_lidar_readings(self):
        """模拟激光雷达扫描"""
        ranges = []
        max_range = 5.0

        for i in range(self.num_lidar_rays):
            angle = self.robot_theta + (2 * np.pi * i / self.num_lidar_rays)
            ray_dir = np.array([np.cos(angle), np.sin(angle)])

            # 射线检测
            min_distance = max_range

            # 检查与边界的交点
            for boundary_dist in self.check_boundary_intersection(
                self.robot_pos, ray_dir):
                if boundary_dist < min_distance:
                    min_distance = boundary_dist

            # 检查与障碍物的交点
            for obstacle in self.obstacles:
                obstacle_dist = self.check_obstacle_intersection(
                    self.robot_pos, ray_dir, obstacle)
                if obstacle_dist < min_distance:
                    min_distance = obstacle_dist

            ranges.append(min_distance)

        return ranges

    def check_boundary_intersection(self, start_pos, direction):
        """检查射线与边界的交点"""
        distances = []
        bounds = self.world_size / 2

        # 检查四个边界
        # 右边界
        if direction[0] > 0:
            t = (bounds - start_pos[0]) / direction[0]
            if t > 0:
                distances.append(t)

        # 左边界
        if direction[0] < 0:
            t = (-bounds - start_pos[0]) / direction[0]
            if t > 0:
                distances.append(t)

        # 上边界
        if direction[1] > 0:
            t = (bounds - start_pos[1]) / direction[1]
            if t > 0:
                distances.append(t)

        # 下边界
        if direction[1] < 0:
            t = (-bounds - start_pos[1]) / direction[1]
            if t > 0:
                distances.append(t)

        return distances

    def check_obstacle_intersection(self, start_pos, direction, obstacle):
        """检查射线与圆形障碍物的交点"""
        # 射线与圆的交点计算
        center = obstacle['center']
        radius = obstacle['size']

        # 向量计算
        oc = start_pos - center
        a = np.dot(direction, direction)
        b = 2.0 * np.dot(oc, direction)
        c = np.dot(oc, oc) - radius * radius

        # 判别式
        discriminant = b * b - 4 * a * c

        if discriminant < 0:
            return float('inf')  # 无交点

        # 计算两个交点
        sqrt_discriminant = np.sqrt(discriminant)
        t1 = (-b - sqrt_discriminant) / (2 * a)
        t2 = (-b + sqrt_discriminant) / (2 * a)

        # 返回最近的正距离
        if t1 > 0:
            return t1
        elif t2 > 0:
            return t2
        else:
            return float('inf')

    def get_observation(self):
        """获取当前观测"""
        # 基本状态
        obs = [
            self.robot_pos[0],  # x位置
            self.robot_pos[1],  # y位置
            self.robot_theta,   # 朝向
            self.robot_vel[0],  # x速度
            self.robot_vel[1],  # y速度
            self.robot_omega,   # 角速度
            self.goal_pos[0],   # 目标x
            self.goal_pos[1],   # 目标y
        ]

        # 激光雷达数据
        lidar_data = self.get_lidar_readings()
        obs.extend(lidar_data)

        return np.array(obs, dtype=np.float32)

    def step(self, action):
        """执行一步动作"""
        # 限制动作范围
        linear_vel = np.clip(action[0], -1.0, 2.0)
        angular_vel = np.clip(action[1], -2.0, 2.0)

        # 更新机器人状态
        self.robot_omega = angular_vel
        self.robot_theta += self.robot_omega * self.dt

        # 保持角度在[-π, π]范围内
        self.robot_theta = ((self.robot_theta + np.pi) % (2 * np.pi)) - np.pi

        # 更新速度和位置
        self.robot_vel[0] = linear_vel * np.cos(self.robot_theta)
        self.robot_vel[1] = linear_vel * np.sin(self.robot_theta)

        new_pos = self.robot_pos + self.robot_vel * self.dt

        # 碰撞检测
        collision = not self.is_position_free(new_pos)

        if not collision:
            self.robot_pos = new_pos

        # 计算奖励
        distance_to_goal = np.linalg.norm(self.robot_pos - self.goal_pos)
        goal_reached = distance_to_goal < self.goal_threshold

        reward = self.calculate_reward(action, collision, goal_reached, distance_to_goal)

        # 检查终止条件
        self.step_count += 1
        done = (goal_reached or collision or
                self.step_count >= self.max_steps or
                self.is_out_of_bounds())

        info = {
            'goal_reached': goal_reached,
            'collision': collision,
            'distance_to_goal': distance_to_goal,
            'step_count': self.step_count
        }

        return self.get_observation(), reward, done, info

    def calculate_reward(self, action, collision, goal_reached, distance_to_goal):
        """计算奖励"""
        reward = 0

        # 目标奖励
        if goal_reached:
            reward += 100

        # 碰撞惩罚
        if collision:
            reward -= 50

        # 距离奖励（引导机器人接近目标）
        reward += (10 - distance_to_goal) * 0.1

        # 时间惩罚（鼓励快速完成）
        reward -= 0.01

        # 动作平滑性奖励
        action_penalty = np.sum(np.abs(action)) * 0.01
        reward -= action_penalty

        return reward

    def is_out_of_bounds(self):
        """检查是否超出边界"""
        bounds = self.world_size / 2
        return (abs(self.robot_pos[0]) > bounds or
                abs(self.robot_pos[1]) > bounds)

    def render(self, mode='human'):
        """渲染环境"""
        if not hasattr(self, 'fig'):
            self.fig, self.ax = plt.subplots(figsize=(10, 10))
            self.ax.set_xlim(-self.world_size/2, self.world_size/2)
            self.ax.set_ylim(-self.world_size/2, self.world_size/2)
            self.ax.set_aspect('equal')
            self.ax.grid(True)

        self.ax.clear()
        self.ax.set_xlim(-self.world_size/2, self.world_size/2)
        self.ax.set_ylim(-self.world_size/2, self.world_size/2)
        self.ax.set_aspect('equal')
        self.ax.grid(True)

        # 绘制障碍物
        for obstacle in self.obstacles:
            circle = patches.Circle(obstacle['center'], obstacle['size'],
                                  color='red', alpha=0.7)
            self.ax.add_patch(circle)

        # 绘制目标
        goal_circle = patches.Circle(self.goal_pos, self.goal_threshold,
                                   color='green', alpha=0.5)
        self.ax.add_patch(goal_circle)

        # 绘制机器人
        robot_circle = patches.Circle(self.robot_pos, self.robot_radius,
                                    color='blue', alpha=0.8)
        self.ax.add_patch(robot_circle)

        # 绘制机器人朝向
        arrow_length = 0.5
        arrow_end = self.robot_pos + arrow_length * np.array([
            np.cos(self.robot_theta), np.sin(self.robot_theta)
        ])
        self.ax.arrow(self.robot_pos[0], self.robot_pos[1],
                     arrow_end[0] - self.robot_pos[0],
                     arrow_end[1] - self.robot_pos[1],
                     head_width=0.1, head_length=0.1, fc='blue', ec='blue')

        # 绘制激光雷达
        lidar_ranges = self.get_lidar_readings()
        for i, distance in enumerate(lidar_ranges):
            angle = self.robot_theta + (2 * np.pi * i / self.num_lidar_rays)
            end_point = self.robot_pos + distance * np.array([
                np.cos(angle), np.sin(angle)
            ])
            self.ax.plot([self.robot_pos[0], end_point[0]],
                        [self.robot_pos[1], end_point[1]],
                        'r-', alpha=0.3, linewidth=0.5)

        self.ax.set_title(f'Step: {self.step_count}, Distance to Goal: {np.linalg.norm(self.robot_pos - self.goal_pos):.2f}')

        if mode == 'human':
            plt.pause(0.01)

        return self.fig
```

### 8.3 环境测试和调试

```python
def test_environment():
    """测试环境的基本功能"""
    env = Simple2DNavigation()

    # 测试重置
    obs = env.reset()
    print(f"Initial observation shape: {obs.shape}")
    print(f"Observation: {obs}")

    # 测试随机动作
    for step in range(100):
        action = env.action_space.sample()
        obs, reward, done, info = env.step(action)

        print(f"Step {step}: Reward={reward:.3f}, Done={done}")
        if done:
            print(f"Episode finished: {info}")
            break

        if step % 10 == 0:
            env.render()

    plt.show()

def benchmark_environment():
    """测试环境性能"""
    import time

    env = Simple2DNavigation()

    start_time = time.time()
    num_steps = 10000

    env.reset()
    for _ in range(num_steps):
        action = env.action_space.sample()
        env.step(action)

    end_time = time.time()
    fps = num_steps / (end_time - start_time)
    print(f"Environment FPS: {fps:.2f}")

# 运行测试
if __name__ == "__main__":
    test_environment()
    benchmark_environment()
```

## 第9讲：项目实施

### 9.1 DQN适配导航任务

#### 9.1.1 网络架构设计

```python
class NavigationDQN(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=256):
        super(NavigationDQN, self).__init__()

        # 特征提取层
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU()
        )

        # Q值输出层
        self.q_layer = nn.Linear(hidden_size//2, action_size)

    def forward(self, x):
        features = self.feature_extractor(x)
        q_values = self.q_layer(features)
        return q_values

class NavigationAgent:
    def __init__(self, state_size, action_size, learning_rate=1e-3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 网络初始化
        self.q_network = NavigationDQN(state_size, action_size).to(self.device)
        self.target_network = NavigationDQN(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # 训练参数
        self.memory = ReplayBuffer(100000)
        self.batch_size = 64
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.target_update_freq = 100

        # 训练记录
        self.episode_rewards = []
        self.episode_lengths = []
        self.success_rate = []

        self.update_target_network()

    def preprocess_state(self, state):
        """状态预处理"""
        # 归一化处理
        processed_state = state.copy()

        # 位置归一化到[-1, 1]
        processed_state[0] = state[0] / 5.0  # x position
        processed_state[1] = state[1] / 5.0  # y position

        # 角度归一化到[-1, 1]
        processed_state[2] = state[2] / np.pi  # theta

        # 速度归一化
        processed_state[3] = state[3] / 2.0  # vx
        processed_state[4] = state[4] / 2.0  # vy
        processed_state[5] = state[5] / 2.0  # omega

        # 目标位置归一化
        processed_state[6] = state[6] / 5.0  # goal_x
        processed_state[7] = state[7] / 5.0  # goal_y

        # 激光雷达数据归一化
        for i in range(8, len(state)):
            processed_state[i] = state[i] / 5.0  # lidar ranges

        return processed_state
```

#### 9.1.2 训练循环实现

```python
def train_navigation_agent(episodes=2000):
    """训练导航智能体"""
    env = Simple2DNavigation()
    agent = NavigationAgent(
        state_size=env.observation_space.shape[0],
        action_size=env.action_space.shape[0] if hasattr(env.action_space, 'shape') else env.action_space.n
    )

    # 如果是连续动作空间，需要离散化
    if isinstance(env.action_space, gym.spaces.Box):
        # 创建离散动作集合
        actions = create_discrete_actions()
        action_size = len(actions)
        agent = NavigationAgent(
            state_size=env.observation_space.shape[0],
            action_size=action_size
        )

    success_window = deque(maxlen=100)

    for episode in range(episodes):
        state = env.reset()
        state = agent.preprocess_state(state)

        total_reward = 0
        step_count = 0

        while True:
            # 选择动作
            action_idx = agent.choose_action(state)

            # 转换为环境动作
            if isinstance(env.action_space, gym.spaces.Box):
                action = actions[action_idx]
            else:
                action = action_idx

            # 执行动作
            next_state, reward, done, info = env.step(action)
            next_state = agent.preprocess_state(next_state)

            # 存储经验
            agent.store_experience(state, action_idx, reward, next_state, done)

            # 学习
            agent.learn()

            state = next_state
            total_reward += reward
            step_count += 1

            if done:
                break

        # 更新目标网络
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()

        # 记录训练数据
        agent.episode_rewards.append(total_reward)
        agent.episode_lengths.append(step_count)

        success = info.get('goal_reached', False)
        success_window.append(1 if success else 0)

        # 计算成功率
        if len(success_window) == 100:
            current_success_rate = np.mean(success_window)
            agent.success_rate.append(current_success_rate)

        # 打印进度
        if episode % 100 == 0:
            avg_reward = np.mean(agent.episode_rewards[-100:])
            avg_length = np.mean(agent.episode_lengths[-100:])
            success_rate = np.mean(success_window) if success_window else 0

            print(f"Episode {episode}")
            print(f"  Average Reward: {avg_reward:.2f}")
            print(f"  Average Length: {avg_length:.1f}")
            print(f"  Success Rate: {success_rate:.2%}")
            print(f"  Epsilon: {agent.epsilon:.3f}")
            print(f"  Memory Size: {len(agent.memory)}")
            print()

        # 早停条件
        if len(success_window) == 100 and np.mean(success_window) > 0.9:
            print(f"\\nTask solved in {episode} episodes!")
            print(f"Success rate: {np.mean(success_window):.2%}")
            break

    return agent, env

def create_discrete_actions():
    """创建离散动作集合"""
    linear_vels = [-0.5, 0.0, 0.5, 1.0, 1.5]
    angular_vels = [-1.5, -0.5, 0.0, 0.5, 1.5]

    actions = []
    for v in linear_vels:
        for w in angular_vels:
            actions.append([v, w])

    return actions
```

### 9.2 实时可视化和监控

```python
class TrainingMonitor:
    def __init__(self):
        self.fig, self.axes = plt.subplots(2, 3, figsize=(18, 10))
        self.fig.suptitle('Navigation Training Monitor')

        # 初始化图表
        self.reward_line, = self.axes[0, 0].plot([], [])
        self.success_line, = self.axes[0, 1].plot([], [])
        self.length_line, = self.axes[0, 2].plot([], [])
        self.loss_line, = self.axes[1, 0].plot([], [])
        self.epsilon_line, = self.axes[1, 1].plot([], [])

        # 设置图表属性
        self.setup_plots()

        plt.ion()
        plt.show()

    def setup_plots(self):
        """设置图表属性"""
        # 奖励图
        self.axes[0, 0].set_title('Episode Rewards')
        self.axes[0, 0].set_xlabel('Episode')
        self.axes[0, 0].set_ylabel('Total Reward')
        self.axes[0, 0].grid(True)

        # 成功率图
        self.axes[0, 1].set_title('Success Rate')
        self.axes[0, 1].set_xlabel('Episode')
        self.axes[0, 1].set_ylabel('Success Rate')
        self.axes[0, 1].set_ylim(0, 1)
        self.axes[0, 1].grid(True)

        # episode长度图
        self.axes[0, 2].set_title('Episode Length')
        self.axes[0, 2].set_xlabel('Episode')
        self.axes[0, 2].set_ylabel('Steps')
        self.axes[0, 2].grid(True)

        # 损失图
        self.axes[1, 0].set_title('Training Loss')
        self.axes[1, 0].set_xlabel('Training Step')
        self.axes[1, 0].set_ylabel('MSE Loss')
        self.axes[1, 0].set_yscale('log')
        self.axes[1, 0].grid(True)

        # Epsilon图
        self.axes[1, 1].set_title('Epsilon Decay')
        self.axes[1, 1].set_xlabel('Episode')
        self.axes[1, 1].set_ylabel('Epsilon')
        self.axes[1, 1].grid(True)

        # Q值分布图（将在update中设置）
        self.axes[1, 2].set_title('Q-Value Distribution')

    def update(self, agent, episode):
        """更新监控图表"""
        # 更新奖励曲线
        episodes = range(len(agent.episode_rewards))
        self.reward_line.set_data(episodes, agent.episode_rewards)
        self.axes[0, 0].relim()
        self.axes[0, 0].autoscale_view()

        # 更新成功率曲线
        if agent.success_rate:
            success_episodes = range(99, 99 + len(agent.success_rate))
            self.success_line.set_data(success_episodes, agent.success_rate)
            self.axes[0, 1].relim()
            self.axes[0, 1].autoscale_view()

        # 更新episode长度曲线
        self.length_line.set_data(episodes, agent.episode_lengths)
        self.axes[0, 2].relim()
        self.axes[0, 2].autoscale_view()

        # 更新损失曲线
        if agent.losses:
            loss_steps = range(len(agent.losses))
            self.loss_line.set_data(loss_steps, agent.losses)
            self.axes[1, 0].relim()
            self.axes[1, 0].autoscale_view()

        # 更新epsilon曲线
        epsilons = [agent.epsilon] * len(episodes)  # 简化版本
        self.epsilon_line.set_data(episodes, epsilons)
        self.axes[1, 1].relim()
        self.axes[1, 1].autoscale_view()

        # 更新Q值分布
        if episode % 50 == 0:  # 每50个episode更新一次
            self.update_q_distribution(agent)

        plt.draw()
        plt.pause(0.01)

    def update_q_distribution(self, agent):
        """更新Q值分布图"""
        # 生成一些随机状态
        sample_states = np.random.randn(1000, agent.q_network.fc1.in_features)
        sample_states = torch.FloatTensor(sample_states).to(agent.device)

        with torch.no_grad():
            q_values = agent.q_network(sample_states).cpu().numpy()

        self.axes[1, 2].clear()
        self.axes[1, 2].hist(q_values.flatten(), bins=50, alpha=0.7)
        self.axes[1, 2].set_title('Q-Value Distribution')
        self.axes[1, 2].set_xlabel('Q-Value')
        self.axes[1, 2].set_ylabel('Frequency')

def train_with_monitoring():
    """带监控的训练"""
    env = Simple2DNavigation()
    agent = NavigationAgent(
        state_size=env.observation_space.shape[0],
        action_size=len(create_discrete_actions())
    )

    monitor = TrainingMonitor()
    actions = create_discrete_actions()

    for episode in range(2000):
        state = env.reset()
        state = agent.preprocess_state(state)

        total_reward = 0
        step_count = 0

        while True:
            action_idx = agent.choose_action(state)
            action = actions[action_idx]

            next_state, reward, done, info = env.step(action)
            next_state = agent.preprocess_state(next_state)

            agent.store_experience(state, action_idx, reward, next_state, done)
            agent.learn()

            state = next_state
            total_reward += reward
            step_count += 1

            if done:
                break

        if episode % agent.target_update_freq == 0:
            agent.update_target_network()

        agent.episode_rewards.append(total_reward)
        agent.episode_lengths.append(step_count)

        # 更新监控
        if episode % 10 == 0:
            monitor.update(agent, episode)

        if episode % 100 == 0:
            print(f"Episode {episode}, Reward: {total_reward:.2f}")

    return agent, env
```

### 9.3 模型保存和加载

```python
def save_model(agent, filepath, metadata=None):
    """保存训练好的模型"""
    save_dict = {
        'model_state_dict': agent.q_network.state_dict(),
        'target_model_state_dict': agent.target_network.state_dict(),
        'optimizer_state_dict': agent.optimizer.state_dict(),
        'training_data': {
            'episode_rewards': agent.episode_rewards,
            'episode_lengths': agent.episode_lengths,
            'success_rate': agent.success_rate,
        },
        'hyperparameters': {
            'gamma': agent.gamma,
            'epsilon': agent.epsilon,
            'epsilon_min': agent.epsilon_min,
            'epsilon_decay': agent.epsilon_decay,
            'batch_size': agent.batch_size,
            'target_update_freq': agent.target_update_freq,
        }
    }

    if metadata:
        save_dict['metadata'] = metadata

    torch.save(save_dict, filepath)
    print(f"Model saved to {filepath}")

def load_model(agent, filepath):
    """加载保存的模型"""
    checkpoint = torch.load(filepath, map_location=agent.device)

    agent.q_network.load_state_dict(checkpoint['model_state_dict'])
    agent.target_network.load_state_dict(checkpoint['target_model_state_dict'])
    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # 恢复训练数据
    if 'training_data' in checkpoint:
        agent.episode_rewards = checkpoint['training_data']['episode_rewards']
        agent.episode_lengths = checkpoint['training_data']['episode_lengths']
        agent.success_rate = checkpoint['training_data']['success_rate']

    # 恢复超参数
    if 'hyperparameters' in checkpoint:
        hyperparams = checkpoint['hyperparameters']
        agent.gamma = hyperparams['gamma']
        agent.epsilon = hyperparams['epsilon']
        agent.epsilon_min = hyperparams['epsilon_min']
        agent.epsilon_decay = hyperparams['epsilon_decay']
        agent.batch_size = hyperparams['batch_size']
        agent.target_update_freq = hyperparams['target_update_freq']

    print(f"Model loaded from {filepath}")
    return checkpoint.get('metadata', None)

def evaluate_model(agent, env, num_episodes=100, render=False):
    """评估模型性能"""
    agent.epsilon = 0  # 关闭探索

    success_count = 0
    total_rewards = []
    episode_lengths = []

    for episode in range(num_episodes):
        state = env.reset()
        state = agent.preprocess_state(state)

        total_reward = 0
        step_count = 0

        while True:
            action_idx = agent.choose_action(state)
            action = create_discrete_actions()[action_idx]

            next_state, reward, done, info = env.step(action)
            next_state = agent.preprocess_state(next_state)

            state = next_state
            total_reward += reward
            step_count += 1

            if render and episode < 5:  # 只渲染前几个episode
                env.render()

            if done:
                if info.get('goal_reached', False):
                    success_count += 1
                break

        total_rewards.append(total_reward)
        episode_lengths.append(step_count)

        if episode % 20 == 0:
            print(f"Evaluation Episode {episode}/{num_episodes}")

    # 计算统计信息
    success_rate = success_count / num_episodes
    avg_reward = np.mean(total_rewards)
    avg_length = np.mean(episode_lengths)

    print(f"\\n=== Evaluation Results ===")
    print(f"Success Rate: {success_rate:.2%}")
    print(f"Average Reward: {avg_reward:.2f}")
    print(f"Average Episode Length: {avg_length:.1f}")
    print(f"Reward Std: {np.std(total_rewards):.2f}")

    return {
        'success_rate': success_rate,
        'avg_reward': avg_reward,
        'avg_length': avg_length,
        'rewards': total_rewards,
        'lengths': episode_lengths
    }

# 完整的训练和评估流程
if __name__ == "__main__":
    print("开始训练导航智能体...")

    # 训练
    agent, env = train_with_monitoring()

    # 保存模型
    save_model(agent, 'navigation_dqn.pth',
              metadata={'training_episodes': len(agent.episode_rewards)})

    # 评估模型
    print("\\n开始评估模型...")
    eval_results = evaluate_model(agent, env, num_episodes=100, render=True)

    print("\\n训练和评估完成！")
```

### 9.4 课后练习

#### 练习1：改进奖励函数
设计更复杂的奖励函数，包括：
- 路径效率奖励
- 能耗考虑
- 舒适度指标

#### 练习2：多目标导航
扩展环境支持多个目标点的顺序访问。

#### 练习3：动态障碍物
在环境中添加移动的障碍物。

#### 练习4：传感器噪声
为激光雷达添加高斯噪声，测试算法的鲁棒性。

### 9.5 课后思考题

1. 如何处理连续动作空间的导航问题？
2. 当环境部分可观测时，如何修改算法？
3. 如何将仿真中的模型迁移到真实机器人？
4. 多智能体导航中需要考虑哪些额外因素？