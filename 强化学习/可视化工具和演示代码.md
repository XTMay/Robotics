# 可视化工具和演示代码

## 1. Q-learning可视化工具

### 1.1 Q值热力图可视化

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.animation import FuncAnimation
import pandas as pd

class QLearningVisualizer:
    def __init__(self, q_table, env_shape=(4, 4)):
        self.q_table = q_table
        self.env_shape = env_shape
        self.action_names = ['Up', 'Down', 'Left', 'Right']

    def plot_q_values_heatmap(self):
        """绘制每个动作的Q值热力图"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()

        for action in range(4):
            # 将Q值重塑为网格形状
            q_values = self.q_table[:, action].reshape(self.env_shape)

            # 绘制热力图
            sns.heatmap(q_values,
                       annot=True,
                       fmt='.2f',
                       cmap='RdYlBu_r',
                       ax=axes[action],
                       cbar_kws={'label': 'Q-value'})

            axes[action].set_title(f'Q-values for Action: {self.action_names[action]}')
            axes[action].set_xlabel('Column')
            axes[action].set_ylabel('Row')

        plt.tight_layout()
        plt.show()

    def plot_policy_map(self):
        """绘制策略地图"""
        # 获取每个状态的最优动作
        policy = np.argmax(self.q_table, axis=1)
        policy_grid = policy.reshape(self.env_shape)

        # 创建箭头符号
        arrow_symbols = ['↑', '↓', '←', '→']

        fig, ax = plt.subplots(figsize=(8, 8))

        # 绘制背景网格
        for i in range(self.env_shape[0] + 1):
            ax.axhline(i - 0.5, color='black', linewidth=1)
        for j in range(self.env_shape[1] + 1):
            ax.axvline(j - 0.5, color='black', linewidth=1)

        # 绘制策略箭头
        for i in range(self.env_shape[0]):
            for j in range(self.env_shape[1]):
                action = policy_grid[i, j]
                ax.text(j, i, arrow_symbols[action],
                       ha='center', va='center', fontsize=24)

        ax.set_xlim(-0.5, self.env_shape[1] - 0.5)
        ax.set_ylim(-0.5, self.env_shape[0] - 0.5)
        ax.set_aspect('equal')
        ax.set_title('Learned Policy (Optimal Actions)')
        ax.set_xticks(range(self.env_shape[1]))
        ax.set_yticks(range(self.env_shape[0]))
        ax.invert_yaxis()  # 让(0,0)在左上角

        plt.show()

    def plot_state_values(self):
        """绘制状态价值函数"""
        # 计算状态价值（每个状态的最大Q值）
        state_values = np.max(self.q_table, axis=1)
        state_values_grid = state_values.reshape(self.env_shape)

        plt.figure(figsize=(8, 6))
        sns.heatmap(state_values_grid,
                   annot=True,
                   fmt='.3f',
                   cmap='viridis',
                   cbar_kws={'label': 'State Value'})

        plt.title('State Value Function V(s)')
        plt.xlabel('Column')
        plt.ylabel('Row')
        plt.show()

def create_q_learning_animation():
    """创建Q-learning学习过程的动画"""
    class QLearningAnimator:
        def __init__(self, env, agent):
            self.env = env
            self.agent = agent
            self.fig, self.axes = plt.subplots(1, 3, figsize=(18, 6))

            # 初始化图表
            self.q_im = None
            self.policy_text = []
            self.reward_line, = self.axes[2].plot([], [])

            # 存储训练数据
            self.episode_rewards = []
            self.q_history = []

        def animate(self, frame):
            """动画更新函数"""
            # 训练一个episode
            state = self.env.reset()
            total_reward = 0

            while True:
                action = self.agent.choose_action(state)
                next_state, reward, done, _ = self.env.step(action)
                self.agent.learn(state, action, reward, next_state, done)

                state = next_state
                total_reward += reward

                if done:
                    break

            self.episode_rewards.append(total_reward)
            self.q_history.append(self.agent.q_table.copy())

            # 更新Q值热力图
            self.update_q_heatmap()

            # 更新策略图
            self.update_policy_map()

            # 更新奖励曲线
            self.update_reward_plot()

            return []

        def update_q_heatmap(self):
            """更新Q值热力图"""
            self.axes[0].clear()
            q_max = np.max(self.agent.q_table, axis=1).reshape(4, 4)

            im = self.axes[0].imshow(q_max, cmap='viridis')
            self.axes[0].set_title('State Values (Max Q-values)')

            # 添加数值标注
            for i in range(4):
                for j in range(4):
                    self.axes[0].text(j, i, f'{q_max[i,j]:.2f}',
                                    ha='center', va='center', color='white')

        def update_policy_map(self):
            """更新策略图"""
            self.axes[1].clear()
            policy = np.argmax(self.agent.q_table, axis=1).reshape(4, 4)
            arrows = ['↑', '↓', '←', '→']

            for i in range(4):
                for j in range(4):
                    self.axes[1].text(j, i, arrows[policy[i,j]],
                                    ha='center', va='center', fontsize=20)

            self.axes[1].set_xlim(-0.5, 3.5)
            self.axes[1].set_ylim(-0.5, 3.5)
            self.axes[1].set_title('Current Policy')
            self.axes[1].invert_yaxis()

        def update_reward_plot(self):
            """更新奖励曲线"""
            episodes = range(len(self.episode_rewards))
            self.reward_line.set_data(episodes, self.episode_rewards)
            self.axes[2].relim()
            self.axes[2].autoscale_view()
            self.axes[2].set_title('Episode Rewards')
            self.axes[2].set_xlabel('Episode')
            self.axes[2].set_ylabel('Total Reward')

# 使用示例
def demo_q_learning_visualization():
    """演示Q-learning可视化"""
    import gym

    # 创建环境和智能体
    env = gym.make('FrozenLake-v1', is_slippery=False)

    # 假设我们有一个训练好的Q表
    # 这里创建一个示例Q表
    q_table = np.random.rand(16, 4) * 0.5

    # 创建可视化器
    visualizer = QLearningVisualizer(q_table)

    # 绘制各种可视化
    visualizer.plot_q_values_heatmap()
    visualizer.plot_policy_map()
    visualizer.plot_state_values()

# 运行演示
demo_q_learning_visualization()
```

### 1.2 训练过程可视化

```python
class TrainingProgressVisualizer:
    def __init__(self):
        self.fig, self.axes = plt.subplots(2, 3, figsize=(18, 12))
        self.fig.suptitle('Q-Learning Training Progress', fontsize=16)

    def plot_learning_curves(self, rewards, epsilons, q_values_history):
        """绘制学习曲线"""
        # 奖励曲线
        self.axes[0, 0].plot(rewards)
        self.axes[0, 0].set_title('Episode Rewards')
        self.axes[0, 0].set_xlabel('Episode')
        self.axes[0, 0].set_ylabel('Total Reward')
        self.axes[0, 0].grid(True)

        # 移动平均
        if len(rewards) > 100:
            moving_avg = pd.Series(rewards).rolling(100).mean()
            self.axes[0, 0].plot(moving_avg, 'r-', label='100-episode average')
            self.axes[0, 0].legend()

        # Epsilon衰减
        self.axes[0, 1].plot(epsilons)
        self.axes[0, 1].set_title('Exploration Rate (Epsilon)')
        self.axes[0, 1].set_xlabel('Episode')
        self.axes[0, 1].set_ylabel('Epsilon')
        self.axes[0, 1].grid(True)

        # Q值收敛
        if q_values_history:
            q_means = [np.mean(q_table) for q_table in q_values_history]
            q_stds = [np.std(q_table) for q_table in q_values_history]

            episodes = range(len(q_means))
            self.axes[0, 2].plot(episodes, q_means, label='Mean Q-value')
            self.axes[0, 2].fill_between(episodes,
                                       np.array(q_means) - np.array(q_stds),
                                       np.array(q_means) + np.array(q_stds),
                                       alpha=0.3)
            self.axes[0, 2].set_title('Q-value Statistics')
            self.axes[0, 2].set_xlabel('Episode')
            self.axes[0, 2].set_ylabel('Q-value')
            self.axes[0, 2].legend()
            self.axes[0, 2].grid(True)

        # 成功率统计
        window_size = 100
        if len(rewards) >= window_size:
            success_rates = []
            for i in range(window_size, len(rewards) + 1):
                recent_rewards = rewards[i-window_size:i]
                success_rate = sum(1 for r in recent_rewards if r > 0) / window_size
                success_rates.append(success_rate)

            success_episodes = range(window_size, len(rewards) + 1)
            self.axes[1, 0].plot(success_episodes, success_rates)
            self.axes[1, 0].set_title(f'Success Rate ({window_size}-episode window)')
            self.axes[1, 0].set_xlabel('Episode')
            self.axes[1, 0].set_ylabel('Success Rate')
            self.axes[1, 0].set_ylim(0, 1)
            self.axes[1, 0].grid(True)

        # 奖励分布直方图
        self.axes[1, 1].hist(rewards, bins=20, alpha=0.7, edgecolor='black')
        self.axes[1, 1].set_title('Reward Distribution')
        self.axes[1, 1].set_xlabel('Total Reward')
        self.axes[1, 1].set_ylabel('Frequency')
        self.axes[1, 1].grid(True)

        # 最终Q表热力图
        if q_values_history:
            final_q_table = q_values_history[-1]
            final_values = np.max(final_q_table, axis=1).reshape(4, 4)

            im = self.axes[1, 2].imshow(final_values, cmap='viridis')
            self.axes[1, 2].set_title('Final State Values')

            # 添加数值标注
            for i in range(4):
                for j in range(4):
                    self.axes[1, 2].text(j, i, f'{final_values[i,j]:.2f}',
                                       ha='center', va='center', color='white')

            plt.colorbar(im, ax=self.axes[1, 2])

        plt.tight_layout()
        plt.show()

def compare_algorithms():
    """比较不同算法的性能"""
    # 模拟不同算法的训练结果
    episodes = range(1000)

    # Q-learning
    q_learning_rewards = np.cumsum(np.random.exponential(0.1, 1000)) * 0.01

    # SARSA
    sarsa_rewards = np.cumsum(np.random.exponential(0.12, 1000)) * 0.008

    # 随机策略
    random_rewards = np.random.normal(-0.5, 0.2, 1000)

    plt.figure(figsize=(12, 8))

    # 绘制学习曲线
    plt.subplot(2, 2, 1)
    plt.plot(episodes, q_learning_rewards, label='Q-Learning', linewidth=2)
    plt.plot(episodes, sarsa_rewards, label='SARSA', linewidth=2)
    plt.plot(episodes, random_rewards, label='Random Policy', linewidth=2)
    plt.title('Learning Curves Comparison')
    plt.xlabel('Episode')
    plt.ylabel('Cumulative Reward')
    plt.legend()
    plt.grid(True)

    # 最终性能比较
    final_performances = [q_learning_rewards[-1], sarsa_rewards[-1], random_rewards[-1]]
    algorithms = ['Q-Learning', 'SARSA', 'Random']

    plt.subplot(2, 2, 2)
    bars = plt.bar(algorithms, final_performances,
                  color=['blue', 'green', 'red'], alpha=0.7)
    plt.title('Final Performance Comparison')
    plt.ylabel('Final Cumulative Reward')

    # 添加数值标签
    for bar, value in zip(bars, final_performances):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{value:.2f}', ha='center', va='bottom')

    # 收敛速度比较
    plt.subplot(2, 2, 3)
    # 计算到达90%最终性能的时间
    q_convergence = next(i for i, r in enumerate(q_learning_rewards)
                        if r >= 0.9 * q_learning_rewards[-1])
    sarsa_convergence = next(i for i, r in enumerate(sarsa_rewards)
                           if r >= 0.9 * sarsa_rewards[-1])

    convergence_times = [q_convergence, sarsa_convergence, len(episodes)]
    bars = plt.bar(algorithms, convergence_times,
                  color=['blue', 'green', 'red'], alpha=0.7)
    plt.title('Convergence Speed (Episodes to 90% performance)')
    plt.ylabel('Episodes')

    for bar, value in zip(bars, convergence_times):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{value}', ha='center', va='bottom')

    # 方差分析
    plt.subplot(2, 2, 4)
    window_size = 100
    q_variance = [np.var(q_learning_rewards[max(0, i-window_size):i+1])
                 for i in range(len(episodes))]
    sarsa_variance = [np.var(sarsa_rewards[max(0, i-window_size):i+1])
                     for i in range(len(episodes))]

    plt.plot(episodes, q_variance, label='Q-Learning Variance', alpha=0.7)
    plt.plot(episodes, sarsa_variance, label='SARSA Variance', alpha=0.7)
    plt.title('Learning Stability (Variance)')
    plt.xlabel('Episode')
    plt.ylabel('Reward Variance')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# 运行比较
compare_algorithms()
```

## 2. DQN可视化工具

### 2.1 神经网络可视化

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle, Circle, FancyBboxPatch
import networkx as nx

class DQNVisualizer:
    def __init__(self, dqn_model):
        self.model = dqn_model
        self.activations = {}
        self.gradients = {}

    def register_hooks(self):
        """注册钩子函数来捕获激活值和梯度"""
        def get_activation(name):
            def hook(model, input, output):
                self.activations[name] = output.detach()
            return hook

        def get_gradient(name):
            def hook(grad):
                self.gradients[name] = grad
            return hook

        # 为每一层注册钩子
        for name, layer in self.model.named_modules():
            if isinstance(layer, nn.Linear):
                layer.register_forward_hook(get_activation(name))
                if layer.weight.requires_grad:
                    layer.weight.register_hook(get_gradient(name))

    def visualize_network_architecture(self):
        """可视化网络架构"""
        fig, ax = plt.subplots(figsize=(14, 10))

        # 定义层的位置和大小
        layers_info = []
        for name, module in self.model.named_children():
            if isinstance(module, nn.Linear):
                layers_info.append({
                    'name': name,
                    'input_size': module.in_features,
                    'output_size': module.out_features
                })

        # 绘制网络架构
        layer_positions = []
        layer_width = 2
        layer_spacing = 4

        for i, layer_info in enumerate(layers_info):
            x = i * layer_spacing
            layer_positions.append(x)

            # 绘制层
            input_size = layer_info['input_size']
            output_size = layer_info['output_size']

            # 输入层神经元
            if i == 0:
                for j in range(min(input_size, 10)):  # 最多显示10个神经元
                    y = j - input_size / 2
                    circle = Circle((x - layer_width/2, y), 0.1,
                                  color='lightblue', ec='black')
                    ax.add_patch(circle)

            # 输出层神经元
            for j in range(min(output_size, 10)):
                y = j - output_size / 2
                circle = Circle((x + layer_width/2, y), 0.1,
                              color='lightcoral', ec='black')
                ax.add_patch(circle)

            # 连接线（简化显示）
            if i > 0:
                prev_x = layer_positions[i-1] + layer_width/2
                curr_x = x - layer_width/2
                ax.plot([prev_x, curr_x], [0, 0], 'k-', alpha=0.3)

            # 层标签
            ax.text(x, -input_size/2 - 1, layer_info['name'],
                   ha='center', fontsize=12, weight='bold')
            ax.text(x, -input_size/2 - 1.5,
                   f"({layer_info['input_size']} → {layer_info['output_size']})",
                   ha='center', fontsize=10)

        ax.set_xlim(-2, len(layers_info) * layer_spacing)
        ax.set_ylim(-8, 8)
        ax.set_aspect('equal')
        ax.axis('off')
        ax.set_title('DQN Network Architecture', fontsize=16, weight='bold')

        plt.tight_layout()
        plt.show()

    def plot_activation_heatmap(self, input_sample):
        """绘制激活值热力图"""
        self.register_hooks()

        # 前向传播
        with torch.no_grad():
            _ = self.model(input_sample)

        # 绘制激活值
        num_layers = len(self.activations)
        fig, axes = plt.subplots(1, num_layers, figsize=(4*num_layers, 6))

        if num_layers == 1:
            axes = [axes]

        for i, (name, activation) in enumerate(self.activations.items()):
            # 取第一个样本的激活值
            act_values = activation[0].numpy()

            # 重塑为2D以便可视化
            if len(act_values.shape) == 1:
                act_values = act_values.reshape(-1, 1)

            im = axes[i].imshow(act_values, cmap='RdYlBu', aspect='auto')
            axes[i].set_title(f'Layer: {name}')
            axes[i].set_xlabel('Neuron Index')
            axes[i].set_ylabel('Activation Value')

            plt.colorbar(im, ax=axes[i])

        plt.tight_layout()
        plt.show()

    def plot_weight_distribution(self):
        """绘制权重分布"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()

        layer_idx = 0
        for name, param in self.model.named_parameters():
            if 'weight' in name and layer_idx < 4:
                weights = param.detach().numpy().flatten()

                axes[layer_idx].hist(weights, bins=50, alpha=0.7, edgecolor='black')
                axes[layer_idx].set_title(f'Weight Distribution: {name}')
                axes[layer_idx].set_xlabel('Weight Value')
                axes[layer_idx].set_ylabel('Frequency')
                axes[layer_idx].grid(True, alpha=0.3)

                # 添加统计信息
                mean_weight = np.mean(weights)
                std_weight = np.std(weights)
                axes[layer_idx].axvline(mean_weight, color='red', linestyle='--',
                                      label=f'Mean: {mean_weight:.3f}')
                axes[layer_idx].axvline(mean_weight + std_weight, color='orange',
                                      linestyle=':', alpha=0.7,
                                      label=f'±1σ: {std_weight:.3f}')
                axes[layer_idx].axvline(mean_weight - std_weight, color='orange',
                                      linestyle=':', alpha=0.7)
                axes[layer_idx].legend()

                layer_idx += 1

        # 隐藏未使用的子图
        for i in range(layer_idx, 4):
            axes[i].axis('off')

        plt.tight_layout()
        plt.show()

def visualize_q_value_landscape():
    """可视化Q值景观"""
    # 创建一个简单的2D状态空间
    x_range = np.linspace(-2, 2, 50)
    y_range = np.linspace(-2, 2, 50)
    X, Y = np.meshgrid(x_range, y_range)

    # 模拟一个简单的Q函数
    def q_function(x, y, action):
        if action == 0:  # 动作0
            return -(x**2 + y**2) + 2*np.exp(-(x-1)**2 - y**2)
        else:  # 动作1
            return -(x**2 + y**2) + 2*np.exp(-x**2 - (y-1)**2)

    # 计算每个位置的Q值
    Q_action0 = np.zeros_like(X)
    Q_action1 = np.zeros_like(X)

    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Q_action0[i, j] = q_function(X[i, j], Y[i, j], 0)
            Q_action1[i, j] = q_function(X[i, j], Y[i, j], 1)

    # 绘制Q值景观
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # 动作0的Q值
    im1 = axes[0].contourf(X, Y, Q_action0, levels=20, cmap='viridis')
    axes[0].set_title('Q-values for Action 0')
    axes[0].set_xlabel('State Dimension 1')
    axes[0].set_ylabel('State Dimension 2')
    plt.colorbar(im1, ax=axes[0])

    # 动作1的Q值
    im2 = axes[1].contourf(X, Y, Q_action1, levels=20, cmap='viridis')
    axes[1].set_title('Q-values for Action 1')
    axes[1].set_xlabel('State Dimension 1')
    axes[1].set_ylabel('State Dimension 2')
    plt.colorbar(im2, ax=axes[1])

    # 最优策略
    optimal_action = (Q_action1 > Q_action0).astype(int)
    im3 = axes[2].contourf(X, Y, optimal_action, levels=[0, 0.5, 1],
                          colors=['red', 'blue'], alpha=0.7)
    axes[2].set_title('Optimal Policy')
    axes[2].set_xlabel('State Dimension 1')
    axes[2].set_ylabel('State Dimension 2')

    # 添加策略图例
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='red', label='Action 0'),
                      Patch(facecolor='blue', label='Action 1')]
    axes[2].legend(handles=legend_elements)

    plt.tight_layout()
    plt.show()

# 运行Q值景观可视化
visualize_q_value_landscape()
```

### 2.2 训练过程动态可视化

```python
class DQNTrainingVisualizer:
    def __init__(self):
        self.fig, self.axes = plt.subplots(2, 3, figsize=(18, 12))
        self.fig.suptitle('DQN Training Progress', fontsize=16)

        # 初始化绘图元素
        self.reward_line, = self.axes[0, 0].plot([], [], 'b-')
        self.loss_line, = self.axes[0, 1].plot([], [], 'r-')
        self.epsilon_line, = self.axes[0, 2].plot([], [], 'g-')
        self.q_mean_line, = self.axes[1, 0].plot([], [], 'purple')
        self.q_std_line, = self.axes[1, 0].plot([], [], 'orange')

        self.setup_plots()

    def setup_plots(self):
        """设置图表属性"""
        # 奖励图
        self.axes[0, 0].set_title('Episode Rewards')
        self.axes[0, 0].set_xlabel('Episode')
        self.axes[0, 0].set_ylabel('Total Reward')
        self.axes[0, 0].grid(True)

        # 损失图
        self.axes[0, 1].set_title('Training Loss')
        self.axes[0, 1].set_xlabel('Training Step')
        self.axes[0, 1].set_ylabel('MSE Loss')
        self.axes[0, 1].set_yscale('log')
        self.axes[0, 1].grid(True)

        # Epsilon图
        self.axes[0, 2].set_title('Exploration Rate')
        self.axes[0, 2].set_xlabel('Episode')
        self.axes[0, 2].set_ylabel('Epsilon')
        self.axes[0, 2].grid(True)

        # Q值统计
        self.axes[1, 0].set_title('Q-value Statistics')
        self.axes[1, 0].set_xlabel('Episode')
        self.axes[1, 0].set_ylabel('Q-value')
        self.axes[1, 0].grid(True)

        # 成功率
        self.axes[1, 1].set_title('Success Rate (100-episode window)')
        self.axes[1, 1].set_xlabel('Episode')
        self.axes[1, 1].set_ylabel('Success Rate')
        self.axes[1, 1].set_ylim(0, 1)
        self.axes[1, 1].grid(True)

        # 网络权重可视化占位
        self.axes[1, 2].set_title('Network Weights Evolution')

    def update(self, episode, reward_history, loss_history, epsilon_history,
               q_stats_history, success_rate_history, network_weights):
        """更新所有图表"""

        # 更新奖励曲线
        episodes = range(len(reward_history))
        self.reward_line.set_data(episodes, reward_history)
        self.axes[0, 0].relim()
        self.axes[0, 0].autoscale_view()

        # 添加移动平均线
        if len(reward_history) > 100:
            moving_avg = pd.Series(reward_history).rolling(100).mean()
            self.axes[0, 0].plot(episodes[99:], moving_avg[99:], 'r--', alpha=0.7)

        # 更新损失曲线
        if loss_history:
            loss_steps = range(len(loss_history))
            self.loss_line.set_data(loss_steps, loss_history)
            self.axes[0, 1].relim()
            self.axes[0, 1].autoscale_view()

        # 更新epsilon曲线
        self.epsilon_line.set_data(episodes, epsilon_history)
        self.axes[0, 2].relim()
        self.axes[0, 2].autoscale_view()

        # 更新Q值统计
        if q_stats_history:
            q_means = [stats['mean'] for stats in q_stats_history]
            q_stds = [stats['std'] for stats in q_stats_history]

            self.q_mean_line.set_data(episodes, q_means)
            self.axes[1, 0].fill_between(episodes,
                                       np.array(q_means) - np.array(q_stds),
                                       np.array(q_means) + np.array(q_stds),
                                       alpha=0.3, color='purple')
            self.axes[1, 0].relim()
            self.axes[1, 0].autoscale_view()

        # 更新成功率
        if success_rate_history:
            success_episodes = range(99, 99 + len(success_rate_history))
            self.axes[1, 1].plot(success_episodes, success_rate_history, 'g-')
            self.axes[1, 1].relim()
            self.axes[1, 1].autoscale_view()

        # 更新网络权重可视化
        if network_weights and episode % 50 == 0:
            self.axes[1, 2].clear()
            self.axes[1, 2].hist(network_weights.flatten(), bins=50, alpha=0.7)
            self.axes[1, 2].set_title(f'Network Weights Distribution (Episode {episode})')
            self.axes[1, 2].set_xlabel('Weight Value')
            self.axes[1, 2].set_ylabel('Frequency')

        plt.draw()
        plt.pause(0.01)

def create_interactive_q_table():
    """创建交互式Q表可视化"""
    import ipywidgets as widgets
    from IPython.display import display

    # 创建示例Q表
    q_table = np.random.rand(16, 4)

    def update_q_visualization(episode_slider_value):
        # 模拟训练过程中Q表的变化
        noise = np.random.normal(0, 0.1, q_table.shape)
        current_q_table = q_table + noise * (episode_slider_value / 100)

        # 绘制Q表热力图
        plt.figure(figsize=(12, 8))

        # 每个动作的Q值
        for action in range(4):
            plt.subplot(2, 2, action + 1)
            q_values_grid = current_q_table[:, action].reshape(4, 4)

            sns.heatmap(q_values_grid, annot=True, fmt='.2f',
                       cmap='RdYlBu_r', center=0)
            plt.title(f'Action {action}')

        plt.tight_layout()
        plt.show()

    # 创建滑块控件
    episode_slider = widgets.IntSlider(
        value=0,
        min=0,
        max=1000,
        step=10,
        description='Episode:',
        continuous_update=False
    )

    # 创建交互式组件
    interactive_plot = widgets.interactive(update_q_visualization,
                                         episode_slider_value=episode_slider)

    display(interactive_plot)

def plot_exploration_visualization():
    """可视化探索vs利用的权衡"""
    episodes = np.arange(1000)
    epsilon_values = 0.95 * (0.995 ** episodes)  # 指数衰减

    # 创建行为分布图
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Epsilon衰减曲线
    axes[0, 0].plot(episodes, epsilon_values, 'b-', linewidth=2)
    axes[0, 0].set_title('Epsilon Decay Over Episodes')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Epsilon (Exploration Rate)')
    axes[0, 0].grid(True)
    axes[0, 0].axhline(y=0.1, color='r', linestyle='--',
                      label='Typical minimum epsilon')
    axes[0, 0].legend()

    # 探索vs利用比例
    exploration_rate = epsilon_values
    exploitation_rate = 1 - epsilon_values

    axes[0, 1].fill_between(episodes, 0, exploration_rate,
                          alpha=0.7, color='orange', label='Exploration')
    axes[0, 1].fill_between(episodes, exploration_rate, 1,
                          alpha=0.7, color='blue', label='Exploitation')
    axes[0, 1].set_title('Exploration vs Exploitation Balance')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Action Selection Probability')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 模拟累积奖励与探索率的关系
    cumulative_rewards = []
    current_reward = 0

    for eps in epsilon_values:
        # 模拟：高探索率初期奖励增长慢，后期快速增长
        if eps > 0.5:
            reward_increment = np.random.normal(0.5, 0.3)
        elif eps > 0.2:
            reward_increment = np.random.normal(1.0, 0.2)
        else:
            reward_increment = np.random.normal(1.5, 0.1)

        current_reward += max(0, reward_increment)
        cumulative_rewards.append(current_reward)

    axes[1, 0].plot(episodes, cumulative_rewards, 'g-', linewidth=2)
    axes[1, 0].set_title('Cumulative Reward vs Exploration')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Cumulative Reward')
    axes[1, 0].grid(True)

    # 创建第二个y轴显示epsilon
    ax2 = axes[1, 0].twinx()
    ax2.plot(episodes, epsilon_values, 'r--', alpha=0.6, label='Epsilon')
    ax2.set_ylabel('Epsilon', color='r')
    ax2.tick_params(axis='y', labelcolor='r')

    # 不同epsilon策略的比较
    strategies = {
        'Constant High (ε=0.3)': [0.3] * len(episodes),
        'Linear Decay': np.linspace(1.0, 0.01, len(episodes)),
        'Exponential Decay': epsilon_values,
        'Constant Low (ε=0.1)': [0.1] * len(episodes)
    }

    for name, strategy in strategies.items():
        axes[1, 1].plot(episodes[:200], strategy[:200],
                       label=name, linewidth=2)

    axes[1, 1].set_title('Different Epsilon Strategies (First 200 episodes)')
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Epsilon')
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()

# 运行探索可视化
plot_exploration_visualization()
```

## 3. 机器人导航可视化

### 3.1 实时环境可视化

```python
class NavigationVisualizer:
    def __init__(self, env):
        self.env = env
        self.fig, self.axes = plt.subplots(1, 2, figsize=(16, 8))

        # 主环境显示
        self.env_ax = self.axes[0]
        self.env_ax.set_xlim(-5, 5)
        self.env_ax.set_ylim(-5, 5)
        self.env_ax.set_aspect('equal')
        self.env_ax.grid(True, alpha=0.3)
        self.env_ax.set_title('Robot Navigation Environment')

        # 传感器数据显示
        self.sensor_ax = self.axes[1]

        # 初始化绘图元素
        self.robot_circle = None
        self.robot_arrow = None
        self.goal_circle = None
        self.obstacle_patches = []
        self.lidar_lines = []
        self.trajectory_line, = self.env_ax.plot([], [], 'b-', alpha=0.5, linewidth=1)

        # 轨迹记录
        self.trajectory = []

        plt.ion()

    def update(self, robot_pos, robot_theta, goal_pos, obstacles, lidar_readings):
        """更新可视化"""
        # 清除之前的绘图元素
        if self.robot_circle:
            self.robot_circle.remove()
        if self.robot_arrow:
            self.robot_arrow.remove()
        if self.goal_circle:
            self.goal_circle.remove()

        for patch in self.obstacle_patches:
            patch.remove()
        self.obstacle_patches.clear()

        for line in self.lidar_lines:
            line.remove()
        self.lidar_lines.clear()

        # 绘制障碍物
        for obstacle in obstacles:
            circle = plt.Circle(obstacle['center'], obstacle['size'],
                              color='red', alpha=0.7)
            self.env_ax.add_patch(circle)
            self.obstacle_patches.append(circle)

        # 绘制目标
        self.goal_circle = plt.Circle(goal_pos, 0.3, color='green', alpha=0.6)
        self.env_ax.add_patch(self.goal_circle)

        # 绘制机器人
        self.robot_circle = plt.Circle(robot_pos, 0.2, color='blue', alpha=0.8)
        self.env_ax.add_patch(self.robot_circle)

        # 绘制机器人朝向
        arrow_length = 0.4
        arrow_end = robot_pos + arrow_length * np.array([
            np.cos(robot_theta), np.sin(robot_theta)
        ])
        self.robot_arrow = self.env_ax.annotate('',
                                              xy=arrow_end,
                                              xytext=robot_pos,
                                              arrowprops=dict(arrowstyle='->',
                                                            color='blue', lw=2))

        # 绘制激光雷达
        for i, distance in enumerate(lidar_readings):
            angle = robot_theta + (2 * np.pi * i / len(lidar_readings))
            end_point = robot_pos + distance * np.array([
                np.cos(angle), np.sin(angle)
            ])
            line = self.env_ax.plot([robot_pos[0], end_point[0]],
                                  [robot_pos[1], end_point[1]],
                                  'r-', alpha=0.4, linewidth=0.8)[0]
            self.lidar_lines.append(line)

        # 更新轨迹
        self.trajectory.append(robot_pos.copy())
        if len(self.trajectory) > 1:
            traj_array = np.array(self.trajectory)
            self.trajectory_line.set_data(traj_array[:, 0], traj_array[:, 1])

        # 绘制传感器数据
        self.plot_sensor_data(lidar_readings, robot_pos, goal_pos)

        plt.draw()
        plt.pause(0.01)

    def plot_sensor_data(self, lidar_readings, robot_pos, goal_pos):
        """绘制传感器数据的极坐标图"""
        self.sensor_ax.clear()

        # 极坐标雷达图
        angles = np.linspace(0, 2*np.pi, len(lidar_readings), endpoint=False)
        angles = np.concatenate((angles, [angles[0]]))  # 闭合图形
        readings = list(lidar_readings) + [lidar_readings[0]]

        self.sensor_ax = plt.subplot(122, projection='polar')
        self.sensor_ax.plot(angles, readings, 'ro-', linewidth=2, markersize=4)
        self.sensor_ax.fill(angles, readings, alpha=0.25, color='red')
        self.sensor_ax.set_title('LiDAR Readings (Polar View)')
        self.sensor_ax.set_ylim(0, 5)

        # 添加距离圈
        self.sensor_ax.set_yticks([1, 2, 3, 4, 5])
        self.sensor_ax.grid(True)

    def reset_trajectory(self):
        """重置轨迹记录"""
        self.trajectory.clear()
        self.trajectory_line.set_data([], [])

def create_training_animation(agent, env, num_episodes=5):
    """创建训练过程动画"""
    visualizer = NavigationVisualizer(env)

    for episode in range(num_episodes):
        print(f"Episode {episode + 1}/{num_episodes}")
        visualizer.reset_trajectory()

        state = env.reset()
        done = False
        step_count = 0

        while not done and step_count < 200:
            # 选择动作
            action_idx = agent.choose_action(state)
            action = agent.action_map[action_idx]  # 假设有action_map

            # 执行动作
            next_state, reward, done, info = env.step(action)

            # 可视化当前状态
            visualizer.update(
                robot_pos=env.robot_pos,
                robot_theta=env.robot_theta,
                goal_pos=env.goal_pos,
                obstacles=env.obstacles,
                lidar_readings=env.get_lidar_readings()
            )

            state = next_state
            step_count += 1

            # 添加暂停以便观察
            plt.pause(0.1)

        print(f"Episode {episode + 1} finished: {info}")
        plt.pause(2)  # 每个episode结束后暂停2秒

    plt.ioff()
    plt.show()

def plot_learning_statistics():
    """绘制学习统计图表"""
    # 模拟训练数据
    episodes = np.arange(1000)

    # 成功率数据（S型增长）
    success_rates = 1 / (1 + np.exp(-0.01 * (episodes - 500))) + np.random.normal(0, 0.05, len(episodes))
    success_rates = np.clip(success_rates, 0, 1)

    # 平均到达时间（随训练减少）
    arrival_times = 200 * np.exp(-0.003 * episodes) + 50 + np.random.normal(0, 10, len(episodes))
    arrival_times = np.clip(arrival_times, 20, 300)

    # 碰撞率（随训练减少）
    collision_rates = 0.8 * np.exp(-0.005 * episodes) + np.random.normal(0, 0.02, len(episodes))
    collision_rates = np.clip(collision_rates, 0, 1)

    # 路径效率（路径长度/最短路径）
    path_efficiency = 3 * np.exp(-0.002 * episodes) + 1 + np.random.normal(0, 0.1, len(episodes))
    path_efficiency = np.clip(path_efficiency, 1, 5)

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 成功率
    axes[0, 0].plot(episodes, success_rates, 'g-', alpha=0.7)
    axes[0, 0].set_title('Success Rate Over Training')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Success Rate')
    axes[0, 0].grid(True)
    axes[0, 0].set_ylim(0, 1)

    # 添加移动平均
    window = 50
    if len(success_rates) > window:
        moving_avg = pd.Series(success_rates).rolling(window).mean()
        axes[0, 0].plot(episodes[window-1:], moving_avg[window-1:],
                       'r-', linewidth=2, label=f'{window}-episode average')
        axes[0, 0].legend()

    # 到达时间
    axes[0, 1].plot(episodes, arrival_times, 'b-', alpha=0.7)
    axes[0, 1].set_title('Average Time to Goal')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Steps to Goal')
    axes[0, 1].grid(True)

    # 碰撞率
    axes[1, 0].plot(episodes, collision_rates, 'r-', alpha=0.7)
    axes[1, 0].set_title('Collision Rate')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Collision Rate')
    axes[1, 0].grid(True)
    axes[1, 0].set_ylim(0, 1)

    # 路径效率
    axes[1, 1].plot(episodes, path_efficiency, 'purple', alpha=0.7)
    axes[1, 1].set_title('Path Efficiency (Path Length / Optimal Length)')
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Efficiency Ratio')
    axes[1, 1].grid(True)
    axes[1, 1].axhline(y=1, color='green', linestyle='--',
                      label='Optimal efficiency')
    axes[1, 1].legend()

    plt.tight_layout()
    plt.show()

# 运行学习统计可视化
plot_learning_statistics()

def create_performance_dashboard():
    """创建性能仪表板"""
    fig = plt.figure(figsize=(20, 12))

    # 创建网格布局
    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)

    # 主要指标
    ax1 = fig.add_subplot(gs[0, :2])
    ax2 = fig.add_subplot(gs[0, 2])
    ax3 = fig.add_subplot(gs[0, 3])

    # 训练曲线
    ax4 = fig.add_subplot(gs[1, :2])
    ax5 = fig.add_subplot(gs[1, 2:])

    # 详细分析
    ax6 = fig.add_subplot(gs[2, :2])
    ax7 = fig.add_subplot(gs[2, 2])
    ax8 = fig.add_subplot(gs[2, 3])

    # 模拟数据
    episodes = np.arange(1000)
    rewards = np.cumsum(np.random.normal(0.1, 0.5, 1000))

    # 1. 主要奖励曲线
    ax1.plot(episodes, rewards, 'b-', alpha=0.7)
    ax1.set_title('Training Rewards', fontsize=14, weight='bold')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Cumulative Reward')
    ax1.grid(True)

    # 2. 当前成功率指示器
    current_success_rate = 0.85
    colors = ['red' if current_success_rate < 0.5 else
             'orange' if current_success_rate < 0.8 else 'green']
    ax2.pie([current_success_rate, 1-current_success_rate],
           colors=[colors[0], 'lightgray'],
           startangle=90, counterclock=False)
    ax2.set_title(f'Success Rate\\n{current_success_rate:.1%}',
                 fontsize=12, weight='bold')

    # 3. 训练进度
    training_progress = 0.7
    ax3.barh(['Training'], [training_progress], color='green', alpha=0.7)
    ax3.set_xlim(0, 1)
    ax3.set_title(f'Training Progress\\n{training_progress:.1%}',
                 fontsize=12, weight='bold')

    # 4. 多指标对比
    metrics = ['Success Rate', 'Avg Reward', 'Collision Rate', 'Path Efficiency']
    values = [0.85, 0.75, 0.15, 0.90]
    bars = ax4.bar(metrics, values, color=['green', 'blue', 'red', 'purple'],
                  alpha=0.7)
    ax4.set_title('Performance Metrics', fontsize=14, weight='bold')
    ax4.set_ylim(0, 1)

    # 添加数值标签
    for bar, value in zip(bars, values):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.2f}', ha='center', va='bottom')

    # 5. 学习曲线对比
    algorithms = ['DQN', 'Q-Learning', 'Random']
    for i, alg in enumerate(algorithms):
        curve = np.cumsum(np.random.normal(0.1-i*0.05, 0.3, 1000))
        ax5.plot(episodes, curve, label=alg, linewidth=2)

    ax5.set_title('Algorithm Comparison', fontsize=14, weight='bold')
    ax5.set_xlabel('Episode')
    ax5.set_ylabel('Performance')
    ax5.legend()
    ax5.grid(True)

    # 6. 探索热力图
    exploration_map = np.random.rand(10, 10)
    im = ax6.imshow(exploration_map, cmap='hot', interpolation='nearest')
    ax6.set_title('Exploration Heatmap', fontsize=14, weight='bold')
    plt.colorbar(im, ax=ax6)

    # 7. 动作分布
    actions = ['Forward', 'Left', 'Right', 'Stop']
    action_counts = [45, 25, 25, 5]
    ax7.pie(action_counts, labels=actions, autopct='%1.1f%%')
    ax7.set_title('Action Distribution', fontsize=12, weight='bold')

    # 8. Q值分布
    q_values = np.random.normal(0, 1, 1000)
    ax8.hist(q_values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    ax8.set_title('Q-value Distribution', fontsize=12, weight='bold')
    ax8.set_xlabel('Q-value')
    ax8.set_ylabel('Frequency')

    plt.suptitle('Robotic Navigation Training Dashboard',
                fontsize=18, weight='bold')
    plt.show()

# 运行性能仪表板
create_performance_dashboard()
```

## 4. 交互式演示工具

### 4.1 实时参数调节器

```python
def create_interactive_hyperparameter_tuner():
    """创建交互式超参数调节器"""
    import ipywidgets as widgets
    from IPython.display import display, clear_output

    # 创建参数控件
    learning_rate_slider = widgets.FloatSlider(
        value=0.001,
        min=0.0001,
        max=0.01,
        step=0.0001,
        description='Learning Rate:',
        style={'description_width': 'initial'}
    )

    epsilon_slider = widgets.FloatSlider(
        value=0.1,
        min=0.01,
        max=1.0,
        step=0.01,
        description='Epsilon:',
        style={'description_width': 'initial'}
    )

    gamma_slider = widgets.FloatSlider(
        value=0.99,
        min=0.8,
        max=0.999,
        step=0.001,
        description='Discount Factor:',
        style={'description_width': 'initial'}
    )

    batch_size_slider = widgets.IntSlider(
        value=32,
        min=8,
        max=128,
        step=8,
        description='Batch Size:',
        style={'description_width': 'initial'}
    )

    # 创建输出区域
    output = widgets.Output()

    def update_visualization(*args):
        with output:
            clear_output(wait=True)

            # 模拟使用新参数进行训练
            lr = learning_rate_slider.value
            eps = epsilon_slider.value
            gamma = gamma_slider.value
            batch_size = batch_size_slider.value

            # 创建模拟的训练结果
            episodes = np.arange(100)

            # 根据参数调整性能曲线
            base_performance = np.cumsum(np.random.normal(0.1, 0.2, 100))

            # 学习率影响收敛速度
            lr_factor = min(lr * 1000, 1.0)  # 归一化

            # epsilon影响初期探索
            eps_factor = eps

            # gamma影响长期奖励
            gamma_factor = gamma

            # 调整性能曲线
            adjusted_performance = base_performance * lr_factor * gamma_factor

            plt.figure(figsize=(12, 8))

            # 主要性能曲线
            plt.subplot(2, 2, 1)
            plt.plot(episodes, adjusted_performance, 'b-', linewidth=2)
            plt.title(f'Training Performance\\n(LR={lr:.4f}, ε={eps:.2f}, γ={gamma:.3f})')
            plt.xlabel('Episode')
            plt.ylabel('Cumulative Reward')
            plt.grid(True)

            # 收敛速度指示器
            plt.subplot(2, 2, 2)
            convergence_speed = lr_factor * 10  # 模拟收敛速度
            plt.bar(['Convergence Speed'], [convergence_speed], color='green', alpha=0.7)
            plt.title('Estimated Convergence Speed')
            plt.ylim(0, 10)

            # 探索vs利用
            plt.subplot(2, 2, 3)
            exploration = [eps * (0.99 ** i) for i in episodes]
            exploitation = [1 - e for e in exploration]

            plt.fill_between(episodes, 0, exploration, alpha=0.7,
                           color='orange', label='Exploration')
            plt.fill_between(episodes, exploration, 1, alpha=0.7,
                           color='blue', label='Exploitation')
            plt.title('Exploration vs Exploitation')
            plt.xlabel('Episode')
            plt.ylabel('Probability')
            plt.legend()

            # 参数敏感性
            plt.subplot(2, 2, 4)
            params = ['LR', 'Epsilon', 'Gamma', 'Batch Size']
            param_values = [lr*1000, eps, gamma, batch_size/128]  # 归一化显示

            colors = ['red', 'orange', 'green', 'blue']
            bars = plt.bar(params, param_values, color=colors, alpha=0.7)
            plt.title('Parameter Values (Normalized)')
            plt.ylim(0, 1.2)

            # 添加数值标签
            for bar, param, value in zip(bars, params,
                                       [lr, eps, gamma, batch_size]):
                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                        f'{value}', ha='center', va='bottom', fontsize=10)

            plt.tight_layout()
            plt.show()

    # 绑定事件
    learning_rate_slider.observe(update_visualization, names='value')
    epsilon_slider.observe(update_visualization, names='value')
    gamma_slider.observe(update_visualization, names='value')
    batch_size_slider.observe(update_visualization, names='value')

    # 显示控件
    controls = widgets.VBox([
        widgets.HTML("<h3>Hyperparameter Tuner</h3>"),
        learning_rate_slider,
        epsilon_slider,
        gamma_slider,
        batch_size_slider,
        output
    ])

    display(controls)

    # 初始显示
    update_visualization()

def create_algorithm_comparison_tool():
    """创建算法比较工具"""
    algorithms = {
        'Q-Learning': {
            'convergence_speed': 0.6,
            'sample_efficiency': 0.4,
            'stability': 0.8,
            'memory_requirement': 0.3,
            'scalability': 0.3
        },
        'DQN': {
            'convergence_speed': 0.8,
            'sample_efficiency': 0.7,
            'stability': 0.6,
            'memory_requirement': 0.8,
            'scalability': 0.9
        },
        'Double DQN': {
            'convergence_speed': 0.8,
            'sample_efficiency': 0.8,
            'stability': 0.8,
            'memory_requirement': 0.8,
            'scalability': 0.9
        },
        'Dueling DQN': {
            'convergence_speed': 0.9,
            'sample_efficiency': 0.9,
            'stability': 0.7,
            'memory_requirement': 0.9,
            'scalability': 0.9
        }
    }

    metrics = list(next(iter(algorithms.values())).keys())

    # 创建雷达图比较
    fig, axes = plt.subplots(2, 2, figsize=(16, 12), subplot_kw=dict(projection='polar'))
    axes = axes.flatten()

    colors = ['blue', 'red', 'green', 'purple']

    for i, (alg_name, alg_data) in enumerate(algorithms.items()):
        ax = axes[i]

        # 准备数据
        values = list(alg_data.values())
        values += values[:1]  # 闭合雷达图

        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]

        # 绘制雷达图
        ax.plot(angles, values, 'o-', linewidth=2, label=alg_name, color=colors[i])
        ax.fill(angles, values, alpha=0.25, color=colors[i])

        # 设置标签
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.set_title(alg_name, size=14, weight='bold', pad=20)
        ax.grid(True)

    plt.suptitle('Algorithm Performance Comparison', fontsize=18, weight='bold')
    plt.tight_layout()
    plt.show()

    # 创建综合比较表
    fig, ax = plt.subplots(figsize=(12, 8))

    # 准备数据矩阵
    data_matrix = []
    for alg_name in algorithms.keys():
        data_matrix.append(list(algorithms[alg_name].values()))

    data_matrix = np.array(data_matrix)

    # 创建热力图
    im = ax.imshow(data_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)

    # 设置标签
    ax.set_xticks(range(len(metrics)))
    ax.set_xticklabels(metrics)
    ax.set_yticks(range(len(algorithms)))
    ax.set_yticklabels(algorithms.keys())

    # 添加数值标注
    for i in range(len(algorithms)):
        for j in range(len(metrics)):
            text = ax.text(j, i, f'{data_matrix[i, j]:.1f}',
                         ha="center", va="center", color="black", weight='bold')

    ax.set_title('Algorithm Performance Matrix', fontsize=16, weight='bold')
    plt.colorbar(im, ax=ax)
    plt.tight_layout()
    plt.show()

# 运行算法比较工具
create_algorithm_comparison_tool()
```

这个可视化工具集提供了：

1. **Q-learning可视化**：Q值热力图、策略地图、状态价值函数
2. **训练过程可视化**：学习曲线、收敛分析、性能指标
3. **DQN可视化**：网络架构、激活值、权重分布、Q值景观
4. **机器人导航可视化**：实时环境显示、传感器数据、轨迹跟踪
5. **交互式工具**：参数调节器、算法比较器、性能仪表板

这些工具能帮助学生更直观地理解强化学习算法的工作原理和训练过程。