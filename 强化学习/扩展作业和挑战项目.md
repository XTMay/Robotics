# 扩展作业和挑战项目

## 阶段一扩展作业

### 作业1：自定义GridWorld环境

**难度**：⭐⭐
**预计时间**：3-4小时

#### 任务描述
创建一个自定义的GridWorld环境，并使用Q-learning解决不同的导航问题。

#### 具体要求
1. **环境设计**：
   - 创建一个8×8的网格世界
   - 设置多个目标点（不同奖励值）
   - 添加不同类型的障碍物（固定障碍、危险区域）
   - 实现随机起始位置

2. **Q-learning实现**：
   - 实现标准Q-learning算法
   - 添加不同的探索策略（ε-贪婪、玻尔兹曼探索、UCB）
   - 比较不同超参数的效果

3. **可视化要求**：
   - 绘制学习曲线
   - 可视化最终策略
   - 制作训练过程动画

#### 代码框架
```python
class CustomGridWorld:
    def __init__(self, size=(8, 8), goals=None, obstacles=None):
        """
        自定义网格世界环境

        Args:
            size: 网格大小
            goals: 目标点列表 [(x, y, reward), ...]
            obstacles: 障碍物列表 [(x, y, penalty), ...]
        """
        self.size = size
        self.goals = goals or [(7, 7, 10), (2, 6, 5)]
        self.obstacles = obstacles or [(3, 3, -5), (4, 4, -5)]
        self.reset()

    def reset(self):
        """重置环境"""
        # TODO: 实现重置逻辑
        pass

    def step(self, action):
        """执行动作"""
        # TODO: 实现状态转移逻辑
        pass

    def render(self):
        """可视化环境"""
        # TODO: 实现可视化
        pass

# 测试不同探索策略
class ExplorationStrategy:
    def epsilon_greedy(self, q_values, epsilon):
        """ε-贪婪策略"""
        pass

    def boltzmann_exploration(self, q_values, temperature):
        """玻尔兹曼探索"""
        pass

    def ucb_exploration(self, q_values, counts, total_steps, c=1.0):
        """UCB探索"""
        pass
```

#### 评分标准
- 环境实现正确性（30%）
- Q-learning算法实现（30%）
- 探索策略比较（20%）
- 可视化质量（20%）

---

### 作业2：多智能体Q-learning

**难度**：⭐⭐⭐
**预计时间**：5-6小时

#### 任务描述
实现多智能体环境下的Q-learning，研究智能体间的交互和竞争。

#### 具体要求
1. **环境设计**：
   - 2-4个智能体同时在网格中活动
   - 智能体可以看到其他智能体的位置
   - 设计竞争和合作的奖励机制

2. **算法实现**：
   - 独立Q-learning（每个智能体独立学习）
   - 集中式训练分散式执行
   - 状态空间包含其他智能体信息

3. **实验分析**：
   - 收敛性分析
   - 策略稳定性
   - 合作vs竞争行为涌现

#### 关键挑战
- 非平稳环境处理
- 状态空间爆炸
- 智能体协调

#### 代码框架
```python
class MultiAgentGridWorld:
    def __init__(self, num_agents=2, grid_size=(6, 6)):
        self.num_agents = num_agents
        self.agents_pos = []
        self.grid_size = grid_size

    def get_joint_state(self):
        """获取联合状态"""
        pass

    def step(self, joint_action):
        """所有智能体同时执行动作"""
        pass

class MultiAgentQLearning:
    def __init__(self, num_agents, state_size, action_size):
        # 每个智能体维护自己的Q表
        self.q_tables = [np.zeros((state_size, action_size))
                        for _ in range(num_agents)]

    def update(self, agent_id, state, action, reward, next_state):
        """更新指定智能体的Q值"""
        pass
```

---

### 作业3：Q-learning变体比较

**难度**：⭐⭐⭐
**预计时间**：4-5小时

#### 任务描述
实现并比较Q-learning的不同变体算法。

#### 要实现的算法
1. **标准Q-learning**
2. **SARSA**（State-Action-Reward-State-Action）
3. **Expected SARSA**
4. **Double Q-learning**

#### 比较维度
- 收敛速度
- 最终性能
- 对参数的敏感性
- 在随机环境中的表现

#### 代码框架
```python
class QLearningVariants:
    def __init__(self, state_size, action_size):
        self.q_table = np.zeros((state_size, action_size))

    def q_learning_update(self, state, action, reward, next_state, alpha, gamma):
        """标准Q-learning更新"""
        pass

    def sarsa_update(self, state, action, reward, next_state, next_action, alpha, gamma):
        """SARSA更新"""
        pass

    def expected_sarsa_update(self, state, action, reward, next_state, policy, alpha, gamma):
        """Expected SARSA更新"""
        pass

class DoubleQLearning:
    def __init__(self, state_size, action_size):
        self.q_table_a = np.zeros((state_size, action_size))
        self.q_table_b = np.zeros((state_size, action_size))

    def update(self, state, action, reward, next_state, alpha, gamma):
        """Double Q-learning更新"""
        pass
```

---

## 阶段二扩展作业

### 作业4：DQN架构实验

**难度**：⭐⭐⭐
**预计时间**：6-8小时

#### 任务描述
设计和比较不同的DQN网络架构，分析架构对性能的影响。

#### 实验内容
1. **网络深度实验**：
   - 2层、3层、4层、5层网络
   - 分析深度对收敛的影响

2. **网络宽度实验**：
   - 不同隐藏层大小：64, 128, 256, 512
   - 分析参数量与性能的关系

3. **激活函数实验**：
   - ReLU、Leaky ReLU、ELU、Swish
   - 比较不同激活函数的效果

4. **正则化技术**：
   - Dropout、Batch Normalization、Layer Normalization
   - 分析正则化对训练稳定性的影响

#### 代码框架
```python
class DQNArchitectures:
    def create_deep_network(self, input_size, output_size, depth, width):
        """创建不同深度和宽度的网络"""
        layers = [nn.Linear(input_size, width), nn.ReLU()]

        for _ in range(depth - 2):
            layers.extend([nn.Linear(width, width), nn.ReLU()])

        layers.append(nn.Linear(width, output_size))
        return nn.Sequential(*layers)

    def create_regularized_network(self, input_size, output_size,
                                 dropout_rate=0.1, use_batch_norm=False):
        """创建带正则化的网络"""
        pass

class ArchitectureExperiment:
    def __init__(self):
        self.results = {}

    def run_experiment(self, config, env, episodes=1000):
        """运行单个架构实验"""
        pass

    def compare_architectures(self):
        """比较不同架构的性能"""
        pass
```

#### 评分标准
- 实验设计完整性（25%）
- 代码实现质量（25%）
- 结果分析深度（25%）
- 可视化和报告（25%）

---

### 作业5：高级DQN变体实现

**难度**：⭐⭐⭐⭐
**预计时间**：8-10小时

#### 任务描述
实现DQN的高级变体，并在复杂环境中进行测试。

#### 要实现的变体
1. **Dueling DQN**
2. **Prioritized Experience Replay DQN**
3. **Noisy DQN**
4. **Rainbow DQN**（综合多种技术）

#### 测试环境
- Atari游戏环境
- 自定义复杂导航环境
- 连续控制任务

#### 代码框架
```python
class DuelingDQN(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=256):
        super(DuelingDQN, self).__init__()
        # TODO: 实现Dueling架构
        pass

    def forward(self, x):
        # TODO: 实现前向传播
        pass

class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        # TODO: 实现优先经验回放
        pass

    def push(self, state, action, reward, next_state, done, error):
        # TODO: 添加经验和计算优先级
        pass

    def sample(self, batch_size, beta=0.4):
        # TODO: 按优先级采样
        pass

class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features):
        # TODO: 实现噪声线性层
        pass
```

---

### 作业6：连续控制DQN

**难度**：⭐⭐⭐⭐
**预计时间**：6-8小时

#### 任务描述
将DQN扩展到连续动作空间，实现DDPG或TD3算法。

#### 具体要求
1. **算法实现**：
   - Actor-Critic架构
   - 确定性策略梯度
   - 目标网络软更新

2. **环境测试**：
   - 连续控制CartPole
   - 机器人臂控制
   - 自定义连续导航环境

3. **技术挑战**：
   - 探索噪声设计
   - 训练稳定性
   - 超参数敏感性

#### 代码框架
```python
class Actor(nn.Module):
    def __init__(self, state_size, action_size, max_action):
        super(Actor, self).__init__()
        # TODO: 实现Actor网络
        pass

class Critic(nn.Module):
    def __init__(self, state_size, action_size):
        super(Critic, self).__init__()
        # TODO: 实现Critic网络
        pass

class DDPG:
    def __init__(self, state_size, action_size, max_action):
        # TODO: 初始化DDPG智能体
        pass

    def select_action(self, state, noise=0.1):
        # TODO: 选择动作（带探索噪声）
        pass

    def update(self, batch):
        # TODO: 更新Actor和Critic网络
        pass
```

---

## 阶段三扩展作业

### 作业7：多目标机器人导航

**难度**：⭐⭐⭐⭐
**预计时间**：10-12小时

#### 任务描述
实现多目标点的机器人导航任务，机器人需要按序访问多个目标点。

#### 具体要求
1. **环境设计**：
   - 多个目标点（3-5个）
   - 动态障碍物
   - 燃料/电量限制

2. **算法扩展**：
   - 分层强化学习
   - 课程学习（Curriculum Learning）
   - 多任务学习

3. **评估指标**：
   - 成功率
   - 路径效率
   - 时间效率
   - 燃料消耗

#### 代码框架
```python
class MultiTargetNavigation:
    def __init__(self, targets, fuel_limit=1000):
        self.targets = targets
        self.fuel_limit = fuel_limit
        self.visited_targets = set()

    def get_state(self):
        # TODO: 包含当前位置、剩余目标、燃料信息
        pass

    def get_reward(self, action, new_position):
        # TODO: 设计多目标奖励函数
        pass

class HierarchicalDQN:
    def __init__(self, meta_controller, sub_controllers):
        self.meta_controller = meta_controller  # 选择子目标
        self.sub_controllers = sub_controllers  # 执行低层动作

    def select_action(self, state, goal):
        # TODO: 分层动作选择
        pass
```

#### 挑战任务
- **动态目标**：目标点会随时间变化
- **多机器人协作**：多个机器人协同完成任务
- **部分可观测**：机器人只能看到局部环境

---

### 作业8：从仿真到现实

**难度**：⭐⭐⭐⭐⭐
**预计时间**：15-20小时

#### 任务描述
研究sim-to-real迁移问题，将仿真中训练的模型应用到真实环境。

#### 具体内容
1. **域适应技术**：
   - 域随机化（Domain Randomization）
   - 对抗训练
   - 渐进式迁移

2. **鲁棒性测试**：
   - 传感器噪声
   - 执行器误差
   - 环境变化

3. **实验设计**：
   - 仿真参数变化
   - 真实数据采集
   - 迁移效果评估

#### 代码框架
```python
class DomainRandomization:
    def __init__(self, param_ranges):
        self.param_ranges = param_ranges

    def randomize_environment(self, env):
        # TODO: 随机化环境参数
        pass

class RobustnessEvaluator:
    def __init__(self):
        self.noise_levels = [0.0, 0.1, 0.2, 0.3, 0.5]

    def test_sensor_noise(self, agent, env):
        # TODO: 测试传感器噪声鲁棒性
        pass

    def test_actuator_error(self, agent, env):
        # TODO: 测试执行器误差鲁棒性
        pass

class SimToRealTransfer:
    def __init__(self, sim_env, real_env):
        self.sim_env = sim_env
        self.real_env = real_env

    def fine_tune_model(self, pre_trained_agent, real_data):
        # TODO: 在真实数据上微调模型
        pass
```

---

## 挑战项目

### 挑战1：强化学习竞赛项目

**难度**：⭐⭐⭐⭐⭐
**预计时间**：20-30小时

#### 项目描述
参与或模拟国际强化学习竞赛，如OpenAI Gym竞赛、机器人学习挑战等。

#### 竞赛任务示例
1. **自动驾驶**：
   - 复杂交通环境导航
   - 多车交互
   - 安全约束优化

2. **机器人操作**：
   - 精细操作任务
   - 视觉伺服控制
   - 人机协作

3. **游戏AI**：
   - 复杂策略游戏
   - 实时决策
   - 对手建模

#### 评估标准
- 算法创新性（30%）
- 性能表现（30%）
- 代码质量（20%）
- 文档和报告（20%）

---

### 挑战2：开源贡献项目

**难度**：⭐⭐⭐⭐
**预计时间**：可长期进行

#### 项目描述
为开源强化学习项目做贡献，提升实际开发能力。

#### 可选项目
1. **OpenAI Gym**：
   - 新环境开发
   - 现有环境改进
   - 文档完善

2. **Stable Baselines3**：
   - 算法实现
   - Bug修复
   - 性能优化

3. **PyBullet**：
   - 机器人环境
   - 物理仿真改进
   - 可视化工具

#### 贡献指南
1. **选择项目和issue**
2. **阅读贡献指南**
3. **实现和测试**
4. **提交Pull Request**
5. **响应反馈和修改**

---

### 挑战3：研究论文复现

**难度**：⭐⭐⭐⭐⭐
**预计时间**：30-50小时

#### 任务描述
选择近年来的重要强化学习论文进行完整复现。

#### 推荐论文
1. **算法类**：
   - "Proximal Policy Optimization Algorithms" (PPO)
   - "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL"
   - "Mastering Atari with Discrete World Models" (DreamerV2)

2. **应用类**：
   - "Learning Dexterous In-Hand Manipulation"
   - "Hide and Seek: Training Agents in Multi-Agent Environments"
   - "Grandmaster level in StarCraft II using multi-agent RL"

#### 复现要求
1. **理论理解**：
   - 详细解读论文
   - 理解核心创新点
   - 分析算法复杂度

2. **代码实现**：
   - 从零实现算法
   - 复现实验结果
   - 消融实验分析

3. **扩展研究**：
   - 改进算法
   - 新实验设计
   - 写作技术报告

#### 评估标准
- 论文理解深度（25%）
- 实现正确性（25%）
- 实验结果（25%）
- 创新扩展（25%）

---

## 进阶研究方向

### 方向1：深度强化学习前沿

#### 研究主题
1. **无模型vs有模型**：
   - Model-Based RL
   - World Models
   - 规划与学习结合

2. **样本效率**：
   - Few-shot Learning
   - Meta Learning
   - Transfer Learning

3. **安全强化学习**：
   - 约束优化
   - 风险敏感学习
   - 可解释性

#### 实践项目
- 实现MuZero算法
- 设计安全约束的导航任务
- 研究元学习在机器人中的应用

### 方向2：多智能体强化学习

#### 研究主题
1. **合作与竞争**：
   - Multi-Agent Actor-Critic
   - 自对弈学习
   - 涌现行为研究

2. **通信与协调**：
   - 学习通信协议
   - 去中心化决策
   - 共识算法

3. **社会学习**：
   - 模仿学习
   - 社会影响建模
   - 群体智能

#### 实践项目
- 实现MADDPG算法
- 设计多机器人协作任务
- 研究通信学习机制

### 方向3：机器人学习

#### 研究主题
1. **操作学习**：
   - 精细操作技能
   - 工具使用学习
   - 人机协作

2. **导航与SLAM**：
   - 视觉导航
   - 动态环境适应
   - 终身学习

3. **具身智能**：
   - 感知-动作闭环
   - 多模态学习
   - 认知架构

#### 实践项目
- 实现机器人抓取学习
- 设计视觉导航系统
- 研究终身学习方法

---

## 学习资源和工具

### 在线课程和教程
1. **David Silver's RL Course** (DeepMind)
2. **CS285: Deep RL** (UC Berkeley)
3. **OpenAI Spinning Up**
4. **Sutton & Barto教科书配套资源**

### 实践平台
1. **OpenAI Gym**：标准强化学习环境
2. **PyBullet**：物理仿真环境
3. **Unity ML-Agents**：游戏和仿真环境
4. **RoboSchool/RoboSuite**：机器人仿真

### 开发工具
1. **Stable Baselines3**：算法库
2. **RLLib**：分布式强化学习
3. **TensorBoard/Wandb**：实验跟踪
4. **Docker**：环境配置

### 竞赛和挑战
1. **NeurIPS强化学习竞赛**
2. **ICRA机器人学习挑战**
3. **Kaggle连接竞赛**
4. **OpenAI/DeepMind挑战**

这个扩展作业和挑战项目集合提供了从基础到高级的完整学习路径，帮助学生深入掌握机器人强化学习的理论和实践。